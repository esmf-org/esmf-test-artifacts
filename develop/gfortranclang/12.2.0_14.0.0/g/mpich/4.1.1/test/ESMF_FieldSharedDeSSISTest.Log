2025-05-08 10:07:27
20250508 102239.555 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20250508 102239.556 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20250508 102239.556 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20250508 102239.556 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20250508 102239.556 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET0 Running with ESMF Version   : v8.9.0b08-5-g079ca68fdc
20250508 102239.556 INFO             PET0 ESMF library build date/time: "May  8 2025" "10:06:33"
20250508 102239.556 INFO             PET0 ESMF library build location : /Users/oehmke/ESMF_AutoTest/gfortranclang_12.2.0_14.0.0_mpich_g_develop/esmf
20250508 102239.556 INFO             PET0 ESMF_COMM                   : mpich
20250508 102239.556 INFO             PET0 ESMF_MOAB                   : enabled
20250508 102239.556 INFO             PET0 ESMF_LAPACK                 : enabled
20250508 102239.556 INFO             PET0 ESMF_NETCDF                 : enabled
20250508 102239.556 INFO             PET0 ESMF_PNETCDF                : disabled
20250508 102239.556 INFO             PET0 ESMF_PIO                    : enabled
20250508 102239.556 INFO             PET0 ESMF_YAMLCPP                : enabled
20250508 102239.556 INFO             PET0 --- VMK::logSystem() start -------------------------------
20250508 102239.556 INFO             PET0 esmfComm=mpich
20250508 102239.556 INFO             PET0 isPthreadsEnabled=1
20250508 102239.556 INFO             PET0 isOpenMPEnabled=0
20250508 102239.556 INFO             PET0 isOpenACCEnabled=0
20250508 102239.557 INFO             PET0 isSsiSharedMemoryEnabled=1
20250508 102239.557 INFO             PET0 isNvmlEnabled=0
20250508 102239.557 INFO             PET0 isNumaEnabled=0
20250508 102239.557 INFO             PET0 ssiCount=1 peCount=6
20250508 102239.557 INFO             PET0 PE=0 SSI=0 SSIPE=0
20250508 102239.557 INFO             PET0 PE=1 SSI=0 SSIPE=1
20250508 102239.557 INFO             PET0 PE=2 SSI=0 SSIPE=2
20250508 102239.557 INFO             PET0 PE=3 SSI=0 SSIPE=3
20250508 102239.557 INFO             PET0 PE=4 SSI=0 SSIPE=4
20250508 102239.557 INFO             PET0 PE=5 SSI=0 SSIPE=5
20250508 102239.557 INFO             PET0 ndevs=0 ndevsSSI=0
20250508 102239.557 INFO             PET0 
20250508 102239.557 INFO             PET0 --- VMK::logSystem() MPI Layer ---------------------------
20250508 102239.557 INFO             PET0 MPI_VERSION=4
20250508 102239.557 INFO             PET0 MPI_SUBVERSION=1
20250508 102239.557 INFO             PET0 MPICH_VERSION=4.2.3
20250508 102239.557 INFO             PET0 mpi_t_okay=1
20250508 102239.557 INFO             PET0 --- VMK::logSystem() MPI Tool Interface Control Vars ---
20250508 102239.557 INFO             PET0 index=   0                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb            - Force nonblocking algorithm
smp           - Force smp algorithm
k_dissemination - Force high radix dissemination algorithm
recexch       - Force recursive exchange algorithm
20250508 102239.557 INFO             PET0 index=   1                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
bcast - Force bcast algorithm
nb    - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=   2                               MPIR_CVAR_BARRIER_DISSEM_KVAL : k value for dissemination exchange based barrier algorithm
20250508 102239.557 INFO             PET0 index=   3                              MPIR_CVAR_BARRIER_RECEXCH_KVAL : k value for recursive exchange based allreduce based barrier
20250508 102239.557 INFO             PET0 index=   4                 MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.557 INFO             PET0 index=   5                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20250508 102239.557 INFO             PET0 index=   6                              MPIR_CVAR_IBARRIER_DISSEM_KVAL : k value for dissemination exchange based ibarrier
20250508 102239.557 INFO             PET0 index=   7                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
tsp_recexch - Force generic transport based recursive exchange algorithm
tsp_k_dissemination - Force generic transport based high-radix dissemination algorithm
20250508 102239.557 INFO             PET0 index=   8                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_bcast - Force bcast algorithm
20250508 102239.557 INFO             PET0 index=   9                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET0 index=  10                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET0 index=  11                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET0 index=  12                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes.
20250508 102239.557 INFO             PET0 index=  13                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial                                - Force Binomial Tree
nb                                      - Force nonblocking algorithm
smp                                     - Force smp algorithm
scatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgather
scatter_ring_allgather                  - Force Scatter Ring
pipelined_tree                          - Force tree-based pipelined algorithm
tree                                    - Force tree-based algorithm
20250508 102239.557 INFO             PET0 index=  14                                   MPIR_CVAR_BCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based bcast
20250508 102239.557 INFO             PET0 index=  15                                   MPIR_CVAR_BCAST_TREE_TYPE : Tree type for tree based bcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.557 INFO             PET0 index=  16                         MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.557 INFO             PET0 index=  17                               MPIR_CVAR_BCAST_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.557 INFO             PET0 index=  18                            MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.557 INFO             PET0 index=  19                          MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.557 INFO             PET0 index=  20                          MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.557 INFO             PET0 index=  21                             MPIR_CVAR_BCAST_IS_NON_BLOCKING : If set to true, MPI_Bcast will use non-blocking send.
20250508 102239.557 INFO             PET0 index=  22                    MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE : Indicates the chunk size for pipelined bcast.
20250508 102239.557 INFO             PET0 index=  23                               MPIR_CVAR_BCAST_RECV_PRE_POST : If set to true, MPI_Bcast will pre-post all the receives.
20250508 102239.557 INFO             PET0 index=  24                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                      - Force nonblocking algorithm
remote_send_local_bcast - Force remote-send-local-bcast algorithm
20250508 102239.557 INFO             PET0 index=  25                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20250508 102239.557 INFO             PET0 index=  26                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.557 INFO             PET0 index=  27                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET0 index=  28                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET0 index=  29                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial                             - Force Binomial algorithm
sched_smp                                  - Force smp algorithm
sched_scatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithm
sched_scatter_ring_allgather               - Force Scatter Ring Allgather algorithm
tsp_tree                               - Force Generic Transport Tree algorithm
tsp_scatterv_recexch_allgatherv        - Force Generic Transport Scatterv followed by Recursive Exchange Allgatherv algorithm
tsp_scatterv_ring_allgatherv           - Force Generic Transport Scatterv followed by Ring Allgatherv algorithm
tsp_ring                               - Force Generic Transport Ring algorithm
20250508 102239.557 INFO             PET0 index=  30                              MPIR_CVAR_IBCAST_SCATTERV_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET0 index=  31                    MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET0 index=  32                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_flat - Force flat algorithm
20250508 102239.557 INFO             PET0 index=  33                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20250508 102239.557 INFO             PET0 index=  34                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  35                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                   - Force linear algorithm
local_gather_remote_send - Force local-gather-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  36                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET0 index=  37                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20250508 102239.557 INFO             PET0 index=  38                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_long  - Force long inter algorithm
sched_short - Force short inter algorithm
20250508 102239.557 INFO             PET0 index=  39                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  40                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  41                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear         - Force linear algorithm
tsp_linear       - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET0 index=  42                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET0 index=  43                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20250508 102239.557 INFO             PET0 index=  44                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  45                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                    - Force linear algorithm
nb                        - Force nonblocking algorithm
remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET0 index=  46                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET0 index=  47                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20250508 102239.557 INFO             PET0 index=  48                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear                    - Force linear algorithm
sched_remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET0 index=  49                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  50                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  51                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET0 index=  52                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET0 index=  53                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET0 index=  54                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET0 index=  55                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
k_brucks           - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
recexch_doubling   - Force recexch distance doubling algorithm
recexch_halving    - Force recexch distance halving algorithm
20250508 102239.557 INFO             PET0 index=  56                             MPIR_CVAR_ALLGATHER_BRUCKS_KVAL : radix (k) value for generic transport brucks based allgather
20250508 102239.557 INFO             PET0 index=  57                            MPIR_CVAR_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather
20250508 102239.557 INFO             PET0 index=  58               MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.557 INFO             PET0 index=  59                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
nb                        - Force nonblocking algorithm
20250508 102239.557 INFO             PET0 index=  60                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20250508 102239.557 INFO             PET0 index=  61                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20250508 102239.557 INFO             PET0 index=  62                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_ring               - Force ring algorithm
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
tsp_ring       - Force generic transport ring algorithm
tsp_brucks     - Force generic transport based brucks algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
20250508 102239.557 INFO             PET0 index=  63                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20250508 102239.557 INFO             PET0 index=  64                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20250508 102239.558 INFO             PET0 index=  65                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
20250508 102239.558 INFO             PET0 index=  66                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET0 index=  67                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20250508 102239.558 INFO             PET0 index=  68                           MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL : k value for radix in brucks based iallgatherv
20250508 102239.558 INFO             PET0 index=  69                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_ring               - Force ring algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
tsp_ring             - Force generic transport ring algorithm
tsp_brucks           - Force generic transport based brucks algorithm
20250508 102239.558 INFO             PET0 index=  70                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET0 index=  71                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20250508 102239.558 INFO             PET0 index=  72                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20250508 102239.558 INFO             PET0 index=  73                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20250508 102239.558 INFO             PET0 index=  74                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks                    - Force brucks algorithm
k_brucks                  - Force Force radix k brucks algorithm
nb                        - Force nonblocking algorithm
pairwise                  - Force pairwise algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET0 index=  75                              MPIR_CVAR_ALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based alltoall
20250508 102239.558 INFO             PET0 index=  76                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET0 index=  77                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks            - Force brucks algorithm
sched_inplace           - Force inplace algorithm
sched_pairwise          - Force pairwise algorithm
sched_permuted_sendrecv - Force permuted sendrecv algorithm
tsp_ring            - Force generic transport based ring algorithm
tsp_brucks          - Force generic transport based brucks algorithm
tsp_scattered       - Force generic transport based scattered algorithm
20250508 102239.558 INFO             PET0 index=  78                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET0 index=  79                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET0 index=  80                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
pairwise_exchange - Force pairwise exchange algorithm
nb                - Force nonblocking algorithm
20250508 102239.558 INFO             PET0 index=  81                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_scattered       - Force generic transport based scattered algorithm
tsp_blocked         - Force generic transport blocked algorithm
tsp_inplace         - Force generic transport inplace algorithm
20250508 102239.558 INFO             PET0 index=  82                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET0 index=  83            MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.558 INFO             PET0 index=  84                   MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.558 INFO             PET0 index=  85                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET0 index=  86                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET0 index=  87                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_blocked   - Force generic transport based blocked algorithm
tsp_inplace   - Force generic transport based inplace algorithm
20250508 102239.558 INFO             PET0 index=  88                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET0 index=  89                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET0 index=  90                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20250508 102239.558 INFO             PET0 index=  91                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial              - Force binomial algorithm
nb                    - Force nonblocking algorithm
smp                   - Force smp algorithm
reduce_scatter_gather - Force reduce scatter gather algorithm
20250508 102239.558 INFO             PET0 index=  92                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_reduce_remote_send - Force local-reduce-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.558 INFO             PET0 index=  93                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20250508 102239.558 INFO             PET0 index=  94                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET0 index=  95                       MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET0 index=  96                             MPIR_CVAR_IREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET0 index=  97                          MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET0 index=  98                        MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET0 index=  99                        MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET0 index= 100                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET0 index= 101                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET0 index= 102                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET0 index= 103                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                   - Force smp algorithm
sched_binomial              - Force binomial algorithm
sched_reduce_scatter_gather - Force reduce scatter gather algorithm
tsp_tree                - Force Generic Transport Tree
tsp_ring                - Force Generic Transport Ring
20250508 102239.558 INFO             PET0 index= 104                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_reduce_remote_send - Force local-reduce-remote-send algorithm
20250508 102239.558 INFO             PET0 index= 105                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET0 index= 106                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20250508 102239.558 INFO             PET0 index= 107                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                       - Force nonblocking algorithm
smp                      - Force smp algorithm
recursive_doubling       - Force recursive doubling algorithm
reduce_scatter_allgather - Force reduce scatter allgather algorithm
tree                     - Force pipelined tree algorithm
recexch                  - Force generic transport recursive exchange algorithm
ring                     - Force ring algorithm
k_reduce_scatter_allgather - Force reduce scatter allgather algorithm
20250508 102239.558 INFO             PET0 index= 108                               MPIR_CVAR_ALLREDUCE_TREE_TYPE : Tree type for tree based allreduce knomial_1 is default as it supports both commutative and non-commutative reduce operations kary      - kary tree type knomial_1 - knomial_1 tree type (tree grows starting from the left of the root) knomial_2 - knomial_2 tree type (tree grows starting from the right of the root) topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET0 index= 109                               MPIR_CVAR_ALLREDUCE_TREE_KVAL : Indicates the branching factor for kary or knomial trees.
20250508 102239.558 INFO             PET0 index= 110                     MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET0 index= 111                           MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET0 index= 112                        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET0 index= 113                      MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET0 index= 114                      MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET0 index= 115                MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based allreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET0 index= 116                   MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET0 index= 117                            MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based allreduce
20250508 102239.558 INFO             PET0 index= 118               MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET0 index= 119                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                    - Force nonblocking algorithm
reduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20250508 102239.558 INFO             PET0 index= 120                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20250508 102239.558 INFO             PET0 index= 121                              MPIR_CVAR_IALLREDUCE_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.558 INFO             PET0 index= 122               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET0 index= 123                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET0 index= 124                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20250508 102239.558 INFO             PET0 index= 125                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_naive                      - Force naive algorithm
sched_smp                        - Force smp algorithm
sched_recursive_doubling         - Force recursive doubling algorithm
sched_reduce_scatter_allgather   - Force reduce scatter allgather algorithm
tsp_recexch_single_buffer    - Force generic transport recursive exchange with single buffer for receives
tsp_recexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
tsp_tree                     - Force generic transport tree algorithm
tsp_ring                     - Force generic transport ring algorithm
tsp_recexch_reduce_scatter_recexch_allgatherv  - Force generic transport recursive exchange with reduce scatter and allgatherv
20250508 102239.558 INFO             PET0 index= 126                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20250508 102239.558 INFO             PET0 index= 127          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20250508 102239.558 INFO             PET0 index= 128                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
noncommutative     - Force noncommutative algorithm
pairwise           - Force pairwise algorithm
recursive_doubling - Force recursive doubling algorithm
recursive_halving  - Force recursive halving algorithm
20250508 102239.558 INFO             PET0 index= 129                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET0 index= 130                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20250508 102239.558 INFO             PET0 index= 131                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET0 index= 132                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET0 index= 133              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
noncommutative     - Force noncommutative algorithm
recursive_doubling - Force recursive doubling algorithm
pairwise           - Force pairwise algorithm
recursive_halving  - Force recursive halving algorithm
nb                 - Force nonblocking algorithm
20250508 102239.558 INFO             PET0 index= 134              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET0 index= 135                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20250508 102239.558 INFO             PET0 index= 136             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET0 index= 137             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET0 index= 138                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
smp                - Force smp algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET0 index= 139                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                  - Force smp algorithm
sched_recursive_doubling   - Force recursive doubling algorithm
tsp_recursive_doubling - Force generic transport recursive doubling algorithm
20250508 102239.558 INFO             PET0 index= 140                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET0 index= 141                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET0 index= 142                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.558 INFO             PET0 index= 143                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.558 INFO             PET0 index= 144               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.558 INFO             PET0 index= 145               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.558 INFO             PET0 index= 146               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.558 INFO             PET0 index= 147               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET0 index= 148              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 149              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 150                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET0 index= 151                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET0 index= 152                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 153                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 154                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET0 index= 155                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET0 index= 156               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 157               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 158                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET0 index= 159                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET0 index= 160               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 161               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET0 index= 162                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 163                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 164                    MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 165                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 166                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 167                      MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 168                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 169                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 170                     MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 171                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 172                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 173                    MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 174                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 175                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 176                    MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 177                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 178                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 179                   MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 180                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 181                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 182                  MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 183                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 184                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 185                 MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 186                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 187                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 188                   MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 189                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 190                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 191                  MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 192                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 193                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 194                  MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 195                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 196                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 197                     MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 198                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 199                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 200                  MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 201                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 202                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 203             MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 204            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 205           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 206       MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 207                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 208                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 209                       MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 210                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 211                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 212                     MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 213              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 214             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 215         MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 216             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 217            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 218        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 219               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 220              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 221          MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 222              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 223             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 224         MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 225              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 226             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 227         MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET0 index= 228                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20250508 102239.559 INFO             PET0 index= 229                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20250508 102239.559 INFO             PET0 index= 230                             MPIR_CVAR_IALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based ialltoall
20250508 102239.559 INFO             PET0 index= 231                   MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR : If set to true, the tsp based brucks algorithm will allocate dedicated send and receive buffers for every neighbor in the brucks algorithm. Otherwise, it would reuse a single buffer for sending and receiving data to/from neighbors
20250508 102239.559 INFO             PET0 index= 232             MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.559 INFO             PET0 index= 233                    MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.559 INFO             PET0 index= 234                                MPIR_CVAR_DEVICE_COLLECTIVES : Variable to select whether the device can override the
MPIR-level collective algorithms.
all     - Always prefer the device collectives
none    - Never pick the device collectives
percoll - Use the per-collective CVARs to decide
20250508 102239.560 INFO             PET0 index= 235                               MPIR_CVAR_COLLECTIVE_FALLBACK : Variable to control what the MPI library should do if the
user-specified collective algorithm does not work for the
arguments passed in by the user.
error   - throw an error
print   - print an error message and fallback to the internally selected algorithm
silent  - silently fallback to the internally selected algorithm
20250508 102239.560 INFO             PET0 index= 236                   MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.560 INFO             PET0 index= 237                                    MPIR_CVAR_HIERARCHY_DUMP : If set to true, each rank will dump the hierarchy data structure to a file named "hierarchy[rank]" in the current folder. If set to false, the hierarchy data structure will not be dumped.
20250508 102239.560 INFO             PET0 index= 238                                  MPIR_CVAR_COORDINATES_FILE : Defines the location of the input coordinates file.
20250508 102239.560 INFO             PET0 index= 239                                    MPIR_CVAR_COLL_TREE_DUMP : If set to true, each rank will dump the tree to a file named "colltree[rank].json" in the current folder. If set to false, the tree will not be dumped.
20250508 102239.560 INFO             PET0 index= 240                                  MPIR_CVAR_COORDINATES_DUMP : If set to true, rank 0 will dump the network coordinates to a file named "coords" in the current folder. If set to false, the network coordinates will not be dumped.
20250508 102239.560 INFO             PET0 index= 241                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20250508 102239.560 INFO             PET0 index= 242                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20250508 102239.560 INFO             PET0 index= 243                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20250508 102239.560 INFO             PET0 index= 244                                MPIR_CVAR_DATALOOP_FAST_SEEK : use a datatype-specialized algorithm to shortcut seeking to the correct location in a noncontiguous buffer
20250508 102239.560 INFO             PET0 index= 245                             MPIR_CVAR_YAKSA_COMPLEX_SUPPORT : This CVAR indicates that complex type reduction is not supported in yaksa.
20250508 102239.560 INFO             PET0 index= 246                                MPIR_CVAR_GPU_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET0 index= 247                           MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET0 index= 248                            MPIR_CVAR_ENABLE_YAKSA_REDUCTION : This cvar enables yaksa based reduction for local reduce.
20250508 102239.560 INFO             PET0 index= 249                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20250508 102239.560 INFO             PET0 index= 250                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20250508 102239.560 INFO             PET0 index= 251                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20250508 102239.560 INFO             PET0 index= 252                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20250508 102239.560 INFO             PET0 index= 253                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20250508 102239.560 INFO             PET0 index= 254                          MPIR_CVAR_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. Can be set to auto or comma-separated list of logical processors. When set to auto - MPICH will automatically select logical CPU cores to decide affinity of the progress threads. When set to comma-separated list of logical processors - In case of N progress threads per process, the first N logical processors from list will be assigned to threads of first local process, the next N logical processors from list - to second local process and so on. For example, thread affinity is "0,1,2,3", 2 progress threads per process and 2 processes per node. Progress threads of first local process will be pinned on logical processors "0,1", progress threads of second local process - on "2,3". Cannot work together with MPIR_CVAR_NUM_CLIQUES or MPIR_CVAR_ODD_EVEN_CLIQUES.
20250508 102239.560 INFO             PET0 index= 255                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20250508 102239.560 INFO             PET0 index= 256                                 MPIR_CVAR_COREDUMP_ON_ABORT : Call libc abort() to generate a corefile
20250508 102239.560 INFO             PET0 index= 257                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20250508 102239.560 INFO             PET0 index= 258                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20250508 102239.560 INFO             PET0 index= 259                                     MPIR_CVAR_DEBUG_SUMMARY : If true, print internal summary of various debug information, such as memory allocation by category. Each layer may print their own summary information. For example, ch4-ofi may print its provider capability settings.
20250508 102239.560 INFO             PET0 index= 260                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20250508 102239.560 INFO             PET0 index= 261                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20250508 102239.560 INFO             PET0 index= 262                    MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST : If true, mpl/ze will use immediate command list for copying
20250508 102239.560 INFO             PET0 index= 263                    MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES : If true, mpl/ze will use command queues in a round-robin fashion. If false, only command queues of index 0 will be used.
20250508 102239.560 INFO             PET0 index= 264                            MPIR_CVAR_NO_COLLECTIVE_FINALIZE : If true, prevent MPI_Finalize to invoke collective behavior such as barrier or communicating to other processes. Consequently, it may result in leaking memory or losing messages due to pre-mature exiting. The default is false, which may invoke collective behaviors at finalize.
20250508 102239.560 INFO             PET0 index= 265                                     MPIR_CVAR_FINALIZE_WAIT : If true, poll progress at MPI_Finalize until reference count on MPI_COMM_WORLD and MPI_COMM_SELF reaches zero. This may be necessary to prevent remote processes hanging if it has pending communication protocols, e.g. a rendezvous send.
20250508 102239.560 INFO             PET0 index= 266                                 MPIR_CVAR_REQUEST_ERR_FATAL : By default, MPI_Waitall, MPI_Testall, MPI_Waitsome, and MPI_Testsome return MPI_ERR_IN_STATUS when one of the request fails. If MPIR_CVAR_REQUEST_ERR_FATAL is set to true, these routines will return the error code of the request immediately. The default MPI_ERRS_ARE_FATAL error handler will dump a error stack in this case, which maybe more convenient for debugging. This cvar will also make nonblocking shched return error right away as it issues operations.
20250508 102239.560 INFO             PET0 index= 267                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during MPI_{Waitany,Waitsome} in terms of number of processed requests before polling.
20250508 102239.560 INFO             PET0 index= 268                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20250508 102239.560 INFO             PET0 index= 269                            MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT : Sets the timeout in seconds to dump outstanding requests when progress wait is not making progress for some time.
20250508 102239.560 INFO             PET0 index= 270                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20250508 102239.560 INFO             PET0 index= 271                                    MPIR_CVAR_QMPI_TOOL_LIST : Set the number and order of QMPI tools to be loaded by the MPI library when it is initialized.
20250508 102239.560 INFO             PET0 index= 272                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20250508 102239.560 INFO             PET0 index= 273                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20250508 102239.560 INFO             PET0 index= 274                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20250508 102239.560 INFO             PET0 index= 275                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20250508 102239.560 INFO             PET0 index= 276                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20250508 102239.560 INFO             PET0 index= 277                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20250508 102239.560 INFO             PET0 index= 278                                  MPIR_CVAR_CLIQUES_BY_BLOCK : Specify to divide processes into cliques by uniform blocks. The default is to divide in round-robin fashion. Used for debugging on a single machine.
20250508 102239.560 INFO             PET0 index= 279                                       MPIR_CVAR_PMI_VERSION : Variable to select runtime PMI version.
1        - PMI (default)
2        - PMI2
x        - PMIx
20250508 102239.560 INFO             PET0 index= 280                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20250508 102239.560 INFO             PET0 index= 281                                        MPIR_CVAR_ENABLE_GPU : Control MPICH GPU support. If set to 0, all GPU support is disabled and we do not query the buffer type internally because we assume no GPU buffer is use.
20250508 102239.560 INFO             PET0 index= 282                               MPIR_CVAR_GPU_HAS_WAIT_KERNEL : If set to 1, avoid allocate allocating GPU registered host buffers for temporary buffers. When stream workq and GPU wait kernels are in use, access APIs for GPU registered memory may cause deadlock.
20250508 102239.560 INFO             PET0 index= 283                               MPIR_CVAR_ENABLE_GPU_REGISTER : Control whether to actually register buffers with the GPU runtime in MPIR_gpu_register_host. This could lower the latency of certain GPU communication at the cost of some amount of GPU memory consumed by the MPI library. By default, registration is enabled.
20250508 102239.560 INFO             PET0 index= 284                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.560 INFO             PET0 index= 285                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20250508 102239.560 INFO             PET0 index= 286                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20250508 102239.560 INFO             PET0 index= 287                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20250508 102239.560 INFO             PET0 index= 288                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20250508 102239.560 INFO             PET0 index= 289                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20250508 102239.560 INFO             PET0 index= 290                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET0 index= 291                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET0 index= 292                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20250508 102239.560 INFO             PET0 index= 293                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20250508 102239.560 INFO             PET0 index= 294                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20250508 102239.560 INFO             PET0 index= 295                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.560 INFO             PET0 index= 296               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20250508 102239.560 INFO             PET0 index= 297                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20250508 102239.560 INFO             PET0 index= 298               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchronization approach.  Change this value if programs fail because they run out of requests or other internal resources
20250508 102239.560 INFO             PET0 index= 299                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be negative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20250508 102239.560 INFO             PET0 index= 300            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation available in the ending synchronization call.
20250508 102239.560 INFO             PET0 index= 301                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20250508 102239.560 INFO             PET0 index= 302                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive value.
20250508 102239.560 INFO             PET0 index= 303                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20250508 102239.560 INFO             PET0 index= 304                                    MPIR_CVAR_CH3_PG_VERBOSE : If set, print the PG state on finalize.
20250508 102239.560 INFO             PET0 index= 305                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET0 index= 306                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET0 index= 307                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET0 index= 308                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET0 index= 309           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediately.  Requires a positive value.
20250508 102239.560 INFO             PET0 index= 310                                  MPIR_CVAR_OFI_USE_PROVIDER : This variable is no longer supported. Use FI_PROVIDER instead to select libfabric providers.
20250508 102239.560 INFO             PET0 index= 311                               MPIR_CVAR_SINGLE_HOST_ENABLED : Set this variable to true to indicate that processes are launched on a single host. The current implication is to avoid the cxi provider to prevent the use of scarce hardware resources.
20250508 102239.560 INFO             PET0 index= 312                    MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE : For long message to be sent using pipeline rather than default RDMA read.
20250508 102239.560 INFO             PET0 index= 313                         MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir                        - Fallback to MPIR collectives
trigger_tree_tagged         - Force triggered ops based Tagged Tree
trigger_tree_rma            - Force triggered ops based RMA Tree
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_OFI_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.560 INFO             PET0 index= 314                                     MPIR_CVAR_OFI_SKIP_IPV6 : Skip IPv6 providers.
20250508 102239.560 INFO             PET0 index= 315                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20250508 102239.560 INFO             PET0 index= 316                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20250508 102239.560 INFO             PET0 index= 317                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20250508 102239.560 INFO             PET0 index= 318                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20250508 102239.560 INFO             PET0 index= 319                    MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS : If true, enable virtual addressing for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.560 INFO             PET0 index= 320                       MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED : If true, require all OFI memory regions must be backed by physical memory pages at the time the registration call is made. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.560 INFO             PET0 index= 321                   MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL : If true, memory registration call supports registering with NULL addresses.
20250508 102239.561 INFO             PET0 index= 322                        MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY : If true, enable provider supplied key for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET0 index= 323                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20250508 102239.561 INFO             PET0 index= 324                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20250508 102239.561 INFO             PET0 index= 325                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20250508 102239.561 INFO             PET0 index= 326                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20250508 102239.561 INFO             PET0 index= 327                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20250508 102239.561 INFO             PET0 index= 328                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20250508 102239.561 INFO             PET0 index= 329              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20250508 102239.561 INFO             PET0 index= 330                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20250508 102239.561 INFO             PET0 index= 331                               MPIR_CVAR_CH4_OFI_ENABLE_HMEM : If true, uses GPU direct RDMA support in the provider.
20250508 102239.561 INFO             PET0 index= 332                            MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM : If true, need to register the buffer to use GPU direct RDMA.
20250508 102239.561 INFO             PET0 index= 333                        MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD : The threshold to start using GPU direct RDMA.
20250508 102239.561 INFO             PET0 index= 334                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET0 index= 335                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET0 index= 336                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET0 index= 337                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET0 index= 338                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET0 index= 339                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20250508 102239.561 INFO             PET0 index= 340                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20250508 102239.561 INFO             PET0 index= 341                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20250508 102239.561 INFO             PET0 index= 342              MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS : Specifies the number of optimized memory regions supported by the provider. An optimized memory region is used for lower-overhead, unordered RMA operations. It uses a low-overhead RX path and additionally, a low-overhead packet format may be used to target an optimized memory region.
20250508 102239.561 INFO             PET0 index= 343                     MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL : Specifies the interval for manually flushing RMA operations when automatic progress is not enabled. It the underlying OFI provider supports auto data progress, this value is ignored. If the value is -1, this optimization will be turned off.
20250508 102239.561 INFO             PET0 index= 344                             MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX : Specifies the maximum number of iovecs to allocate for RMA operations to/from noncontiguous buffers.
20250508 102239.561 INFO             PET0 index= 345                        MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which OFI native path switches from eager to rendezvous mode. It does not affect the AM path eager limit. Having this gives a way to reliably test native non-path. If the number is positive, OFI will init the MPIDI_OFI_global.max_msg_size to the value of cvar. If the number is negative, OFI will init the MPIDI_OFI_globa.max_msg_size using whatever provider gives (which might be unlimited for socket provider).
20250508 102239.561 INFO             PET0 index= 346                                  MPIR_CVAR_CH4_OFI_MAX_NICS : If set to positive number, this cvar determines the maximum number of physical nics to use (if more than one is available). If the number is -1, underlying netmod or shmmod automatically uses an optimal number depending on what is detected on the system up to the limit determined by MPIDI_MAX_NICS (in ofi_types.h).
20250508 102239.561 INFO             PET0 index= 347                 MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING : If true, this cvar enables striping of large messages across multiple NICs.
20250508 102239.561 INFO             PET0 index= 348              MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD : Striping will happen for message sizes beyond this threshold.
20250508 102239.561 INFO             PET0 index= 349                  MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING : Multi-NIC hashing means to use more than one NIC to send and receive messages above a certain size.  If set to positive number, this feature will be turned on. If set to 0, this feature will be turned off. If the number is -1, MPICH automatically determines whether to use multi-nic hashing depending on what is detected on the system (e.g., number of NICs available, number of processes sharing the NICs).
20250508 102239.561 INFO             PET0 index= 350                     MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE : Controls the multirecv am buffer size. It is recommended to match this to the hugepage size so that the buffer can be allocated at the page boundary.
20250508 102239.561 INFO             PET0 index= 351                                  MPIR_CVAR_OFI_USE_MIN_NICS : If true and all nodes do not have the same number of NICs, MPICH will fall back to using the fewest number of NICs instead of returning an error.
20250508 102239.561 INFO             PET0 index= 352                          MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED : If true, enable OFI triggered ops for MPI collectives.
20250508 102239.561 INFO             PET0 index= 353                      MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the sender side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET0 index= 354                   MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the receiver side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET0 index= 355                       MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE : If true, enable pipeline for GPU data transfer. GPU pipeline does not support non-contiguous datatypes or mixed buffer types (i.e. GPU send buffer, host recv buffer). If GPU pipeline is enabled, the unsupported scenarios will cause undefined behavior if encountered.
20250508 102239.561 INFO             PET0 index= 356                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD : This is the threshold to start using GPU pipeline.
20250508 102239.561 INFO             PET0 index= 357                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU pipeline data transfer.
20250508 102239.561 INFO             PET0 index= 358        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU pipeline data transfer in each block/chunk of the pool.
20250508 102239.561 INFO             PET0 index= 359              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU pipeline data transfer
20250508 102239.561 INFO             PET0 index= 360              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the sender side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET0 index= 361              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the receiver side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET0 index= 362                      MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE : Avoid use fi_inject_write. For some provider, e.g. tcp;ofi_rxm, inject write may break the synchronization.
20250508 102239.561 INFO             PET0 index= 363                                       MPIR_CVAR_UCX_DT_RECV : Variable to select method for receiving noncontiguous data
true                - Use UCX datatype with pack/unpack callbacks
false               - MPICH will decide to pack/unpack at completion or use IOVs
based on the datatype
20250508 102239.561 INFO             PET0 index= 364                          MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE : By default, we will cache ipc handles using the specialized cache mechanism. If the
gpu-specific backend does not implement a specialized cache, then we will fallback to
the generic cache mechanism. Users can optionally force the generic cache mechanism or
disable ipc caching entirely.
generic - use the cache mechanism in the generic layer
specialized - use the cache mechanism in a gpu-specific mpl layer (if applicable)
disabled - disable caching completely
20250508 102239.561 INFO             PET0 index= 365                         MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD (in bytes), then enable GPU-based single copy protocol for intranode communication. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET0 index= 366                    MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE : If a send message size is less than or equal to MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE (in bytes), then enable GPU-basedfast memcpy. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET0 index= 367                       MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE : Variable to select implementation for ZE shareable IPC handle
pidfd - use pidfd_getfd syscall to implement shareable IPC handle
drmfd - force to use device fd-based shareable IPC handle
20250508 102239.561 INFO             PET0 index= 368                           MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE : By default, select engine type automatically
auto - select automatically
compute - use compute engine
copy_high_bandwidth - use high-bandwidth copy engine
copy_low_latency - use low-latency copy engine
20250508 102239.561 INFO             PET0 index= 369                   MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL : By default, use read protocol.
auto - select automatically
read - use read protocol
write - use write protocol if remote device is visible
20250508 102239.561 INFO             PET0 index= 370                           MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR : If an address is used more than once in the last ten send operations, map it for IPC use even if it is below the IPC threshold.
20250508 102239.561 INFO             PET0 index= 371                                  MPIR_CVAR_CH4_XPMEM_ENABLE : To manually disable XPMEM set to 0. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET0 index= 372                       MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD (in bytes), then enable XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET0 index= 373                       MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
ipc_read - Uses read-based collective with ipc
20250508 102239.561 INFO             PET0 index= 374                      MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET0 index= 375                      MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET0 index= 376                     MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET0 index= 377                   MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node allreduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET0 index= 378                     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node barrier
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET0 index= 379                    MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node alltoall
mpir           - Fallback to MPIR collectives (default)
ipc_read    - Uses read-based collective with ipc
20250508 102239.561 INFO             PET0 index= 380                              MPIR_CVAR_POSIX_POLL_FREQUENCY : This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.561 INFO             PET0 index= 381                 MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read bcast only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET0 index= 382              MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read alltoall only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET0 index= 383                         MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD : Use posix optimized collectives (release_gather) only when the total number of Bcast, Reduce, Barrier, and Allreduce calls on the node level communicator is more than this threshold.
20250508 102239.561 INFO             PET0 index= 384                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : If non-empty, this cvar specifies which shm posix eager module to use
20250508 102239.561 INFO             PET0 index= 385         MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.561 INFO             PET0 index= 386                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS : The number of cells used for the depth of the iqueue.
20250508 102239.561 INFO             PET0 index= 387                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE : Size of each cell.
20250508 102239.561 INFO             PET0 index= 388                           MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE : Maximum shared memory created per node for optimized intra-node collectives (in KB)
20250508 102239.561 INFO             PET0 index= 389                 MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the bcast buffer (in bytes)
20250508 102239.561 INFO             PET0 index= 390                         MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS : Number of cells the bcast buffer is divided into
20250508 102239.561 INFO             PET0 index= 391                MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the reduce buffer per rank (in bytes)
20250508 102239.561 INFO             PET0 index= 392                        MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS : Number of cells the reduce buffer is divided into, for each rank
20250508 102239.561 INFO             PET0 index= 393                         MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node bcast
20250508 102239.561 INFO             PET0 index= 394                         MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE : Tree type for intra-node bcast tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET0 index= 395                        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node reduce
20250508 102239.561 INFO             PET0 index= 396                        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE : Tree type for intra-node reduce tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET0 index= 397             MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES : Enable collective specific intra-node trees which leverage the memory hierarchy of a machine. Depends on hwloc to extract the binding information of each rank. Pick a leader rank per package (socket), then create a per_package tree for ranks on a same package, package leaders tree for package leaders. For Bcast - Assemble the per_package and package_leaders tree in such a way that leaders interact among themselves first before interacting with package local ranks. Both the package_leaders and per_package trees are left skewed (children are added from left to right, first child to be added is the first one to be processed in traversal) For Reduce - Assemble the per_package and package_leaders tree in such a way that a leader rank interacts with its package local ranks first, then with the other package leaders. Both the per_package and package_leaders tree is right skewed (children are added in reverse order, first child to be added is the last one to be processed in traversal) The tree radix and tree type of
20250508 102239.561 INFO             PET0 index= 398                               MPIR_CVAR_BARRIER_COMPOSITION : Select composition (inter_node + intra_node) for Barrier 0 Auto selection 1 NM + SHM 2 NM only
20250508 102239.561 INFO             PET0 index= 399                                 MPIR_CVAR_BCAST_COMPOSITION : Select composition (inter_node + intra_node) for Bcast 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.561 INFO             PET0 index= 400                             MPIR_CVAR_ALLREDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Allreduce 0 Auto selection 1 NM + SHM with reduce + bcast 2 NM only composition 3 SHM only composition 4 Multi leaders based inter node + intra node composition
20250508 102239.561 INFO             PET0 index= 401                             MPIR_CVAR_ALLGATHER_COMPOSITION : Select composition (inter_node + intra_node) for Allgather 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.561 INFO             PET0 index= 402                              MPIR_CVAR_ALLTOALL_COMPOSITION : Select composition (inter_node + intra_node) for Alltoall 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.561 INFO             PET0 index= 403                                MPIR_CVAR_REDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Reduce 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.561 INFO             PET0 index= 404                             MPIR_CVAR_ALLTOALL_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Alltoall (in bytes)
20250508 102239.561 INFO             PET0 index= 405                            MPIR_CVAR_ALLGATHER_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Allgather (in bytes)
20250508 102239.561 INFO             PET0 index= 406                                   MPIR_CVAR_NUM_MULTI_LEADS : Number of leader ranks per node to be used for multi-leaders based collective algorithms
20250508 102239.561 INFO             PET0 index= 407                          MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER : Shared memory region per node-leader for multi-leaders based composition for MPI_Allreduce (in bytes) If it is undefined by the user, it is set to the message size of the first call to the algorithm. Max shared memory size is limited to 4MB.
20250508 102239.561 INFO             PET0 index= 408                        MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER : Amount of data reduced in allreduce delta composition's reduce local step (in bytes). Smaller msg size per leader avoids cache misses and improves performance. Experiments indicate 512 to be the best value.
20250508 102239.561 INFO             PET0 index= 409                      MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS : number of offsets in the allreduce delta composition's local copy The value of 2 performed the best in our 2 NIC test cases.
20250508 102239.561 INFO             PET0 index= 410                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20250508 102239.561 INFO             PET0 index= 411                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20250508 102239.562 INFO             PET0 index= 412                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20250508 102239.562 INFO             PET0 index= 413                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20250508 102239.562 INFO             PET0 index= 414                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) lockless
20250508 102239.562 INFO             PET0 index= 415                                      MPIR_CVAR_CH4_NUM_VCIS : Sets the number of VCIs to be implicitly used (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET0 index= 416                                  MPIR_CVAR_CH4_RESERVE_VCIS : Sets the number of VCIs that user can explicitly allocate (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET0 index= 417               MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.562 INFO             PET0 index= 418                               MPIR_CVAR_CH4_IOV_DENSITY_MIN : Defines the threshold of high-density datatype. The density is calculated by (datatype_size / datatype_num_contig_blocks).
20250508 102239.562 INFO             PET0 index= 419                              MPIR_CVAR_CH4_PACK_BUFFER_SIZE : Specifies the number of buffers for packing/unpacking active messages in each block of the pool. The size here should be greater or equal to the max of the eager buffer limit of SHM and NETMOD.
20250508 102239.562 INFO             PET0 index= 420                    MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK : Specifies the number of buffers for packing/unpacking active messages in each block of the pool.
20250508 102239.562 INFO             PET0 index= 421                          MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS : Specifies the max number of buffers for packing/unpacking buffers in the pool. Use 0 for unlimited.
20250508 102239.562 INFO             PET0 index= 422                       MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU collectives data transfer.
20250508 102239.562 INFO             PET0 index= 423                MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU collectives data transfer in each block/chunk of the pool.
20250508 102239.562 INFO             PET0 index= 424                      MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU collectives data transfer.
20250508 102239.562 INFO             PET0 index= 425                               MPIR_CVAR_CH4_GLOBAL_PROGRESS : If on, poll global progress every once a while. With per-vci configuration, turning global progress off may improve the threading performance.
20250508 102239.562 INFO             PET0 index= 426                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.562 INFO             PET0 index= 427                           MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ : Enable stream enqueue operations via stream work queue. Requires progress thread on the corresponding MPIX stream. Reference: MPIX_Stream_progress and MPIX_Start_progress_thread.
20250508 102239.562 INFO             PET0 index= 428                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20250508 102239.562 INFO             PET0 index= 429                MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS : If true, allows RMA synchronization calls to dynamically reduce the frequency of internal progress polling for incoming RMA active messages received on the target process. The RMA synchronization call initially polls progress with a low frequency (defined by MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL) to reduce synchronization overhead. Once any RMA active message has been received, it will always poll progress once at every synchronization call to ensure prompt target-side progress. Effective only for passive target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}.
20250508 102239.562 INFO             PET0 index= 430                      MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL : Specifies a static interval of progress polling for incoming RMA active messages received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Invalid when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET0 index= 431             MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL : Specifies the interval of progress polling with low frequency for incoming RMA active message received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Used when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET0 index= 432            MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE : The genq shmem code allocates pools of cells on each process and, when needed, a cell is removed from the pool and passed to another process. This can happen by either removing a cell from the pool of the sending process or from the pool of the receiving process. This CVAR determines which pool to use. If true, the cell will come from the sender-side. If false, the cell will com from the receiver-side. There are specific advantages of using receiver-side cells when combined with the "avx" fast configure option, which allows MPICH to use AVX streaming copy intrintrinsics, when available, to avoid polluting the cache of the sender with the data being copied to the receiver. Using receiver-side cells does have the trade-off of requiring an MPMC lock for the free queue rather than an MPSC lock, which is used for sender-side cells. Initial performance analysis shows that using the MPMC lock in this case had no significant performance loss. By default, the queue will continue to use sender-side queues until the pe
20250508 102239.562 INFO             PET0 index= 433                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20250508 102239.562 INFO             PET0 index= 434                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20250508 102239.562 INFO             PET0 index= 435                             MPIR_CVAR_SHM_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20250508 102239.562 INFO             PET0 index= 436                                 MPIR_CVAR_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20250508 102239.562 INFO             PET0 index= 437                                MPIR_CVAR_ENABLE_HEAVY_YIELD : If enabled, use nanosleep to ensure other threads have a chance to grab the lock. Note: this may not work with some thread runtimes, e.g. non-preemptive user-level threads.
20250508 102239.562 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20250508 102239.562 INFO             PET0 main: --- VMK::log() start -------------------------------------
20250508 102239.562 INFO             PET0 main: vm located at: 0x127606570
20250508 102239.562 INFO             PET0 main: mpionly=1 threadsflag=0
20250508 102239.562 INFO             PET0 main: ssiCount=1 localSsi=0
20250508 102239.562 INFO             PET0 main: devCount=0 ssiLocalDevCount=0
20250508 102239.562 INFO             PET0 main: petCount=6 ssiLocalPetCount=6
20250508 102239.562 INFO             PET0 main: localPet=0 mypthid=0x1edb24f40 ssiLocalPet=0 currentSsiPe=-1
20250508 102239.562 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.562 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.562 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20250508 102239.562 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.562 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20250508 102239.562 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.562 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20250508 102239.562 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.562 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20250508 102239.562 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.562 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20250508 102239.562 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.562 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20250508 102239.562 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20250508 102239.562 INFO             PET0 Executing 'userm1_setvm'
20250508 102239.563 INFO             PET0 Executing 'userm1_register'
20250508 102239.563 INFO             PET0 Executing 'userm2_setvm'
20250508 102239.563 INFO             PET0 Executing 'userm2_register'
20250508 102239.563 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20250508 102239.563 INFO             PET0 model1: vm located at: 0x127607710
20250508 102239.563 INFO             PET0 model1: mpionly=1 threadsflag=0
20250508 102239.563 INFO             PET0 model1: ssiCount=1 localSsi=0
20250508 102239.563 INFO             PET0 model1: devCount=0 ssiLocalDevCount=0
20250508 102239.563 INFO             PET0 model1: petCount=6 ssiLocalPetCount=6
20250508 102239.563 INFO             PET0 model1: localPet=0 mypthid=0x1edb24f40 ssiLocalPet=0 currentSsiPe=-1
20250508 102239.563 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.563 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.563 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20250508 102239.563 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.563 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20250508 102239.563 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.563 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20250508 102239.563 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.563 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20250508 102239.563 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.563 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20250508 102239.563 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.563 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20250508 102239.563 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20250508 102239.568 INFO             PET0 Entering 'user1_run'
20250508 102239.568 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20250508 102239.838 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20250508 102240.072 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20250508 102240.296 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20250508 102240.524 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20250508 102240.748 INFO             PET0 Exiting 'user1_run'
20250508 102240.756 INFO             PET0 Entering 'user2_run'
20250508 102240.756 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20250508 102240.756 INFO             PET0 model2: vm located at: 0x1276088b0
20250508 102240.756 INFO             PET0 model2: mpionly=0 threadsflag=0
20250508 102240.756 INFO             PET0 model2: ssiCount=1 localSsi=0
20250508 102240.756 INFO             PET0 model2: devCount=0 ssiLocalDevCount=0
20250508 102240.756 INFO             PET0 model2: petCount=1 ssiLocalPetCount=1
20250508 102240.756 INFO             PET0 model2: localPet=0 mypthid=0x1edb24f40 ssiLocalPet=0 currentSsiPe=-1
20250508 102240.756 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102240.756 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=6 accCount=0
20250508 102240.756 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20250508 102240.756 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20250508 102240.756 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20250508 102240.756 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20250508 102240.756 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20250508 102240.756 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20250508 102240.756 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20250508 102240.756 INFO             PET0  user2_run: ssiLocalDeCount=           6
20250508 102240.757 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20250508 102241.028 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102241.314 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102241.598 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102241.880 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102242.164 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102242.445 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20250508 102242.714 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102242.994 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102243.279 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102243.562 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102243.842 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102244.124 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20250508 102244.402 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102244.680 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102244.958 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102245.237 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102245.514 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102245.791 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20250508 102246.059 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102246.329 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102246.599 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102246.870 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102247.141 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102247.410 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20250508 102247.678 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102247.947 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102248.215 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102248.488 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102248.764 INFO             PET0  user2_run: OpenMP thread:   124413816  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102249.038 INFO             PET0  user2_run: All data correct.
20250508 102249.038 INFO             PET0 Exiting 'user2_run'
20250508 102249.060 INFO             PET0  NUMBER_OF_PROCESSORS           6
20250508 102249.061 INFO             PET0  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20250508 102249.061 INFO             PET0  Finalizing ESMF
20250508 102249.061 INFO             PET0 ESMCI_IO_Handler.C:335 ESMCI::IO_Handler::finalize() 
20250508 102249.061 INFO             PET0 ESMCI_PIO_Handler.C:357 ESMCI::PIO_Handler::finalize() 
20250508 102249.061 INFO             PET0 ESMCI_IO_Handler.C:337 ESMCI::IO_Handler::finalize() after finalize, localrc = 0
20250508 102249.061 INFO             PET0 ESMCI_IO_Handler.C:360 ESMCI::IO_Handler::finalize() before return, localrc = 0
20250508 102239.555 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20250508 102239.556 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20250508 102239.556 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20250508 102239.556 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20250508 102239.556 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET1 Running with ESMF Version   : v8.9.0b08-5-g079ca68fdc
20250508 102239.556 INFO             PET1 ESMF library build date/time: "May  8 2025" "10:06:33"
20250508 102239.556 INFO             PET1 ESMF library build location : /Users/oehmke/ESMF_AutoTest/gfortranclang_12.2.0_14.0.0_mpich_g_develop/esmf
20250508 102239.556 INFO             PET1 ESMF_COMM                   : mpich
20250508 102239.556 INFO             PET1 ESMF_MOAB                   : enabled
20250508 102239.556 INFO             PET1 ESMF_LAPACK                 : enabled
20250508 102239.556 INFO             PET1 ESMF_NETCDF                 : enabled
20250508 102239.556 INFO             PET1 ESMF_PNETCDF                : disabled
20250508 102239.556 INFO             PET1 ESMF_PIO                    : enabled
20250508 102239.556 INFO             PET1 ESMF_YAMLCPP                : enabled
20250508 102239.556 INFO             PET1 --- VMK::logSystem() start -------------------------------
20250508 102239.556 INFO             PET1 esmfComm=mpich
20250508 102239.556 INFO             PET1 isPthreadsEnabled=1
20250508 102239.556 INFO             PET1 isOpenMPEnabled=0
20250508 102239.557 INFO             PET1 isOpenACCEnabled=0
20250508 102239.557 INFO             PET1 isSsiSharedMemoryEnabled=1
20250508 102239.557 INFO             PET1 isNvmlEnabled=0
20250508 102239.557 INFO             PET1 isNumaEnabled=0
20250508 102239.557 INFO             PET1 ssiCount=1 peCount=6
20250508 102239.557 INFO             PET1 PE=0 SSI=0 SSIPE=0
20250508 102239.557 INFO             PET1 PE=1 SSI=0 SSIPE=1
20250508 102239.557 INFO             PET1 PE=2 SSI=0 SSIPE=2
20250508 102239.557 INFO             PET1 PE=3 SSI=0 SSIPE=3
20250508 102239.557 INFO             PET1 PE=4 SSI=0 SSIPE=4
20250508 102239.557 INFO             PET1 PE=5 SSI=0 SSIPE=5
20250508 102239.557 INFO             PET1 ndevs=0 ndevsSSI=0
20250508 102239.557 INFO             PET1 
20250508 102239.557 INFO             PET1 --- VMK::logSystem() MPI Layer ---------------------------
20250508 102239.557 INFO             PET1 MPI_VERSION=4
20250508 102239.557 INFO             PET1 MPI_SUBVERSION=1
20250508 102239.557 INFO             PET1 MPICH_VERSION=4.2.3
20250508 102239.557 INFO             PET1 mpi_t_okay=1
20250508 102239.557 INFO             PET1 --- VMK::logSystem() MPI Tool Interface Control Vars ---
20250508 102239.557 INFO             PET1 index=   0                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb            - Force nonblocking algorithm
smp           - Force smp algorithm
k_dissemination - Force high radix dissemination algorithm
recexch       - Force recursive exchange algorithm
20250508 102239.557 INFO             PET1 index=   1                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
bcast - Force bcast algorithm
nb    - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=   2                               MPIR_CVAR_BARRIER_DISSEM_KVAL : k value for dissemination exchange based barrier algorithm
20250508 102239.557 INFO             PET1 index=   3                              MPIR_CVAR_BARRIER_RECEXCH_KVAL : k value for recursive exchange based allreduce based barrier
20250508 102239.557 INFO             PET1 index=   4                 MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.557 INFO             PET1 index=   5                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20250508 102239.557 INFO             PET1 index=   6                              MPIR_CVAR_IBARRIER_DISSEM_KVAL : k value for dissemination exchange based ibarrier
20250508 102239.557 INFO             PET1 index=   7                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
tsp_recexch - Force generic transport based recursive exchange algorithm
tsp_k_dissemination - Force generic transport based high-radix dissemination algorithm
20250508 102239.557 INFO             PET1 index=   8                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_bcast - Force bcast algorithm
20250508 102239.557 INFO             PET1 index=   9                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET1 index=  10                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET1 index=  11                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET1 index=  12                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes.
20250508 102239.557 INFO             PET1 index=  13                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial                                - Force Binomial Tree
nb                                      - Force nonblocking algorithm
smp                                     - Force smp algorithm
scatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgather
scatter_ring_allgather                  - Force Scatter Ring
pipelined_tree                          - Force tree-based pipelined algorithm
tree                                    - Force tree-based algorithm
20250508 102239.557 INFO             PET1 index=  14                                   MPIR_CVAR_BCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based bcast
20250508 102239.557 INFO             PET1 index=  15                                   MPIR_CVAR_BCAST_TREE_TYPE : Tree type for tree based bcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.557 INFO             PET1 index=  16                         MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.557 INFO             PET1 index=  17                               MPIR_CVAR_BCAST_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.557 INFO             PET1 index=  18                            MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.557 INFO             PET1 index=  19                          MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.557 INFO             PET1 index=  20                          MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.557 INFO             PET1 index=  21                             MPIR_CVAR_BCAST_IS_NON_BLOCKING : If set to true, MPI_Bcast will use non-blocking send.
20250508 102239.557 INFO             PET1 index=  22                    MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE : Indicates the chunk size for pipelined bcast.
20250508 102239.557 INFO             PET1 index=  23                               MPIR_CVAR_BCAST_RECV_PRE_POST : If set to true, MPI_Bcast will pre-post all the receives.
20250508 102239.557 INFO             PET1 index=  24                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                      - Force nonblocking algorithm
remote_send_local_bcast - Force remote-send-local-bcast algorithm
20250508 102239.557 INFO             PET1 index=  25                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20250508 102239.557 INFO             PET1 index=  26                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.557 INFO             PET1 index=  27                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET1 index=  28                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET1 index=  29                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial                             - Force Binomial algorithm
sched_smp                                  - Force smp algorithm
sched_scatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithm
sched_scatter_ring_allgather               - Force Scatter Ring Allgather algorithm
tsp_tree                               - Force Generic Transport Tree algorithm
tsp_scatterv_recexch_allgatherv        - Force Generic Transport Scatterv followed by Recursive Exchange Allgatherv algorithm
tsp_scatterv_ring_allgatherv           - Force Generic Transport Scatterv followed by Ring Allgatherv algorithm
tsp_ring                               - Force Generic Transport Ring algorithm
20250508 102239.557 INFO             PET1 index=  30                              MPIR_CVAR_IBCAST_SCATTERV_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET1 index=  31                    MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET1 index=  32                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_flat - Force flat algorithm
20250508 102239.557 INFO             PET1 index=  33                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20250508 102239.557 INFO             PET1 index=  34                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=  35                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                   - Force linear algorithm
local_gather_remote_send - Force local-gather-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=  36                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET1 index=  37                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20250508 102239.557 INFO             PET1 index=  38                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_long  - Force long inter algorithm
sched_short - Force short inter algorithm
20250508 102239.557 INFO             PET1 index=  39                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=  40                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=  41                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear         - Force linear algorithm
tsp_linear       - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET1 index=  42                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET1 index=  43                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20250508 102239.557 INFO             PET1 index=  44                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=  45                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                    - Force linear algorithm
nb                        - Force nonblocking algorithm
remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET1 index=  46                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET1 index=  47                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20250508 102239.557 INFO             PET1 index=  48                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear                    - Force linear algorithm
sched_remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET1 index=  49                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=  50                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET1 index=  51                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET1 index=  52                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET1 index=  53                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET1 index=  54                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET1 index=  55                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
k_brucks           - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
recexch_doubling   - Force recexch distance doubling algorithm
recexch_halving    - Force recexch distance halving algorithm
20250508 102239.558 INFO             PET1 index=  56                             MPIR_CVAR_ALLGATHER_BRUCKS_KVAL : radix (k) value for generic transport brucks based allgather
20250508 102239.558 INFO             PET1 index=  57                            MPIR_CVAR_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather
20250508 102239.558 INFO             PET1 index=  58               MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET1 index=  59                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
nb                        - Force nonblocking algorithm
20250508 102239.558 INFO             PET1 index=  60                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20250508 102239.558 INFO             PET1 index=  61                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20250508 102239.558 INFO             PET1 index=  62                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_ring               - Force ring algorithm
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
tsp_ring       - Force generic transport ring algorithm
tsp_brucks     - Force generic transport based brucks algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
20250508 102239.558 INFO             PET1 index=  63                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20250508 102239.558 INFO             PET1 index=  64                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20250508 102239.558 INFO             PET1 index=  65                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
20250508 102239.558 INFO             PET1 index=  66                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET1 index=  67                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20250508 102239.558 INFO             PET1 index=  68                           MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL : k value for radix in brucks based iallgatherv
20250508 102239.558 INFO             PET1 index=  69                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_ring               - Force ring algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
tsp_ring             - Force generic transport ring algorithm
tsp_brucks           - Force generic transport based brucks algorithm
20250508 102239.558 INFO             PET1 index=  70                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET1 index=  71                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20250508 102239.558 INFO             PET1 index=  72                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20250508 102239.558 INFO             PET1 index=  73                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20250508 102239.558 INFO             PET1 index=  74                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks                    - Force brucks algorithm
k_brucks                  - Force Force radix k brucks algorithm
nb                        - Force nonblocking algorithm
pairwise                  - Force pairwise algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET1 index=  75                              MPIR_CVAR_ALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based alltoall
20250508 102239.558 INFO             PET1 index=  76                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET1 index=  77                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks            - Force brucks algorithm
sched_inplace           - Force inplace algorithm
sched_pairwise          - Force pairwise algorithm
sched_permuted_sendrecv - Force permuted sendrecv algorithm
tsp_ring            - Force generic transport based ring algorithm
tsp_brucks          - Force generic transport based brucks algorithm
tsp_scattered       - Force generic transport based scattered algorithm
20250508 102239.558 INFO             PET1 index=  78                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET1 index=  79                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET1 index=  80                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
pairwise_exchange - Force pairwise exchange algorithm
nb                - Force nonblocking algorithm
20250508 102239.558 INFO             PET1 index=  81                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_scattered       - Force generic transport based scattered algorithm
tsp_blocked         - Force generic transport blocked algorithm
tsp_inplace         - Force generic transport inplace algorithm
20250508 102239.558 INFO             PET1 index=  82                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET1 index=  83            MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.558 INFO             PET1 index=  84                   MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.558 INFO             PET1 index=  85                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET1 index=  86                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET1 index=  87                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_blocked   - Force generic transport based blocked algorithm
tsp_inplace   - Force generic transport based inplace algorithm
20250508 102239.558 INFO             PET1 index=  88                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET1 index=  89                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET1 index=  90                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20250508 102239.558 INFO             PET1 index=  91                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial              - Force binomial algorithm
nb                    - Force nonblocking algorithm
smp                   - Force smp algorithm
reduce_scatter_gather - Force reduce scatter gather algorithm
20250508 102239.558 INFO             PET1 index=  92                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_reduce_remote_send - Force local-reduce-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.558 INFO             PET1 index=  93                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20250508 102239.558 INFO             PET1 index=  94                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET1 index=  95                       MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET1 index=  96                             MPIR_CVAR_IREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET1 index=  97                          MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET1 index=  98                        MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET1 index=  99                        MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET1 index= 100                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET1 index= 101                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET1 index= 102                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET1 index= 103                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                   - Force smp algorithm
sched_binomial              - Force binomial algorithm
sched_reduce_scatter_gather - Force reduce scatter gather algorithm
tsp_tree                - Force Generic Transport Tree
tsp_ring                - Force Generic Transport Ring
20250508 102239.558 INFO             PET1 index= 104                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_reduce_remote_send - Force local-reduce-remote-send algorithm
20250508 102239.558 INFO             PET1 index= 105                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET1 index= 106                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20250508 102239.558 INFO             PET1 index= 107                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                       - Force nonblocking algorithm
smp                      - Force smp algorithm
recursive_doubling       - Force recursive doubling algorithm
reduce_scatter_allgather - Force reduce scatter allgather algorithm
tree                     - Force pipelined tree algorithm
recexch                  - Force generic transport recursive exchange algorithm
ring                     - Force ring algorithm
k_reduce_scatter_allgather - Force reduce scatter allgather algorithm
20250508 102239.558 INFO             PET1 index= 108                               MPIR_CVAR_ALLREDUCE_TREE_TYPE : Tree type for tree based allreduce knomial_1 is default as it supports both commutative and non-commutative reduce operations kary      - kary tree type knomial_1 - knomial_1 tree type (tree grows starting from the left of the root) knomial_2 - knomial_2 tree type (tree grows starting from the right of the root) topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET1 index= 109                               MPIR_CVAR_ALLREDUCE_TREE_KVAL : Indicates the branching factor for kary or knomial trees.
20250508 102239.558 INFO             PET1 index= 110                     MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET1 index= 111                           MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET1 index= 112                        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET1 index= 113                      MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET1 index= 114                      MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET1 index= 115                MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based allreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET1 index= 116                   MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET1 index= 117                            MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based allreduce
20250508 102239.558 INFO             PET1 index= 118               MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET1 index= 119                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                    - Force nonblocking algorithm
reduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20250508 102239.558 INFO             PET1 index= 120                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20250508 102239.558 INFO             PET1 index= 121                              MPIR_CVAR_IALLREDUCE_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.558 INFO             PET1 index= 122               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET1 index= 123                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET1 index= 124                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20250508 102239.558 INFO             PET1 index= 125                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_naive                      - Force naive algorithm
sched_smp                        - Force smp algorithm
sched_recursive_doubling         - Force recursive doubling algorithm
sched_reduce_scatter_allgather   - Force reduce scatter allgather algorithm
tsp_recexch_single_buffer    - Force generic transport recursive exchange with single buffer for receives
tsp_recexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
tsp_tree                     - Force generic transport tree algorithm
tsp_ring                     - Force generic transport ring algorithm
tsp_recexch_reduce_scatter_recexch_allgatherv  - Force generic transport recursive exchange with reduce scatter and allgatherv
20250508 102239.558 INFO             PET1 index= 126                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20250508 102239.558 INFO             PET1 index= 127          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20250508 102239.558 INFO             PET1 index= 128                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
noncommutative     - Force noncommutative algorithm
pairwise           - Force pairwise algorithm
recursive_doubling - Force recursive doubling algorithm
recursive_halving  - Force recursive halving algorithm
20250508 102239.558 INFO             PET1 index= 129                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET1 index= 130                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20250508 102239.558 INFO             PET1 index= 131                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET1 index= 132                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET1 index= 133              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
noncommutative     - Force noncommutative algorithm
recursive_doubling - Force recursive doubling algorithm
pairwise           - Force pairwise algorithm
recursive_halving  - Force recursive halving algorithm
nb                 - Force nonblocking algorithm
20250508 102239.558 INFO             PET1 index= 134              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET1 index= 135                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20250508 102239.558 INFO             PET1 index= 136             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET1 index= 137             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET1 index= 138                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
smp                - Force smp algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET1 index= 139                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                  - Force smp algorithm
sched_recursive_doubling   - Force recursive doubling algorithm
tsp_recursive_doubling - Force generic transport recursive doubling algorithm
20250508 102239.558 INFO             PET1 index= 140                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET1 index= 141                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET1 index= 142                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET1 index= 143                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET1 index= 144               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 145               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 146               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 147               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 148              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 149              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 150                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 151                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 152                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 153                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 154                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 155                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 156               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 157               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 158                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 159                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET1 index= 160               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 161               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET1 index= 162                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 163                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 164                    MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 165                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 166                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 167                      MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 168                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 169                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 170                     MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 171                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 172                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 173                    MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 174                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 175                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 176                    MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 177                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 178                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 179                   MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 180                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 181                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 182                  MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 183                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 184                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 185                 MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 186                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 187                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 188                   MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 189                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 190                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 191                  MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 192                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 193                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 194                  MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 195                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 196                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 197                     MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 198                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 199                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 200                  MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 201                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 202                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 203             MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 204            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 205           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 206       MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 207                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 208                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 209                       MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 210                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 211                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 212                     MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 213              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 214             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 215         MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 216             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 217            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 218        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 219               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 220              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 221          MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 222              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 223             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 224         MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET1 index= 225              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET1 index= 226             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET1 index= 227         MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET1 index= 228                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20250508 102239.560 INFO             PET1 index= 229                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20250508 102239.560 INFO             PET1 index= 230                             MPIR_CVAR_IALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based ialltoall
20250508 102239.560 INFO             PET1 index= 231                   MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR : If set to true, the tsp based brucks algorithm will allocate dedicated send and receive buffers for every neighbor in the brucks algorithm. Otherwise, it would reuse a single buffer for sending and receiving data to/from neighbors
20250508 102239.560 INFO             PET1 index= 232             MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.560 INFO             PET1 index= 233                    MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.560 INFO             PET1 index= 234                                MPIR_CVAR_DEVICE_COLLECTIVES : Variable to select whether the device can override the
MPIR-level collective algorithms.
all     - Always prefer the device collectives
none    - Never pick the device collectives
percoll - Use the per-collective CVARs to decide
20250508 102239.560 INFO             PET1 index= 235                               MPIR_CVAR_COLLECTIVE_FALLBACK : Variable to control what the MPI library should do if the
user-specified collective algorithm does not work for the
arguments passed in by the user.
error   - throw an error
print   - print an error message and fallback to the internally selected algorithm
silent  - silently fallback to the internally selected algorithm
20250508 102239.560 INFO             PET1 index= 236                   MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.560 INFO             PET1 index= 237                                    MPIR_CVAR_HIERARCHY_DUMP : If set to true, each rank will dump the hierarchy data structure to a file named "hierarchy[rank]" in the current folder. If set to false, the hierarchy data structure will not be dumped.
20250508 102239.560 INFO             PET1 index= 238                                  MPIR_CVAR_COORDINATES_FILE : Defines the location of the input coordinates file.
20250508 102239.560 INFO             PET1 index= 239                                    MPIR_CVAR_COLL_TREE_DUMP : If set to true, each rank will dump the tree to a file named "colltree[rank].json" in the current folder. If set to false, the tree will not be dumped.
20250508 102239.560 INFO             PET1 index= 240                                  MPIR_CVAR_COORDINATES_DUMP : If set to true, rank 0 will dump the network coordinates to a file named "coords" in the current folder. If set to false, the network coordinates will not be dumped.
20250508 102239.560 INFO             PET1 index= 241                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20250508 102239.560 INFO             PET1 index= 242                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20250508 102239.560 INFO             PET1 index= 243                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20250508 102239.560 INFO             PET1 index= 244                                MPIR_CVAR_DATALOOP_FAST_SEEK : use a datatype-specialized algorithm to shortcut seeking to the correct location in a noncontiguous buffer
20250508 102239.560 INFO             PET1 index= 245                             MPIR_CVAR_YAKSA_COMPLEX_SUPPORT : This CVAR indicates that complex type reduction is not supported in yaksa.
20250508 102239.560 INFO             PET1 index= 246                                MPIR_CVAR_GPU_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET1 index= 247                           MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET1 index= 248                            MPIR_CVAR_ENABLE_YAKSA_REDUCTION : This cvar enables yaksa based reduction for local reduce.
20250508 102239.560 INFO             PET1 index= 249                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20250508 102239.560 INFO             PET1 index= 250                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20250508 102239.560 INFO             PET1 index= 251                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20250508 102239.560 INFO             PET1 index= 252                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20250508 102239.560 INFO             PET1 index= 253                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20250508 102239.560 INFO             PET1 index= 254                          MPIR_CVAR_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. Can be set to auto or comma-separated list of logical processors. When set to auto - MPICH will automatically select logical CPU cores to decide affinity of the progress threads. When set to comma-separated list of logical processors - In case of N progress threads per process, the first N logical processors from list will be assigned to threads of first local process, the next N logical processors from list - to second local process and so on. For example, thread affinity is "0,1,2,3", 2 progress threads per process and 2 processes per node. Progress threads of first local process will be pinned on logical processors "0,1", progress threads of second local process - on "2,3". Cannot work together with MPIR_CVAR_NUM_CLIQUES or MPIR_CVAR_ODD_EVEN_CLIQUES.
20250508 102239.560 INFO             PET1 index= 255                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20250508 102239.560 INFO             PET1 index= 256                                 MPIR_CVAR_COREDUMP_ON_ABORT : Call libc abort() to generate a corefile
20250508 102239.560 INFO             PET1 index= 257                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20250508 102239.560 INFO             PET1 index= 258                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20250508 102239.560 INFO             PET1 index= 259                                     MPIR_CVAR_DEBUG_SUMMARY : If true, print internal summary of various debug information, such as memory allocation by category. Each layer may print their own summary information. For example, ch4-ofi may print its provider capability settings.
20250508 102239.560 INFO             PET1 index= 260                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20250508 102239.560 INFO             PET1 index= 261                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20250508 102239.560 INFO             PET1 index= 262                    MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST : If true, mpl/ze will use immediate command list for copying
20250508 102239.560 INFO             PET1 index= 263                    MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES : If true, mpl/ze will use command queues in a round-robin fashion. If false, only command queues of index 0 will be used.
20250508 102239.560 INFO             PET1 index= 264                            MPIR_CVAR_NO_COLLECTIVE_FINALIZE : If true, prevent MPI_Finalize to invoke collective behavior such as barrier or communicating to other processes. Consequently, it may result in leaking memory or losing messages due to pre-mature exiting. The default is false, which may invoke collective behaviors at finalize.
20250508 102239.560 INFO             PET1 index= 265                                     MPIR_CVAR_FINALIZE_WAIT : If true, poll progress at MPI_Finalize until reference count on MPI_COMM_WORLD and MPI_COMM_SELF reaches zero. This may be necessary to prevent remote processes hanging if it has pending communication protocols, e.g. a rendezvous send.
20250508 102239.560 INFO             PET1 index= 266                                 MPIR_CVAR_REQUEST_ERR_FATAL : By default, MPI_Waitall, MPI_Testall, MPI_Waitsome, and MPI_Testsome return MPI_ERR_IN_STATUS when one of the request fails. If MPIR_CVAR_REQUEST_ERR_FATAL is set to true, these routines will return the error code of the request immediately. The default MPI_ERRS_ARE_FATAL error handler will dump a error stack in this case, which maybe more convenient for debugging. This cvar will also make nonblocking shched return error right away as it issues operations.
20250508 102239.560 INFO             PET1 index= 267                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during MPI_{Waitany,Waitsome} in terms of number of processed requests before polling.
20250508 102239.560 INFO             PET1 index= 268                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20250508 102239.560 INFO             PET1 index= 269                            MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT : Sets the timeout in seconds to dump outstanding requests when progress wait is not making progress for some time.
20250508 102239.560 INFO             PET1 index= 270                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20250508 102239.560 INFO             PET1 index= 271                                    MPIR_CVAR_QMPI_TOOL_LIST : Set the number and order of QMPI tools to be loaded by the MPI library when it is initialized.
20250508 102239.560 INFO             PET1 index= 272                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20250508 102239.560 INFO             PET1 index= 273                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20250508 102239.560 INFO             PET1 index= 274                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20250508 102239.560 INFO             PET1 index= 275                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20250508 102239.560 INFO             PET1 index= 276                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20250508 102239.560 INFO             PET1 index= 277                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20250508 102239.560 INFO             PET1 index= 278                                  MPIR_CVAR_CLIQUES_BY_BLOCK : Specify to divide processes into cliques by uniform blocks. The default is to divide in round-robin fashion. Used for debugging on a single machine.
20250508 102239.560 INFO             PET1 index= 279                                       MPIR_CVAR_PMI_VERSION : Variable to select runtime PMI version.
1        - PMI (default)
2        - PMI2
x        - PMIx
20250508 102239.560 INFO             PET1 index= 280                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20250508 102239.560 INFO             PET1 index= 281                                        MPIR_CVAR_ENABLE_GPU : Control MPICH GPU support. If set to 0, all GPU support is disabled and we do not query the buffer type internally because we assume no GPU buffer is use.
20250508 102239.560 INFO             PET1 index= 282                               MPIR_CVAR_GPU_HAS_WAIT_KERNEL : If set to 1, avoid allocate allocating GPU registered host buffers for temporary buffers. When stream workq and GPU wait kernels are in use, access APIs for GPU registered memory may cause deadlock.
20250508 102239.560 INFO             PET1 index= 283                               MPIR_CVAR_ENABLE_GPU_REGISTER : Control whether to actually register buffers with the GPU runtime in MPIR_gpu_register_host. This could lower the latency of certain GPU communication at the cost of some amount of GPU memory consumed by the MPI library. By default, registration is enabled.
20250508 102239.560 INFO             PET1 index= 284                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.560 INFO             PET1 index= 285                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20250508 102239.560 INFO             PET1 index= 286                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20250508 102239.560 INFO             PET1 index= 287                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20250508 102239.560 INFO             PET1 index= 288                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20250508 102239.560 INFO             PET1 index= 289                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20250508 102239.560 INFO             PET1 index= 290                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET1 index= 291                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET1 index= 292                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20250508 102239.560 INFO             PET1 index= 293                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20250508 102239.560 INFO             PET1 index= 294                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20250508 102239.560 INFO             PET1 index= 295                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.560 INFO             PET1 index= 296               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20250508 102239.560 INFO             PET1 index= 297                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20250508 102239.560 INFO             PET1 index= 298               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchronization approach.  Change this value if programs fail because they run out of requests or other internal resources
20250508 102239.560 INFO             PET1 index= 299                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be negative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20250508 102239.560 INFO             PET1 index= 300            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation available in the ending synchronization call.
20250508 102239.560 INFO             PET1 index= 301                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20250508 102239.560 INFO             PET1 index= 302                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive value.
20250508 102239.560 INFO             PET1 index= 303                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20250508 102239.560 INFO             PET1 index= 304                                    MPIR_CVAR_CH3_PG_VERBOSE : If set, print the PG state on finalize.
20250508 102239.560 INFO             PET1 index= 305                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET1 index= 306                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET1 index= 307                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET1 index= 308                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET1 index= 309           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediately.  Requires a positive value.
20250508 102239.560 INFO             PET1 index= 310                                  MPIR_CVAR_OFI_USE_PROVIDER : This variable is no longer supported. Use FI_PROVIDER instead to select libfabric providers.
20250508 102239.560 INFO             PET1 index= 311                               MPIR_CVAR_SINGLE_HOST_ENABLED : Set this variable to true to indicate that processes are launched on a single host. The current implication is to avoid the cxi provider to prevent the use of scarce hardware resources.
20250508 102239.560 INFO             PET1 index= 312                    MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE : For long message to be sent using pipeline rather than default RDMA read.
20250508 102239.561 INFO             PET1 index= 313                         MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir                        - Fallback to MPIR collectives
trigger_tree_tagged         - Force triggered ops based Tagged Tree
trigger_tree_rma            - Force triggered ops based RMA Tree
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_OFI_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET1 index= 314                                     MPIR_CVAR_OFI_SKIP_IPV6 : Skip IPv6 providers.
20250508 102239.561 INFO             PET1 index= 315                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20250508 102239.561 INFO             PET1 index= 316                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20250508 102239.561 INFO             PET1 index= 317                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20250508 102239.561 INFO             PET1 index= 318                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20250508 102239.561 INFO             PET1 index= 319                    MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS : If true, enable virtual addressing for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET1 index= 320                       MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED : If true, require all OFI memory regions must be backed by physical memory pages at the time the registration call is made. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET1 index= 321                   MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL : If true, memory registration call supports registering with NULL addresses.
20250508 102239.561 INFO             PET1 index= 322                        MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY : If true, enable provider supplied key for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET1 index= 323                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20250508 102239.561 INFO             PET1 index= 324                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20250508 102239.561 INFO             PET1 index= 325                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20250508 102239.561 INFO             PET1 index= 326                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20250508 102239.561 INFO             PET1 index= 327                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20250508 102239.561 INFO             PET1 index= 328                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20250508 102239.561 INFO             PET1 index= 329              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20250508 102239.561 INFO             PET1 index= 330                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20250508 102239.561 INFO             PET1 index= 331                               MPIR_CVAR_CH4_OFI_ENABLE_HMEM : If true, uses GPU direct RDMA support in the provider.
20250508 102239.561 INFO             PET1 index= 332                            MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM : If true, need to register the buffer to use GPU direct RDMA.
20250508 102239.561 INFO             PET1 index= 333                        MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD : The threshold to start using GPU direct RDMA.
20250508 102239.561 INFO             PET1 index= 334                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET1 index= 335                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET1 index= 336                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET1 index= 337                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET1 index= 338                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET1 index= 339                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20250508 102239.561 INFO             PET1 index= 340                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20250508 102239.561 INFO             PET1 index= 341                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20250508 102239.561 INFO             PET1 index= 342              MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS : Specifies the number of optimized memory regions supported by the provider. An optimized memory region is used for lower-overhead, unordered RMA operations. It uses a low-overhead RX path and additionally, a low-overhead packet format may be used to target an optimized memory region.
20250508 102239.561 INFO             PET1 index= 343                     MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL : Specifies the interval for manually flushing RMA operations when automatic progress is not enabled. It the underlying OFI provider supports auto data progress, this value is ignored. If the value is -1, this optimization will be turned off.
20250508 102239.561 INFO             PET1 index= 344                             MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX : Specifies the maximum number of iovecs to allocate for RMA operations to/from noncontiguous buffers.
20250508 102239.561 INFO             PET1 index= 345                        MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which OFI native path switches from eager to rendezvous mode. It does not affect the AM path eager limit. Having this gives a way to reliably test native non-path. If the number is positive, OFI will init the MPIDI_OFI_global.max_msg_size to the value of cvar. If the number is negative, OFI will init the MPIDI_OFI_globa.max_msg_size using whatever provider gives (which might be unlimited for socket provider).
20250508 102239.561 INFO             PET1 index= 346                                  MPIR_CVAR_CH4_OFI_MAX_NICS : If set to positive number, this cvar determines the maximum number of physical nics to use (if more than one is available). If the number is -1, underlying netmod or shmmod automatically uses an optimal number depending on what is detected on the system up to the limit determined by MPIDI_MAX_NICS (in ofi_types.h).
20250508 102239.561 INFO             PET1 index= 347                 MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING : If true, this cvar enables striping of large messages across multiple NICs.
20250508 102239.561 INFO             PET1 index= 348              MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD : Striping will happen for message sizes beyond this threshold.
20250508 102239.561 INFO             PET1 index= 349                  MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING : Multi-NIC hashing means to use more than one NIC to send and receive messages above a certain size.  If set to positive number, this feature will be turned on. If set to 0, this feature will be turned off. If the number is -1, MPICH automatically determines whether to use multi-nic hashing depending on what is detected on the system (e.g., number of NICs available, number of processes sharing the NICs).
20250508 102239.561 INFO             PET1 index= 350                     MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE : Controls the multirecv am buffer size. It is recommended to match this to the hugepage size so that the buffer can be allocated at the page boundary.
20250508 102239.561 INFO             PET1 index= 351                                  MPIR_CVAR_OFI_USE_MIN_NICS : If true and all nodes do not have the same number of NICs, MPICH will fall back to using the fewest number of NICs instead of returning an error.
20250508 102239.561 INFO             PET1 index= 352                          MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED : If true, enable OFI triggered ops for MPI collectives.
20250508 102239.561 INFO             PET1 index= 353                      MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the sender side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET1 index= 354                   MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the receiver side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET1 index= 355                       MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE : If true, enable pipeline for GPU data transfer. GPU pipeline does not support non-contiguous datatypes or mixed buffer types (i.e. GPU send buffer, host recv buffer). If GPU pipeline is enabled, the unsupported scenarios will cause undefined behavior if encountered.
20250508 102239.561 INFO             PET1 index= 356                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD : This is the threshold to start using GPU pipeline.
20250508 102239.561 INFO             PET1 index= 357                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU pipeline data transfer.
20250508 102239.561 INFO             PET1 index= 358        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU pipeline data transfer in each block/chunk of the pool.
20250508 102239.561 INFO             PET1 index= 359              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU pipeline data transfer
20250508 102239.561 INFO             PET1 index= 360              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the sender side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET1 index= 361              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the receiver side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET1 index= 362                      MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE : Avoid use fi_inject_write. For some provider, e.g. tcp;ofi_rxm, inject write may break the synchronization.
20250508 102239.561 INFO             PET1 index= 363                                       MPIR_CVAR_UCX_DT_RECV : Variable to select method for receiving noncontiguous data
true                - Use UCX datatype with pack/unpack callbacks
false               - MPICH will decide to pack/unpack at completion or use IOVs
based on the datatype
20250508 102239.561 INFO             PET1 index= 364                          MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE : By default, we will cache ipc handles using the specialized cache mechanism. If the
gpu-specific backend does not implement a specialized cache, then we will fallback to
the generic cache mechanism. Users can optionally force the generic cache mechanism or
disable ipc caching entirely.
generic - use the cache mechanism in the generic layer
specialized - use the cache mechanism in a gpu-specific mpl layer (if applicable)
disabled - disable caching completely
20250508 102239.561 INFO             PET1 index= 365                         MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD (in bytes), then enable GPU-based single copy protocol for intranode communication. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET1 index= 366                    MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE : If a send message size is less than or equal to MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE (in bytes), then enable GPU-basedfast memcpy. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET1 index= 367                       MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE : Variable to select implementation for ZE shareable IPC handle
pidfd - use pidfd_getfd syscall to implement shareable IPC handle
drmfd - force to use device fd-based shareable IPC handle
20250508 102239.561 INFO             PET1 index= 368                           MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE : By default, select engine type automatically
auto - select automatically
compute - use compute engine
copy_high_bandwidth - use high-bandwidth copy engine
copy_low_latency - use low-latency copy engine
20250508 102239.561 INFO             PET1 index= 369                   MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL : By default, use read protocol.
auto - select automatically
read - use read protocol
write - use write protocol if remote device is visible
20250508 102239.561 INFO             PET1 index= 370                           MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR : If an address is used more than once in the last ten send operations, map it for IPC use even if it is below the IPC threshold.
20250508 102239.561 INFO             PET1 index= 371                                  MPIR_CVAR_CH4_XPMEM_ENABLE : To manually disable XPMEM set to 0. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET1 index= 372                       MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD (in bytes), then enable XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET1 index= 373                       MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
ipc_read - Uses read-based collective with ipc
20250508 102239.561 INFO             PET1 index= 374                      MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET1 index= 375                      MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET1 index= 376                     MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET1 index= 377                   MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node allreduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET1 index= 378                     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node barrier
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET1 index= 379                    MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node alltoall
mpir           - Fallback to MPIR collectives (default)
ipc_read    - Uses read-based collective with ipc
20250508 102239.561 INFO             PET1 index= 380                              MPIR_CVAR_POSIX_POLL_FREQUENCY : This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.561 INFO             PET1 index= 381                 MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read bcast only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET1 index= 382              MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read alltoall only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET1 index= 383                         MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD : Use posix optimized collectives (release_gather) only when the total number of Bcast, Reduce, Barrier, and Allreduce calls on the node level communicator is more than this threshold.
20250508 102239.561 INFO             PET1 index= 384                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : If non-empty, this cvar specifies which shm posix eager module to use
20250508 102239.561 INFO             PET1 index= 385         MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.561 INFO             PET1 index= 386                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS : The number of cells used for the depth of the iqueue.
20250508 102239.561 INFO             PET1 index= 387                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE : Size of each cell.
20250508 102239.561 INFO             PET1 index= 388                           MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE : Maximum shared memory created per node for optimized intra-node collectives (in KB)
20250508 102239.561 INFO             PET1 index= 389                 MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the bcast buffer (in bytes)
20250508 102239.561 INFO             PET1 index= 390                         MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS : Number of cells the bcast buffer is divided into
20250508 102239.561 INFO             PET1 index= 391                MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the reduce buffer per rank (in bytes)
20250508 102239.561 INFO             PET1 index= 392                        MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS : Number of cells the reduce buffer is divided into, for each rank
20250508 102239.561 INFO             PET1 index= 393                         MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node bcast
20250508 102239.561 INFO             PET1 index= 394                         MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE : Tree type for intra-node bcast tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET1 index= 395                        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node reduce
20250508 102239.561 INFO             PET1 index= 396                        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE : Tree type for intra-node reduce tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET1 index= 397             MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES : Enable collective specific intra-node trees which leverage the memory hierarchy of a machine. Depends on hwloc to extract the binding information of each rank. Pick a leader rank per package (socket), then create a per_package tree for ranks on a same package, package leaders tree for package leaders. For Bcast - Assemble the per_package and package_leaders tree in such a way that leaders interact among themselves first before interacting with package local ranks. Both the package_leaders and per_package trees are left skewed (children are added from left to right, first child to be added is the first one to be processed in traversal) For Reduce - Assemble the per_package and package_leaders tree in such a way that a leader rank interacts with its package local ranks first, then with the other package leaders. Both the per_package and package_leaders tree is right skewed (children are added in reverse order, first child to be added is the last one to be processed in traversal) The tree radix and tree type of
20250508 102239.561 INFO             PET1 index= 398                               MPIR_CVAR_BARRIER_COMPOSITION : Select composition (inter_node + intra_node) for Barrier 0 Auto selection 1 NM + SHM 2 NM only
20250508 102239.561 INFO             PET1 index= 399                                 MPIR_CVAR_BCAST_COMPOSITION : Select composition (inter_node + intra_node) for Bcast 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.561 INFO             PET1 index= 400                             MPIR_CVAR_ALLREDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Allreduce 0 Auto selection 1 NM + SHM with reduce + bcast 2 NM only composition 3 SHM only composition 4 Multi leaders based inter node + intra node composition
20250508 102239.561 INFO             PET1 index= 401                             MPIR_CVAR_ALLGATHER_COMPOSITION : Select composition (inter_node + intra_node) for Allgather 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.561 INFO             PET1 index= 402                              MPIR_CVAR_ALLTOALL_COMPOSITION : Select composition (inter_node + intra_node) for Alltoall 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.562 INFO             PET1 index= 403                                MPIR_CVAR_REDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Reduce 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.562 INFO             PET1 index= 404                             MPIR_CVAR_ALLTOALL_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Alltoall (in bytes)
20250508 102239.562 INFO             PET1 index= 405                            MPIR_CVAR_ALLGATHER_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Allgather (in bytes)
20250508 102239.562 INFO             PET1 index= 406                                   MPIR_CVAR_NUM_MULTI_LEADS : Number of leader ranks per node to be used for multi-leaders based collective algorithms
20250508 102239.562 INFO             PET1 index= 407                          MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER : Shared memory region per node-leader for multi-leaders based composition for MPI_Allreduce (in bytes) If it is undefined by the user, it is set to the message size of the first call to the algorithm. Max shared memory size is limited to 4MB.
20250508 102239.562 INFO             PET1 index= 408                        MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER : Amount of data reduced in allreduce delta composition's reduce local step (in bytes). Smaller msg size per leader avoids cache misses and improves performance. Experiments indicate 512 to be the best value.
20250508 102239.562 INFO             PET1 index= 409                      MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS : number of offsets in the allreduce delta composition's local copy The value of 2 performed the best in our 2 NIC test cases.
20250508 102239.562 INFO             PET1 index= 410                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20250508 102239.562 INFO             PET1 index= 411                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20250508 102239.562 INFO             PET1 index= 412                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20250508 102239.562 INFO             PET1 index= 413                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20250508 102239.562 INFO             PET1 index= 414                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) lockless
20250508 102239.562 INFO             PET1 index= 415                                      MPIR_CVAR_CH4_NUM_VCIS : Sets the number of VCIs to be implicitly used (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET1 index= 416                                  MPIR_CVAR_CH4_RESERVE_VCIS : Sets the number of VCIs that user can explicitly allocate (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET1 index= 417               MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.562 INFO             PET1 index= 418                               MPIR_CVAR_CH4_IOV_DENSITY_MIN : Defines the threshold of high-density datatype. The density is calculated by (datatype_size / datatype_num_contig_blocks).
20250508 102239.562 INFO             PET1 index= 419                              MPIR_CVAR_CH4_PACK_BUFFER_SIZE : Specifies the number of buffers for packing/unpacking active messages in each block of the pool. The size here should be greater or equal to the max of the eager buffer limit of SHM and NETMOD.
20250508 102239.562 INFO             PET1 index= 420                    MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK : Specifies the number of buffers for packing/unpacking active messages in each block of the pool.
20250508 102239.562 INFO             PET1 index= 421                          MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS : Specifies the max number of buffers for packing/unpacking buffers in the pool. Use 0 for unlimited.
20250508 102239.562 INFO             PET1 index= 422                       MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU collectives data transfer.
20250508 102239.562 INFO             PET1 index= 423                MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU collectives data transfer in each block/chunk of the pool.
20250508 102239.562 INFO             PET1 index= 424                      MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU collectives data transfer.
20250508 102239.562 INFO             PET1 index= 425                               MPIR_CVAR_CH4_GLOBAL_PROGRESS : If on, poll global progress every once a while. With per-vci configuration, turning global progress off may improve the threading performance.
20250508 102239.562 INFO             PET1 index= 426                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.562 INFO             PET1 index= 427                           MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ : Enable stream enqueue operations via stream work queue. Requires progress thread on the corresponding MPIX stream. Reference: MPIX_Stream_progress and MPIX_Start_progress_thread.
20250508 102239.562 INFO             PET1 index= 428                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20250508 102239.562 INFO             PET1 index= 429                MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS : If true, allows RMA synchronization calls to dynamically reduce the frequency of internal progress polling for incoming RMA active messages received on the target process. The RMA synchronization call initially polls progress with a low frequency (defined by MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL) to reduce synchronization overhead. Once any RMA active message has been received, it will always poll progress once at every synchronization call to ensure prompt target-side progress. Effective only for passive target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}.
20250508 102239.562 INFO             PET1 index= 430                      MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL : Specifies a static interval of progress polling for incoming RMA active messages received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Invalid when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET1 index= 431             MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL : Specifies the interval of progress polling with low frequency for incoming RMA active message received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Used when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET1 index= 432            MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE : The genq shmem code allocates pools of cells on each process and, when needed, a cell is removed from the pool and passed to another process. This can happen by either removing a cell from the pool of the sending process or from the pool of the receiving process. This CVAR determines which pool to use. If true, the cell will come from the sender-side. If false, the cell will com from the receiver-side. There are specific advantages of using receiver-side cells when combined with the "avx" fast configure option, which allows MPICH to use AVX streaming copy intrintrinsics, when available, to avoid polluting the cache of the sender with the data being copied to the receiver. Using receiver-side cells does have the trade-off of requiring an MPMC lock for the free queue rather than an MPSC lock, which is used for sender-side cells. Initial performance analysis shows that using the MPMC lock in this case had no significant performance loss. By default, the queue will continue to use sender-side queues until the pe
20250508 102239.562 INFO             PET1 index= 433                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20250508 102239.562 INFO             PET1 index= 434                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20250508 102239.562 INFO             PET1 index= 435                             MPIR_CVAR_SHM_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20250508 102239.562 INFO             PET1 index= 436                                 MPIR_CVAR_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20250508 102239.562 INFO             PET1 index= 437                                MPIR_CVAR_ENABLE_HEAVY_YIELD : If enabled, use nanosleep to ensure other threads have a chance to grab the lock. Note: this may not work with some thread runtimes, e.g. non-preemptive user-level threads.
20250508 102239.562 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20250508 102239.562 INFO             PET1 main: --- VMK::log() start -------------------------------------
20250508 102239.562 INFO             PET1 main: vm located at: 0x140606570
20250508 102239.562 INFO             PET1 main: mpionly=1 threadsflag=0
20250508 102239.562 INFO             PET1 main: ssiCount=1 localSsi=0
20250508 102239.562 INFO             PET1 main: devCount=0 ssiLocalDevCount=0
20250508 102239.562 INFO             PET1 main: petCount=6 ssiLocalPetCount=6
20250508 102239.562 INFO             PET1 main: localPet=1 mypthid=0x1edb24f40 ssiLocalPet=1 currentSsiPe=-1
20250508 102239.562 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.562 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.562 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20250508 102239.562 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.562 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20250508 102239.562 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.562 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20250508 102239.562 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.562 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20250508 102239.562 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.562 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20250508 102239.562 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.562 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20250508 102239.562 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20250508 102239.563 INFO             PET1 Executing 'userm1_setvm'
20250508 102239.563 INFO             PET1 Executing 'userm1_register'
20250508 102239.563 INFO             PET1 Executing 'userm2_setvm'
20250508 102239.563 DEBUG            PET1 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 DEBUG            PET1 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20250508 102239.563 INFO             PET1 model1: vm located at: 0x142805220
20250508 102239.563 INFO             PET1 model1: mpionly=1 threadsflag=0
20250508 102239.563 INFO             PET1 model1: ssiCount=1 localSsi=0
20250508 102239.563 INFO             PET1 model1: devCount=0 ssiLocalDevCount=0
20250508 102239.563 INFO             PET1 model1: petCount=6 ssiLocalPetCount=6
20250508 102239.563 INFO             PET1 model1: localPet=1 mypthid=0x1edb24f40 ssiLocalPet=1 currentSsiPe=-1
20250508 102239.563 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.563 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.563 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20250508 102239.563 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.563 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20250508 102239.563 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.563 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20250508 102239.563 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.563 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20250508 102239.563 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.563 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20250508 102239.563 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.563 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20250508 102239.563 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20250508 102239.568 INFO             PET1 Entering 'user1_run'
20250508 102239.568 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102239.841 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102240.074 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102240.300 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102240.528 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20250508 102240.753 INFO             PET1 Exiting 'user1_run'
20250508 102249.040 INFO             PET1  NUMBER_OF_PROCESSORS           6
20250508 102249.040 INFO             PET1  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20250508 102249.040 INFO             PET1  Finalizing ESMF
20250508 102249.040 INFO             PET1 ESMCI_IO_Handler.C:335 ESMCI::IO_Handler::finalize() 
20250508 102249.040 INFO             PET1 ESMCI_PIO_Handler.C:357 ESMCI::PIO_Handler::finalize() 
20250508 102249.040 INFO             PET1 ESMCI_IO_Handler.C:337 ESMCI::IO_Handler::finalize() after finalize, localrc = 0
20250508 102249.040 INFO             PET1 ESMCI_IO_Handler.C:360 ESMCI::IO_Handler::finalize() before return, localrc = 0
20250508 102239.555 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20250508 102239.556 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20250508 102239.556 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20250508 102239.556 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20250508 102239.556 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET2 Running with ESMF Version   : v8.9.0b08-5-g079ca68fdc
20250508 102239.556 INFO             PET2 ESMF library build date/time: "May  8 2025" "10:06:33"
20250508 102239.556 INFO             PET2 ESMF library build location : /Users/oehmke/ESMF_AutoTest/gfortranclang_12.2.0_14.0.0_mpich_g_develop/esmf
20250508 102239.556 INFO             PET2 ESMF_COMM                   : mpich
20250508 102239.556 INFO             PET2 ESMF_MOAB                   : enabled
20250508 102239.556 INFO             PET2 ESMF_LAPACK                 : enabled
20250508 102239.556 INFO             PET2 ESMF_NETCDF                 : enabled
20250508 102239.556 INFO             PET2 ESMF_PNETCDF                : disabled
20250508 102239.556 INFO             PET2 ESMF_PIO                    : enabled
20250508 102239.556 INFO             PET2 ESMF_YAMLCPP                : enabled
20250508 102239.556 INFO             PET2 --- VMK::logSystem() start -------------------------------
20250508 102239.556 INFO             PET2 esmfComm=mpich
20250508 102239.556 INFO             PET2 isPthreadsEnabled=1
20250508 102239.556 INFO             PET2 isOpenMPEnabled=0
20250508 102239.557 INFO             PET2 isOpenACCEnabled=0
20250508 102239.557 INFO             PET2 isSsiSharedMemoryEnabled=1
20250508 102239.557 INFO             PET2 isNvmlEnabled=0
20250508 102239.557 INFO             PET2 isNumaEnabled=0
20250508 102239.557 INFO             PET2 ssiCount=1 peCount=6
20250508 102239.557 INFO             PET2 PE=0 SSI=0 SSIPE=0
20250508 102239.557 INFO             PET2 PE=1 SSI=0 SSIPE=1
20250508 102239.557 INFO             PET2 PE=2 SSI=0 SSIPE=2
20250508 102239.557 INFO             PET2 PE=3 SSI=0 SSIPE=3
20250508 102239.557 INFO             PET2 PE=4 SSI=0 SSIPE=4
20250508 102239.557 INFO             PET2 PE=5 SSI=0 SSIPE=5
20250508 102239.557 INFO             PET2 ndevs=0 ndevsSSI=0
20250508 102239.557 INFO             PET2 
20250508 102239.557 INFO             PET2 --- VMK::logSystem() MPI Layer ---------------------------
20250508 102239.557 INFO             PET2 MPI_VERSION=4
20250508 102239.557 INFO             PET2 MPI_SUBVERSION=1
20250508 102239.557 INFO             PET2 MPICH_VERSION=4.2.3
20250508 102239.557 INFO             PET2 mpi_t_okay=1
20250508 102239.557 INFO             PET2 --- VMK::logSystem() MPI Tool Interface Control Vars ---
20250508 102239.557 INFO             PET2 index=   0                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb            - Force nonblocking algorithm
smp           - Force smp algorithm
k_dissemination - Force high radix dissemination algorithm
recexch       - Force recursive exchange algorithm
20250508 102239.557 INFO             PET2 index=   1                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
bcast - Force bcast algorithm
nb    - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=   2                               MPIR_CVAR_BARRIER_DISSEM_KVAL : k value for dissemination exchange based barrier algorithm
20250508 102239.557 INFO             PET2 index=   3                              MPIR_CVAR_BARRIER_RECEXCH_KVAL : k value for recursive exchange based allreduce based barrier
20250508 102239.557 INFO             PET2 index=   4                 MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.557 INFO             PET2 index=   5                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20250508 102239.557 INFO             PET2 index=   6                              MPIR_CVAR_IBARRIER_DISSEM_KVAL : k value for dissemination exchange based ibarrier
20250508 102239.557 INFO             PET2 index=   7                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
tsp_recexch - Force generic transport based recursive exchange algorithm
tsp_k_dissemination - Force generic transport based high-radix dissemination algorithm
20250508 102239.557 INFO             PET2 index=   8                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_bcast - Force bcast algorithm
20250508 102239.557 INFO             PET2 index=   9                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET2 index=  10                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET2 index=  11                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET2 index=  12                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes.
20250508 102239.557 INFO             PET2 index=  13                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial                                - Force Binomial Tree
nb                                      - Force nonblocking algorithm
smp                                     - Force smp algorithm
scatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgather
scatter_ring_allgather                  - Force Scatter Ring
pipelined_tree                          - Force tree-based pipelined algorithm
tree                                    - Force tree-based algorithm
20250508 102239.557 INFO             PET2 index=  14                                   MPIR_CVAR_BCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based bcast
20250508 102239.557 INFO             PET2 index=  15                                   MPIR_CVAR_BCAST_TREE_TYPE : Tree type for tree based bcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.557 INFO             PET2 index=  16                         MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.557 INFO             PET2 index=  17                               MPIR_CVAR_BCAST_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.557 INFO             PET2 index=  18                            MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.557 INFO             PET2 index=  19                          MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.557 INFO             PET2 index=  20                          MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.557 INFO             PET2 index=  21                             MPIR_CVAR_BCAST_IS_NON_BLOCKING : If set to true, MPI_Bcast will use non-blocking send.
20250508 102239.557 INFO             PET2 index=  22                    MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE : Indicates the chunk size for pipelined bcast.
20250508 102239.557 INFO             PET2 index=  23                               MPIR_CVAR_BCAST_RECV_PRE_POST : If set to true, MPI_Bcast will pre-post all the receives.
20250508 102239.557 INFO             PET2 index=  24                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                      - Force nonblocking algorithm
remote_send_local_bcast - Force remote-send-local-bcast algorithm
20250508 102239.557 INFO             PET2 index=  25                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20250508 102239.557 INFO             PET2 index=  26                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.557 INFO             PET2 index=  27                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET2 index=  28                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET2 index=  29                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial                             - Force Binomial algorithm
sched_smp                                  - Force smp algorithm
sched_scatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithm
sched_scatter_ring_allgather               - Force Scatter Ring Allgather algorithm
tsp_tree                               - Force Generic Transport Tree algorithm
tsp_scatterv_recexch_allgatherv        - Force Generic Transport Scatterv followed by Recursive Exchange Allgatherv algorithm
tsp_scatterv_ring_allgatherv           - Force Generic Transport Scatterv followed by Ring Allgatherv algorithm
tsp_ring                               - Force Generic Transport Ring algorithm
20250508 102239.557 INFO             PET2 index=  30                              MPIR_CVAR_IBCAST_SCATTERV_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET2 index=  31                    MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET2 index=  32                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_flat - Force flat algorithm
20250508 102239.557 INFO             PET2 index=  33                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20250508 102239.557 INFO             PET2 index=  34                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=  35                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                   - Force linear algorithm
local_gather_remote_send - Force local-gather-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=  36                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET2 index=  37                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20250508 102239.557 INFO             PET2 index=  38                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_long  - Force long inter algorithm
sched_short - Force short inter algorithm
20250508 102239.557 INFO             PET2 index=  39                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=  40                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=  41                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear         - Force linear algorithm
tsp_linear       - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET2 index=  42                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET2 index=  43                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20250508 102239.557 INFO             PET2 index=  44                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=  45                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                    - Force linear algorithm
nb                        - Force nonblocking algorithm
remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET2 index=  46                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET2 index=  47                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20250508 102239.557 INFO             PET2 index=  48                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear                    - Force linear algorithm
sched_remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET2 index=  49                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=  50                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET2 index=  51                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET2 index=  52                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET2 index=  53                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET2 index=  54                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET2 index=  55                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
k_brucks           - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
recexch_doubling   - Force recexch distance doubling algorithm
recexch_halving    - Force recexch distance halving algorithm
20250508 102239.557 INFO             PET2 index=  56                             MPIR_CVAR_ALLGATHER_BRUCKS_KVAL : radix (k) value for generic transport brucks based allgather
20250508 102239.557 INFO             PET2 index=  57                            MPIR_CVAR_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather
20250508 102239.557 INFO             PET2 index=  58               MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET2 index=  59                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
nb                        - Force nonblocking algorithm
20250508 102239.558 INFO             PET2 index=  60                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20250508 102239.558 INFO             PET2 index=  61                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20250508 102239.558 INFO             PET2 index=  62                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_ring               - Force ring algorithm
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
tsp_ring       - Force generic transport ring algorithm
tsp_brucks     - Force generic transport based brucks algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
20250508 102239.558 INFO             PET2 index=  63                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20250508 102239.558 INFO             PET2 index=  64                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20250508 102239.558 INFO             PET2 index=  65                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
20250508 102239.558 INFO             PET2 index=  66                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET2 index=  67                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20250508 102239.558 INFO             PET2 index=  68                           MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL : k value for radix in brucks based iallgatherv
20250508 102239.558 INFO             PET2 index=  69                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_ring               - Force ring algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
tsp_ring             - Force generic transport ring algorithm
tsp_brucks           - Force generic transport based brucks algorithm
20250508 102239.558 INFO             PET2 index=  70                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET2 index=  71                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20250508 102239.558 INFO             PET2 index=  72                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20250508 102239.558 INFO             PET2 index=  73                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20250508 102239.558 INFO             PET2 index=  74                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks                    - Force brucks algorithm
k_brucks                  - Force Force radix k brucks algorithm
nb                        - Force nonblocking algorithm
pairwise                  - Force pairwise algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET2 index=  75                              MPIR_CVAR_ALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based alltoall
20250508 102239.558 INFO             PET2 index=  76                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET2 index=  77                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks            - Force brucks algorithm
sched_inplace           - Force inplace algorithm
sched_pairwise          - Force pairwise algorithm
sched_permuted_sendrecv - Force permuted sendrecv algorithm
tsp_ring            - Force generic transport based ring algorithm
tsp_brucks          - Force generic transport based brucks algorithm
tsp_scattered       - Force generic transport based scattered algorithm
20250508 102239.558 INFO             PET2 index=  78                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET2 index=  79                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET2 index=  80                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
pairwise_exchange - Force pairwise exchange algorithm
nb                - Force nonblocking algorithm
20250508 102239.558 INFO             PET2 index=  81                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_scattered       - Force generic transport based scattered algorithm
tsp_blocked         - Force generic transport blocked algorithm
tsp_inplace         - Force generic transport inplace algorithm
20250508 102239.558 INFO             PET2 index=  82                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET2 index=  83            MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.558 INFO             PET2 index=  84                   MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.558 INFO             PET2 index=  85                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET2 index=  86                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET2 index=  87                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_blocked   - Force generic transport based blocked algorithm
tsp_inplace   - Force generic transport based inplace algorithm
20250508 102239.558 INFO             PET2 index=  88                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET2 index=  89                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET2 index=  90                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20250508 102239.558 INFO             PET2 index=  91                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial              - Force binomial algorithm
nb                    - Force nonblocking algorithm
smp                   - Force smp algorithm
reduce_scatter_gather - Force reduce scatter gather algorithm
20250508 102239.558 INFO             PET2 index=  92                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_reduce_remote_send - Force local-reduce-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.558 INFO             PET2 index=  93                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20250508 102239.558 INFO             PET2 index=  94                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET2 index=  95                       MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET2 index=  96                             MPIR_CVAR_IREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET2 index=  97                          MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET2 index=  98                        MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET2 index=  99                        MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET2 index= 100                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET2 index= 101                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET2 index= 102                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET2 index= 103                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                   - Force smp algorithm
sched_binomial              - Force binomial algorithm
sched_reduce_scatter_gather - Force reduce scatter gather algorithm
tsp_tree                - Force Generic Transport Tree
tsp_ring                - Force Generic Transport Ring
20250508 102239.558 INFO             PET2 index= 104                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_reduce_remote_send - Force local-reduce-remote-send algorithm
20250508 102239.558 INFO             PET2 index= 105                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET2 index= 106                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20250508 102239.558 INFO             PET2 index= 107                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                       - Force nonblocking algorithm
smp                      - Force smp algorithm
recursive_doubling       - Force recursive doubling algorithm
reduce_scatter_allgather - Force reduce scatter allgather algorithm
tree                     - Force pipelined tree algorithm
recexch                  - Force generic transport recursive exchange algorithm
ring                     - Force ring algorithm
k_reduce_scatter_allgather - Force reduce scatter allgather algorithm
20250508 102239.558 INFO             PET2 index= 108                               MPIR_CVAR_ALLREDUCE_TREE_TYPE : Tree type for tree based allreduce knomial_1 is default as it supports both commutative and non-commutative reduce operations kary      - kary tree type knomial_1 - knomial_1 tree type (tree grows starting from the left of the root) knomial_2 - knomial_2 tree type (tree grows starting from the right of the root) topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET2 index= 109                               MPIR_CVAR_ALLREDUCE_TREE_KVAL : Indicates the branching factor for kary or knomial trees.
20250508 102239.558 INFO             PET2 index= 110                     MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET2 index= 111                           MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET2 index= 112                        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET2 index= 113                      MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET2 index= 114                      MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET2 index= 115                MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based allreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET2 index= 116                   MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET2 index= 117                            MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based allreduce
20250508 102239.558 INFO             PET2 index= 118               MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET2 index= 119                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                    - Force nonblocking algorithm
reduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20250508 102239.558 INFO             PET2 index= 120                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20250508 102239.558 INFO             PET2 index= 121                              MPIR_CVAR_IALLREDUCE_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.558 INFO             PET2 index= 122               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET2 index= 123                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET2 index= 124                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20250508 102239.558 INFO             PET2 index= 125                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_naive                      - Force naive algorithm
sched_smp                        - Force smp algorithm
sched_recursive_doubling         - Force recursive doubling algorithm
sched_reduce_scatter_allgather   - Force reduce scatter allgather algorithm
tsp_recexch_single_buffer    - Force generic transport recursive exchange with single buffer for receives
tsp_recexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
tsp_tree                     - Force generic transport tree algorithm
tsp_ring                     - Force generic transport ring algorithm
tsp_recexch_reduce_scatter_recexch_allgatherv  - Force generic transport recursive exchange with reduce scatter and allgatherv
20250508 102239.558 INFO             PET2 index= 126                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20250508 102239.558 INFO             PET2 index= 127          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20250508 102239.558 INFO             PET2 index= 128                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
noncommutative     - Force noncommutative algorithm
pairwise           - Force pairwise algorithm
recursive_doubling - Force recursive doubling algorithm
recursive_halving  - Force recursive halving algorithm
20250508 102239.558 INFO             PET2 index= 129                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET2 index= 130                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20250508 102239.558 INFO             PET2 index= 131                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET2 index= 132                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET2 index= 133              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
noncommutative     - Force noncommutative algorithm
recursive_doubling - Force recursive doubling algorithm
pairwise           - Force pairwise algorithm
recursive_halving  - Force recursive halving algorithm
nb                 - Force nonblocking algorithm
20250508 102239.558 INFO             PET2 index= 134              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET2 index= 135                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20250508 102239.558 INFO             PET2 index= 136             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET2 index= 137             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET2 index= 138                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
smp                - Force smp algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET2 index= 139                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                  - Force smp algorithm
sched_recursive_doubling   - Force recursive doubling algorithm
tsp_recursive_doubling - Force generic transport recursive doubling algorithm
20250508 102239.558 INFO             PET2 index= 140                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET2 index= 141                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET2 index= 142                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.558 INFO             PET2 index= 143                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET2 index= 144               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 145               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 146               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 147               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 148              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 149              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 150                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 151                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 152                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 153                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 154                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 155                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 156               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 157               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 158                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 159                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET2 index= 160               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 161               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET2 index= 162                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 163                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 164                    MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 165                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 166                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 167                      MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 168                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 169                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 170                     MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 171                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 172                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 173                    MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 174                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 175                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 176                    MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 177                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 178                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 179                   MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 180                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 181                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 182                  MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 183                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 184                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 185                 MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 186                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 187                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 188                   MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 189                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 190                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 191                  MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 192                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 193                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 194                  MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 195                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 196                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 197                     MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 198                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 199                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 200                  MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 201                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 202                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 203             MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 204            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 205           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 206       MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 207                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 208                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 209                       MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 210                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 211                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 212                     MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 213              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 214             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 215         MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 216             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 217            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 218        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 219               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 220              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 221          MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 222              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 223             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 224         MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 225              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 226             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET2 index= 227         MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET2 index= 228                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20250508 102239.560 INFO             PET2 index= 229                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20250508 102239.560 INFO             PET2 index= 230                             MPIR_CVAR_IALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based ialltoall
20250508 102239.560 INFO             PET2 index= 231                   MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR : If set to true, the tsp based brucks algorithm will allocate dedicated send and receive buffers for every neighbor in the brucks algorithm. Otherwise, it would reuse a single buffer for sending and receiving data to/from neighbors
20250508 102239.560 INFO             PET2 index= 232             MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.560 INFO             PET2 index= 233                    MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.560 INFO             PET2 index= 234                                MPIR_CVAR_DEVICE_COLLECTIVES : Variable to select whether the device can override the
MPIR-level collective algorithms.
all     - Always prefer the device collectives
none    - Never pick the device collectives
percoll - Use the per-collective CVARs to decide
20250508 102239.560 INFO             PET2 index= 235                               MPIR_CVAR_COLLECTIVE_FALLBACK : Variable to control what the MPI library should do if the
user-specified collective algorithm does not work for the
arguments passed in by the user.
error   - throw an error
print   - print an error message and fallback to the internally selected algorithm
silent  - silently fallback to the internally selected algorithm
20250508 102239.560 INFO             PET2 index= 236                   MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.560 INFO             PET2 index= 237                                    MPIR_CVAR_HIERARCHY_DUMP : If set to true, each rank will dump the hierarchy data structure to a file named "hierarchy[rank]" in the current folder. If set to false, the hierarchy data structure will not be dumped.
20250508 102239.560 INFO             PET2 index= 238                                  MPIR_CVAR_COORDINATES_FILE : Defines the location of the input coordinates file.
20250508 102239.560 INFO             PET2 index= 239                                    MPIR_CVAR_COLL_TREE_DUMP : If set to true, each rank will dump the tree to a file named "colltree[rank].json" in the current folder. If set to false, the tree will not be dumped.
20250508 102239.560 INFO             PET2 index= 240                                  MPIR_CVAR_COORDINATES_DUMP : If set to true, rank 0 will dump the network coordinates to a file named "coords" in the current folder. If set to false, the network coordinates will not be dumped.
20250508 102239.560 INFO             PET2 index= 241                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20250508 102239.560 INFO             PET2 index= 242                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20250508 102239.560 INFO             PET2 index= 243                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20250508 102239.560 INFO             PET2 index= 244                                MPIR_CVAR_DATALOOP_FAST_SEEK : use a datatype-specialized algorithm to shortcut seeking to the correct location in a noncontiguous buffer
20250508 102239.560 INFO             PET2 index= 245                             MPIR_CVAR_YAKSA_COMPLEX_SUPPORT : This CVAR indicates that complex type reduction is not supported in yaksa.
20250508 102239.560 INFO             PET2 index= 246                                MPIR_CVAR_GPU_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET2 index= 247                           MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET2 index= 248                            MPIR_CVAR_ENABLE_YAKSA_REDUCTION : This cvar enables yaksa based reduction for local reduce.
20250508 102239.560 INFO             PET2 index= 249                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20250508 102239.560 INFO             PET2 index= 250                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20250508 102239.560 INFO             PET2 index= 251                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20250508 102239.560 INFO             PET2 index= 252                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20250508 102239.560 INFO             PET2 index= 253                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20250508 102239.560 INFO             PET2 index= 254                          MPIR_CVAR_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. Can be set to auto or comma-separated list of logical processors. When set to auto - MPICH will automatically select logical CPU cores to decide affinity of the progress threads. When set to comma-separated list of logical processors - In case of N progress threads per process, the first N logical processors from list will be assigned to threads of first local process, the next N logical processors from list - to second local process and so on. For example, thread affinity is "0,1,2,3", 2 progress threads per process and 2 processes per node. Progress threads of first local process will be pinned on logical processors "0,1", progress threads of second local process - on "2,3". Cannot work together with MPIR_CVAR_NUM_CLIQUES or MPIR_CVAR_ODD_EVEN_CLIQUES.
20250508 102239.560 INFO             PET2 index= 255                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20250508 102239.560 INFO             PET2 index= 256                                 MPIR_CVAR_COREDUMP_ON_ABORT : Call libc abort() to generate a corefile
20250508 102239.560 INFO             PET2 index= 257                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20250508 102239.560 INFO             PET2 index= 258                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20250508 102239.560 INFO             PET2 index= 259                                     MPIR_CVAR_DEBUG_SUMMARY : If true, print internal summary of various debug information, such as memory allocation by category. Each layer may print their own summary information. For example, ch4-ofi may print its provider capability settings.
20250508 102239.560 INFO             PET2 index= 260                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20250508 102239.560 INFO             PET2 index= 261                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20250508 102239.560 INFO             PET2 index= 262                    MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST : If true, mpl/ze will use immediate command list for copying
20250508 102239.560 INFO             PET2 index= 263                    MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES : If true, mpl/ze will use command queues in a round-robin fashion. If false, only command queues of index 0 will be used.
20250508 102239.560 INFO             PET2 index= 264                            MPIR_CVAR_NO_COLLECTIVE_FINALIZE : If true, prevent MPI_Finalize to invoke collective behavior such as barrier or communicating to other processes. Consequently, it may result in leaking memory or losing messages due to pre-mature exiting. The default is false, which may invoke collective behaviors at finalize.
20250508 102239.560 INFO             PET2 index= 265                                     MPIR_CVAR_FINALIZE_WAIT : If true, poll progress at MPI_Finalize until reference count on MPI_COMM_WORLD and MPI_COMM_SELF reaches zero. This may be necessary to prevent remote processes hanging if it has pending communication protocols, e.g. a rendezvous send.
20250508 102239.560 INFO             PET2 index= 266                                 MPIR_CVAR_REQUEST_ERR_FATAL : By default, MPI_Waitall, MPI_Testall, MPI_Waitsome, and MPI_Testsome return MPI_ERR_IN_STATUS when one of the request fails. If MPIR_CVAR_REQUEST_ERR_FATAL is set to true, these routines will return the error code of the request immediately. The default MPI_ERRS_ARE_FATAL error handler will dump a error stack in this case, which maybe more convenient for debugging. This cvar will also make nonblocking shched return error right away as it issues operations.
20250508 102239.560 INFO             PET2 index= 267                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during MPI_{Waitany,Waitsome} in terms of number of processed requests before polling.
20250508 102239.560 INFO             PET2 index= 268                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20250508 102239.560 INFO             PET2 index= 269                            MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT : Sets the timeout in seconds to dump outstanding requests when progress wait is not making progress for some time.
20250508 102239.560 INFO             PET2 index= 270                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20250508 102239.560 INFO             PET2 index= 271                                    MPIR_CVAR_QMPI_TOOL_LIST : Set the number and order of QMPI tools to be loaded by the MPI library when it is initialized.
20250508 102239.560 INFO             PET2 index= 272                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20250508 102239.560 INFO             PET2 index= 273                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20250508 102239.560 INFO             PET2 index= 274                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20250508 102239.560 INFO             PET2 index= 275                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20250508 102239.560 INFO             PET2 index= 276                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20250508 102239.560 INFO             PET2 index= 277                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20250508 102239.560 INFO             PET2 index= 278                                  MPIR_CVAR_CLIQUES_BY_BLOCK : Specify to divide processes into cliques by uniform blocks. The default is to divide in round-robin fashion. Used for debugging on a single machine.
20250508 102239.560 INFO             PET2 index= 279                                       MPIR_CVAR_PMI_VERSION : Variable to select runtime PMI version.
1        - PMI (default)
2        - PMI2
x        - PMIx
20250508 102239.560 INFO             PET2 index= 280                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20250508 102239.560 INFO             PET2 index= 281                                        MPIR_CVAR_ENABLE_GPU : Control MPICH GPU support. If set to 0, all GPU support is disabled and we do not query the buffer type internally because we assume no GPU buffer is use.
20250508 102239.560 INFO             PET2 index= 282                               MPIR_CVAR_GPU_HAS_WAIT_KERNEL : If set to 1, avoid allocate allocating GPU registered host buffers for temporary buffers. When stream workq and GPU wait kernels are in use, access APIs for GPU registered memory may cause deadlock.
20250508 102239.560 INFO             PET2 index= 283                               MPIR_CVAR_ENABLE_GPU_REGISTER : Control whether to actually register buffers with the GPU runtime in MPIR_gpu_register_host. This could lower the latency of certain GPU communication at the cost of some amount of GPU memory consumed by the MPI library. By default, registration is enabled.
20250508 102239.560 INFO             PET2 index= 284                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.560 INFO             PET2 index= 285                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20250508 102239.560 INFO             PET2 index= 286                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20250508 102239.560 INFO             PET2 index= 287                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20250508 102239.560 INFO             PET2 index= 288                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20250508 102239.560 INFO             PET2 index= 289                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20250508 102239.560 INFO             PET2 index= 290                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET2 index= 291                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET2 index= 292                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20250508 102239.560 INFO             PET2 index= 293                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20250508 102239.560 INFO             PET2 index= 294                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20250508 102239.560 INFO             PET2 index= 295                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.560 INFO             PET2 index= 296               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20250508 102239.560 INFO             PET2 index= 297                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20250508 102239.560 INFO             PET2 index= 298               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchronization approach.  Change this value if programs fail because they run out of requests or other internal resources
20250508 102239.560 INFO             PET2 index= 299                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be negative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20250508 102239.560 INFO             PET2 index= 300            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation available in the ending synchronization call.
20250508 102239.560 INFO             PET2 index= 301                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20250508 102239.560 INFO             PET2 index= 302                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive value.
20250508 102239.560 INFO             PET2 index= 303                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20250508 102239.560 INFO             PET2 index= 304                                    MPIR_CVAR_CH3_PG_VERBOSE : If set, print the PG state on finalize.
20250508 102239.560 INFO             PET2 index= 305                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET2 index= 306                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET2 index= 307                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET2 index= 308                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET2 index= 309           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediately.  Requires a positive value.
20250508 102239.560 INFO             PET2 index= 310                                  MPIR_CVAR_OFI_USE_PROVIDER : This variable is no longer supported. Use FI_PROVIDER instead to select libfabric providers.
20250508 102239.560 INFO             PET2 index= 311                               MPIR_CVAR_SINGLE_HOST_ENABLED : Set this variable to true to indicate that processes are launched on a single host. The current implication is to avoid the cxi provider to prevent the use of scarce hardware resources.
20250508 102239.560 INFO             PET2 index= 312                    MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE : For long message to be sent using pipeline rather than default RDMA read.
20250508 102239.561 INFO             PET2 index= 313                         MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir                        - Fallback to MPIR collectives
trigger_tree_tagged         - Force triggered ops based Tagged Tree
trigger_tree_rma            - Force triggered ops based RMA Tree
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_OFI_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET2 index= 314                                     MPIR_CVAR_OFI_SKIP_IPV6 : Skip IPv6 providers.
20250508 102239.561 INFO             PET2 index= 315                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20250508 102239.561 INFO             PET2 index= 316                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20250508 102239.561 INFO             PET2 index= 317                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20250508 102239.561 INFO             PET2 index= 318                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20250508 102239.561 INFO             PET2 index= 319                    MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS : If true, enable virtual addressing for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET2 index= 320                       MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED : If true, require all OFI memory regions must be backed by physical memory pages at the time the registration call is made. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET2 index= 321                   MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL : If true, memory registration call supports registering with NULL addresses.
20250508 102239.561 INFO             PET2 index= 322                        MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY : If true, enable provider supplied key for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET2 index= 323                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20250508 102239.561 INFO             PET2 index= 324                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20250508 102239.561 INFO             PET2 index= 325                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20250508 102239.561 INFO             PET2 index= 326                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20250508 102239.561 INFO             PET2 index= 327                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20250508 102239.561 INFO             PET2 index= 328                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20250508 102239.561 INFO             PET2 index= 329              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20250508 102239.561 INFO             PET2 index= 330                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20250508 102239.561 INFO             PET2 index= 331                               MPIR_CVAR_CH4_OFI_ENABLE_HMEM : If true, uses GPU direct RDMA support in the provider.
20250508 102239.561 INFO             PET2 index= 332                            MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM : If true, need to register the buffer to use GPU direct RDMA.
20250508 102239.561 INFO             PET2 index= 333                        MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD : The threshold to start using GPU direct RDMA.
20250508 102239.561 INFO             PET2 index= 334                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET2 index= 335                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET2 index= 336                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET2 index= 337                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET2 index= 338                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET2 index= 339                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20250508 102239.561 INFO             PET2 index= 340                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20250508 102239.561 INFO             PET2 index= 341                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20250508 102239.561 INFO             PET2 index= 342              MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS : Specifies the number of optimized memory regions supported by the provider. An optimized memory region is used for lower-overhead, unordered RMA operations. It uses a low-overhead RX path and additionally, a low-overhead packet format may be used to target an optimized memory region.
20250508 102239.561 INFO             PET2 index= 343                     MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL : Specifies the interval for manually flushing RMA operations when automatic progress is not enabled. It the underlying OFI provider supports auto data progress, this value is ignored. If the value is -1, this optimization will be turned off.
20250508 102239.561 INFO             PET2 index= 344                             MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX : Specifies the maximum number of iovecs to allocate for RMA operations to/from noncontiguous buffers.
20250508 102239.561 INFO             PET2 index= 345                        MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which OFI native path switches from eager to rendezvous mode. It does not affect the AM path eager limit. Having this gives a way to reliably test native non-path. If the number is positive, OFI will init the MPIDI_OFI_global.max_msg_size to the value of cvar. If the number is negative, OFI will init the MPIDI_OFI_globa.max_msg_size using whatever provider gives (which might be unlimited for socket provider).
20250508 102239.561 INFO             PET2 index= 346                                  MPIR_CVAR_CH4_OFI_MAX_NICS : If set to positive number, this cvar determines the maximum number of physical nics to use (if more than one is available). If the number is -1, underlying netmod or shmmod automatically uses an optimal number depending on what is detected on the system up to the limit determined by MPIDI_MAX_NICS (in ofi_types.h).
20250508 102239.561 INFO             PET2 index= 347                 MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING : If true, this cvar enables striping of large messages across multiple NICs.
20250508 102239.561 INFO             PET2 index= 348              MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD : Striping will happen for message sizes beyond this threshold.
20250508 102239.561 INFO             PET2 index= 349                  MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING : Multi-NIC hashing means to use more than one NIC to send and receive messages above a certain size.  If set to positive number, this feature will be turned on. If set to 0, this feature will be turned off. If the number is -1, MPICH automatically determines whether to use multi-nic hashing depending on what is detected on the system (e.g., number of NICs available, number of processes sharing the NICs).
20250508 102239.561 INFO             PET2 index= 350                     MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE : Controls the multirecv am buffer size. It is recommended to match this to the hugepage size so that the buffer can be allocated at the page boundary.
20250508 102239.561 INFO             PET2 index= 351                                  MPIR_CVAR_OFI_USE_MIN_NICS : If true and all nodes do not have the same number of NICs, MPICH will fall back to using the fewest number of NICs instead of returning an error.
20250508 102239.561 INFO             PET2 index= 352                          MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED : If true, enable OFI triggered ops for MPI collectives.
20250508 102239.561 INFO             PET2 index= 353                      MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the sender side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET2 index= 354                   MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the receiver side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET2 index= 355                       MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE : If true, enable pipeline for GPU data transfer. GPU pipeline does not support non-contiguous datatypes or mixed buffer types (i.e. GPU send buffer, host recv buffer). If GPU pipeline is enabled, the unsupported scenarios will cause undefined behavior if encountered.
20250508 102239.561 INFO             PET2 index= 356                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD : This is the threshold to start using GPU pipeline.
20250508 102239.561 INFO             PET2 index= 357                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU pipeline data transfer.
20250508 102239.561 INFO             PET2 index= 358        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU pipeline data transfer in each block/chunk of the pool.
20250508 102239.561 INFO             PET2 index= 359              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU pipeline data transfer
20250508 102239.561 INFO             PET2 index= 360              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the sender side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET2 index= 361              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the receiver side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET2 index= 362                      MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE : Avoid use fi_inject_write. For some provider, e.g. tcp;ofi_rxm, inject write may break the synchronization.
20250508 102239.561 INFO             PET2 index= 363                                       MPIR_CVAR_UCX_DT_RECV : Variable to select method for receiving noncontiguous data
true                - Use UCX datatype with pack/unpack callbacks
false               - MPICH will decide to pack/unpack at completion or use IOVs
based on the datatype
20250508 102239.561 INFO             PET2 index= 364                          MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE : By default, we will cache ipc handles using the specialized cache mechanism. If the
gpu-specific backend does not implement a specialized cache, then we will fallback to
the generic cache mechanism. Users can optionally force the generic cache mechanism or
disable ipc caching entirely.
generic - use the cache mechanism in the generic layer
specialized - use the cache mechanism in a gpu-specific mpl layer (if applicable)
disabled - disable caching completely
20250508 102239.561 INFO             PET2 index= 365                         MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD (in bytes), then enable GPU-based single copy protocol for intranode communication. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET2 index= 366                    MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE : If a send message size is less than or equal to MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE (in bytes), then enable GPU-basedfast memcpy. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET2 index= 367                       MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE : Variable to select implementation for ZE shareable IPC handle
pidfd - use pidfd_getfd syscall to implement shareable IPC handle
drmfd - force to use device fd-based shareable IPC handle
20250508 102239.561 INFO             PET2 index= 368                           MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE : By default, select engine type automatically
auto - select automatically
compute - use compute engine
copy_high_bandwidth - use high-bandwidth copy engine
copy_low_latency - use low-latency copy engine
20250508 102239.561 INFO             PET2 index= 369                   MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL : By default, use read protocol.
auto - select automatically
read - use read protocol
write - use write protocol if remote device is visible
20250508 102239.561 INFO             PET2 index= 370                           MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR : If an address is used more than once in the last ten send operations, map it for IPC use even if it is below the IPC threshold.
20250508 102239.561 INFO             PET2 index= 371                                  MPIR_CVAR_CH4_XPMEM_ENABLE : To manually disable XPMEM set to 0. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET2 index= 372                       MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD (in bytes), then enable XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET2 index= 373                       MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
ipc_read - Uses read-based collective with ipc
20250508 102239.561 INFO             PET2 index= 374                      MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET2 index= 375                      MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET2 index= 376                     MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET2 index= 377                   MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node allreduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET2 index= 378                     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node barrier
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET2 index= 379                    MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node alltoall
mpir           - Fallback to MPIR collectives (default)
ipc_read    - Uses read-based collective with ipc
20250508 102239.561 INFO             PET2 index= 380                              MPIR_CVAR_POSIX_POLL_FREQUENCY : This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.561 INFO             PET2 index= 381                 MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read bcast only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET2 index= 382              MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read alltoall only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET2 index= 383                         MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD : Use posix optimized collectives (release_gather) only when the total number of Bcast, Reduce, Barrier, and Allreduce calls on the node level communicator is more than this threshold.
20250508 102239.561 INFO             PET2 index= 384                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : If non-empty, this cvar specifies which shm posix eager module to use
20250508 102239.561 INFO             PET2 index= 385         MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.561 INFO             PET2 index= 386                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS : The number of cells used for the depth of the iqueue.
20250508 102239.561 INFO             PET2 index= 387                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE : Size of each cell.
20250508 102239.561 INFO             PET2 index= 388                           MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE : Maximum shared memory created per node for optimized intra-node collectives (in KB)
20250508 102239.561 INFO             PET2 index= 389                 MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the bcast buffer (in bytes)
20250508 102239.561 INFO             PET2 index= 390                         MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS : Number of cells the bcast buffer is divided into
20250508 102239.561 INFO             PET2 index= 391                MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the reduce buffer per rank (in bytes)
20250508 102239.561 INFO             PET2 index= 392                        MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS : Number of cells the reduce buffer is divided into, for each rank
20250508 102239.561 INFO             PET2 index= 393                         MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node bcast
20250508 102239.561 INFO             PET2 index= 394                         MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE : Tree type for intra-node bcast tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET2 index= 395                        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node reduce
20250508 102239.561 INFO             PET2 index= 396                        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE : Tree type for intra-node reduce tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET2 index= 397             MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES : Enable collective specific intra-node trees which leverage the memory hierarchy of a machine. Depends on hwloc to extract the binding information of each rank. Pick a leader rank per package (socket), then create a per_package tree for ranks on a same package, package leaders tree for package leaders. For Bcast - Assemble the per_package and package_leaders tree in such a way that leaders interact among themselves first before interacting with package local ranks. Both the package_leaders and per_package trees are left skewed (children are added from left to right, first child to be added is the first one to be processed in traversal) For Reduce - Assemble the per_package and package_leaders tree in such a way that a leader rank interacts with its package local ranks first, then with the other package leaders. Both the per_package and package_leaders tree is right skewed (children are added in reverse order, first child to be added is the last one to be processed in traversal) The tree radix and tree type of
20250508 102239.561 INFO             PET2 index= 398                               MPIR_CVAR_BARRIER_COMPOSITION : Select composition (inter_node + intra_node) for Barrier 0 Auto selection 1 NM + SHM 2 NM only
20250508 102239.561 INFO             PET2 index= 399                                 MPIR_CVAR_BCAST_COMPOSITION : Select composition (inter_node + intra_node) for Bcast 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.561 INFO             PET2 index= 400                             MPIR_CVAR_ALLREDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Allreduce 0 Auto selection 1 NM + SHM with reduce + bcast 2 NM only composition 3 SHM only composition 4 Multi leaders based inter node + intra node composition
20250508 102239.562 INFO             PET2 index= 401                             MPIR_CVAR_ALLGATHER_COMPOSITION : Select composition (inter_node + intra_node) for Allgather 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.562 INFO             PET2 index= 402                              MPIR_CVAR_ALLTOALL_COMPOSITION : Select composition (inter_node + intra_node) for Alltoall 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.562 INFO             PET2 index= 403                                MPIR_CVAR_REDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Reduce 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.562 INFO             PET2 index= 404                             MPIR_CVAR_ALLTOALL_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Alltoall (in bytes)
20250508 102239.562 INFO             PET2 index= 405                            MPIR_CVAR_ALLGATHER_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Allgather (in bytes)
20250508 102239.562 INFO             PET2 index= 406                                   MPIR_CVAR_NUM_MULTI_LEADS : Number of leader ranks per node to be used for multi-leaders based collective algorithms
20250508 102239.562 INFO             PET2 index= 407                          MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER : Shared memory region per node-leader for multi-leaders based composition for MPI_Allreduce (in bytes) If it is undefined by the user, it is set to the message size of the first call to the algorithm. Max shared memory size is limited to 4MB.
20250508 102239.562 INFO             PET2 index= 408                        MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER : Amount of data reduced in allreduce delta composition's reduce local step (in bytes). Smaller msg size per leader avoids cache misses and improves performance. Experiments indicate 512 to be the best value.
20250508 102239.562 INFO             PET2 index= 409                      MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS : number of offsets in the allreduce delta composition's local copy The value of 2 performed the best in our 2 NIC test cases.
20250508 102239.562 INFO             PET2 index= 410                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20250508 102239.562 INFO             PET2 index= 411                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20250508 102239.562 INFO             PET2 index= 412                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20250508 102239.562 INFO             PET2 index= 413                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20250508 102239.562 INFO             PET2 index= 414                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) lockless
20250508 102239.562 INFO             PET2 index= 415                                      MPIR_CVAR_CH4_NUM_VCIS : Sets the number of VCIs to be implicitly used (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET2 index= 416                                  MPIR_CVAR_CH4_RESERVE_VCIS : Sets the number of VCIs that user can explicitly allocate (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET2 index= 417               MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.562 INFO             PET2 index= 418                               MPIR_CVAR_CH4_IOV_DENSITY_MIN : Defines the threshold of high-density datatype. The density is calculated by (datatype_size / datatype_num_contig_blocks).
20250508 102239.562 INFO             PET2 index= 419                              MPIR_CVAR_CH4_PACK_BUFFER_SIZE : Specifies the number of buffers for packing/unpacking active messages in each block of the pool. The size here should be greater or equal to the max of the eager buffer limit of SHM and NETMOD.
20250508 102239.562 INFO             PET2 index= 420                    MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK : Specifies the number of buffers for packing/unpacking active messages in each block of the pool.
20250508 102239.562 INFO             PET2 index= 421                          MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS : Specifies the max number of buffers for packing/unpacking buffers in the pool. Use 0 for unlimited.
20250508 102239.562 INFO             PET2 index= 422                       MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU collectives data transfer.
20250508 102239.562 INFO             PET2 index= 423                MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU collectives data transfer in each block/chunk of the pool.
20250508 102239.562 INFO             PET2 index= 424                      MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU collectives data transfer.
20250508 102239.562 INFO             PET2 index= 425                               MPIR_CVAR_CH4_GLOBAL_PROGRESS : If on, poll global progress every once a while. With per-vci configuration, turning global progress off may improve the threading performance.
20250508 102239.562 INFO             PET2 index= 426                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.562 INFO             PET2 index= 427                           MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ : Enable stream enqueue operations via stream work queue. Requires progress thread on the corresponding MPIX stream. Reference: MPIX_Stream_progress and MPIX_Start_progress_thread.
20250508 102239.562 INFO             PET2 index= 428                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20250508 102239.562 INFO             PET2 index= 429                MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS : If true, allows RMA synchronization calls to dynamically reduce the frequency of internal progress polling for incoming RMA active messages received on the target process. The RMA synchronization call initially polls progress with a low frequency (defined by MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL) to reduce synchronization overhead. Once any RMA active message has been received, it will always poll progress once at every synchronization call to ensure prompt target-side progress. Effective only for passive target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}.
20250508 102239.562 INFO             PET2 index= 430                      MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL : Specifies a static interval of progress polling for incoming RMA active messages received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Invalid when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET2 index= 431             MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL : Specifies the interval of progress polling with low frequency for incoming RMA active message received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Used when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET2 index= 432            MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE : The genq shmem code allocates pools of cells on each process and, when needed, a cell is removed from the pool and passed to another process. This can happen by either removing a cell from the pool of the sending process or from the pool of the receiving process. This CVAR determines which pool to use. If true, the cell will come from the sender-side. If false, the cell will com from the receiver-side. There are specific advantages of using receiver-side cells when combined with the "avx" fast configure option, which allows MPICH to use AVX streaming copy intrintrinsics, when available, to avoid polluting the cache of the sender with the data being copied to the receiver. Using receiver-side cells does have the trade-off of requiring an MPMC lock for the free queue rather than an MPSC lock, which is used for sender-side cells. Initial performance analysis shows that using the MPMC lock in this case had no significant performance loss. By default, the queue will continue to use sender-side queues until the pe
20250508 102239.562 INFO             PET2 index= 433                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20250508 102239.562 INFO             PET2 index= 434                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20250508 102239.562 INFO             PET2 index= 435                             MPIR_CVAR_SHM_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20250508 102239.562 INFO             PET2 index= 436                                 MPIR_CVAR_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20250508 102239.562 INFO             PET2 index= 437                                MPIR_CVAR_ENABLE_HEAVY_YIELD : If enabled, use nanosleep to ensure other threads have a chance to grab the lock. Note: this may not work with some thread runtimes, e.g. non-preemptive user-level threads.
20250508 102239.562 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20250508 102239.562 INFO             PET2 main: --- VMK::log() start -------------------------------------
20250508 102239.562 INFO             PET2 main: vm located at: 0x148e06570
20250508 102239.562 INFO             PET2 main: mpionly=1 threadsflag=0
20250508 102239.562 INFO             PET2 main: ssiCount=1 localSsi=0
20250508 102239.562 INFO             PET2 main: devCount=0 ssiLocalDevCount=0
20250508 102239.562 INFO             PET2 main: petCount=6 ssiLocalPetCount=6
20250508 102239.562 INFO             PET2 main: localPet=2 mypthid=0x1edb24f40 ssiLocalPet=2 currentSsiPe=-1
20250508 102239.562 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.562 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.562 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20250508 102239.562 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.562 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20250508 102239.562 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.562 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20250508 102239.562 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.562 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20250508 102239.562 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.562 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20250508 102239.562 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.562 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20250508 102239.562 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20250508 102239.563 INFO             PET2 Executing 'userm1_setvm'
20250508 102239.563 INFO             PET2 Executing 'userm1_register'
20250508 102239.563 INFO             PET2 Executing 'userm2_setvm'
20250508 102239.563 DEBUG            PET2 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 DEBUG            PET2 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20250508 102239.563 INFO             PET2 model1: vm located at: 0x13a804e80
20250508 102239.563 INFO             PET2 model1: mpionly=1 threadsflag=0
20250508 102239.563 INFO             PET2 model1: ssiCount=1 localSsi=0
20250508 102239.563 INFO             PET2 model1: devCount=0 ssiLocalDevCount=0
20250508 102239.563 INFO             PET2 model1: petCount=6 ssiLocalPetCount=6
20250508 102239.563 INFO             PET2 model1: localPet=2 mypthid=0x1edb24f40 ssiLocalPet=2 currentSsiPe=-1
20250508 102239.563 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.563 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.563 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20250508 102239.563 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.563 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20250508 102239.563 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.563 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20250508 102239.563 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.563 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20250508 102239.563 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.563 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20250508 102239.563 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.563 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20250508 102239.563 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20250508 102239.568 INFO             PET2 Entering 'user1_run'
20250508 102239.568 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102239.840 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102240.070 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102240.295 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102240.521 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20250508 102240.748 INFO             PET2 Exiting 'user1_run'
20250508 102249.040 INFO             PET2  NUMBER_OF_PROCESSORS           6
20250508 102249.040 INFO             PET2  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20250508 102249.040 INFO             PET2  Finalizing ESMF
20250508 102249.040 INFO             PET2 ESMCI_IO_Handler.C:335 ESMCI::IO_Handler::finalize() 
20250508 102249.040 INFO             PET2 ESMCI_PIO_Handler.C:357 ESMCI::PIO_Handler::finalize() 
20250508 102249.040 INFO             PET2 ESMCI_IO_Handler.C:337 ESMCI::IO_Handler::finalize() after finalize, localrc = 0
20250508 102249.040 INFO             PET2 ESMCI_IO_Handler.C:360 ESMCI::IO_Handler::finalize() before return, localrc = 0
20250508 102239.555 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20250508 102239.556 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20250508 102239.556 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20250508 102239.556 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20250508 102239.556 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET3 Running with ESMF Version   : v8.9.0b08-5-g079ca68fdc
20250508 102239.556 INFO             PET3 ESMF library build date/time: "May  8 2025" "10:06:33"
20250508 102239.556 INFO             PET3 ESMF library build location : /Users/oehmke/ESMF_AutoTest/gfortranclang_12.2.0_14.0.0_mpich_g_develop/esmf
20250508 102239.556 INFO             PET3 ESMF_COMM                   : mpich
20250508 102239.556 INFO             PET3 ESMF_MOAB                   : enabled
20250508 102239.556 INFO             PET3 ESMF_LAPACK                 : enabled
20250508 102239.556 INFO             PET3 ESMF_NETCDF                 : enabled
20250508 102239.556 INFO             PET3 ESMF_PNETCDF                : disabled
20250508 102239.556 INFO             PET3 ESMF_PIO                    : enabled
20250508 102239.556 INFO             PET3 ESMF_YAMLCPP                : enabled
20250508 102239.556 INFO             PET3 --- VMK::logSystem() start -------------------------------
20250508 102239.556 INFO             PET3 esmfComm=mpich
20250508 102239.556 INFO             PET3 isPthreadsEnabled=1
20250508 102239.556 INFO             PET3 isOpenMPEnabled=0
20250508 102239.557 INFO             PET3 isOpenACCEnabled=0
20250508 102239.557 INFO             PET3 isSsiSharedMemoryEnabled=1
20250508 102239.557 INFO             PET3 isNvmlEnabled=0
20250508 102239.557 INFO             PET3 isNumaEnabled=0
20250508 102239.557 INFO             PET3 ssiCount=1 peCount=6
20250508 102239.557 INFO             PET3 PE=0 SSI=0 SSIPE=0
20250508 102239.557 INFO             PET3 PE=1 SSI=0 SSIPE=1
20250508 102239.557 INFO             PET3 PE=2 SSI=0 SSIPE=2
20250508 102239.557 INFO             PET3 PE=3 SSI=0 SSIPE=3
20250508 102239.557 INFO             PET3 PE=4 SSI=0 SSIPE=4
20250508 102239.557 INFO             PET3 PE=5 SSI=0 SSIPE=5
20250508 102239.557 INFO             PET3 ndevs=0 ndevsSSI=0
20250508 102239.557 INFO             PET3 
20250508 102239.557 INFO             PET3 --- VMK::logSystem() MPI Layer ---------------------------
20250508 102239.557 INFO             PET3 MPI_VERSION=4
20250508 102239.557 INFO             PET3 MPI_SUBVERSION=1
20250508 102239.557 INFO             PET3 MPICH_VERSION=4.2.3
20250508 102239.557 INFO             PET3 mpi_t_okay=1
20250508 102239.557 INFO             PET3 --- VMK::logSystem() MPI Tool Interface Control Vars ---
20250508 102239.557 INFO             PET3 index=   0                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb            - Force nonblocking algorithm
smp           - Force smp algorithm
k_dissemination - Force high radix dissemination algorithm
recexch       - Force recursive exchange algorithm
20250508 102239.557 INFO             PET3 index=   1                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
bcast - Force bcast algorithm
nb    - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=   2                               MPIR_CVAR_BARRIER_DISSEM_KVAL : k value for dissemination exchange based barrier algorithm
20250508 102239.557 INFO             PET3 index=   3                              MPIR_CVAR_BARRIER_RECEXCH_KVAL : k value for recursive exchange based allreduce based barrier
20250508 102239.557 INFO             PET3 index=   4                 MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.557 INFO             PET3 index=   5                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20250508 102239.557 INFO             PET3 index=   6                              MPIR_CVAR_IBARRIER_DISSEM_KVAL : k value for dissemination exchange based ibarrier
20250508 102239.557 INFO             PET3 index=   7                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
tsp_recexch - Force generic transport based recursive exchange algorithm
tsp_k_dissemination - Force generic transport based high-radix dissemination algorithm
20250508 102239.557 INFO             PET3 index=   8                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_bcast - Force bcast algorithm
20250508 102239.557 INFO             PET3 index=   9                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET3 index=  10                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET3 index=  11                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET3 index=  12                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes.
20250508 102239.557 INFO             PET3 index=  13                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial                                - Force Binomial Tree
nb                                      - Force nonblocking algorithm
smp                                     - Force smp algorithm
scatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgather
scatter_ring_allgather                  - Force Scatter Ring
pipelined_tree                          - Force tree-based pipelined algorithm
tree                                    - Force tree-based algorithm
20250508 102239.557 INFO             PET3 index=  14                                   MPIR_CVAR_BCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based bcast
20250508 102239.557 INFO             PET3 index=  15                                   MPIR_CVAR_BCAST_TREE_TYPE : Tree type for tree based bcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.557 INFO             PET3 index=  16                         MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.557 INFO             PET3 index=  17                               MPIR_CVAR_BCAST_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.557 INFO             PET3 index=  18                            MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.557 INFO             PET3 index=  19                          MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.557 INFO             PET3 index=  20                          MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.557 INFO             PET3 index=  21                             MPIR_CVAR_BCAST_IS_NON_BLOCKING : If set to true, MPI_Bcast will use non-blocking send.
20250508 102239.557 INFO             PET3 index=  22                    MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE : Indicates the chunk size for pipelined bcast.
20250508 102239.557 INFO             PET3 index=  23                               MPIR_CVAR_BCAST_RECV_PRE_POST : If set to true, MPI_Bcast will pre-post all the receives.
20250508 102239.557 INFO             PET3 index=  24                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                      - Force nonblocking algorithm
remote_send_local_bcast - Force remote-send-local-bcast algorithm
20250508 102239.557 INFO             PET3 index=  25                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20250508 102239.557 INFO             PET3 index=  26                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.557 INFO             PET3 index=  27                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET3 index=  28                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET3 index=  29                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial                             - Force Binomial algorithm
sched_smp                                  - Force smp algorithm
sched_scatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithm
sched_scatter_ring_allgather               - Force Scatter Ring Allgather algorithm
tsp_tree                               - Force Generic Transport Tree algorithm
tsp_scatterv_recexch_allgatherv        - Force Generic Transport Scatterv followed by Recursive Exchange Allgatherv algorithm
tsp_scatterv_ring_allgatherv           - Force Generic Transport Scatterv followed by Ring Allgatherv algorithm
tsp_ring                               - Force Generic Transport Ring algorithm
20250508 102239.557 INFO             PET3 index=  30                              MPIR_CVAR_IBCAST_SCATTERV_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET3 index=  31                    MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET3 index=  32                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_flat - Force flat algorithm
20250508 102239.557 INFO             PET3 index=  33                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20250508 102239.557 INFO             PET3 index=  34                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=  35                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                   - Force linear algorithm
local_gather_remote_send - Force local-gather-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=  36                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET3 index=  37                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20250508 102239.557 INFO             PET3 index=  38                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_long  - Force long inter algorithm
sched_short - Force short inter algorithm
20250508 102239.557 INFO             PET3 index=  39                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=  40                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=  41                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear         - Force linear algorithm
tsp_linear       - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET3 index=  42                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET3 index=  43                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20250508 102239.557 INFO             PET3 index=  44                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=  45                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                    - Force linear algorithm
nb                        - Force nonblocking algorithm
remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET3 index=  46                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET3 index=  47                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20250508 102239.557 INFO             PET3 index=  48                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear                    - Force linear algorithm
sched_remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET3 index=  49                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=  50                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET3 index=  51                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET3 index=  52                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET3 index=  53                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET3 index=  54                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET3 index=  55                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
k_brucks           - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
recexch_doubling   - Force recexch distance doubling algorithm
recexch_halving    - Force recexch distance halving algorithm
20250508 102239.558 INFO             PET3 index=  56                             MPIR_CVAR_ALLGATHER_BRUCKS_KVAL : radix (k) value for generic transport brucks based allgather
20250508 102239.558 INFO             PET3 index=  57                            MPIR_CVAR_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather
20250508 102239.558 INFO             PET3 index=  58               MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET3 index=  59                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
nb                        - Force nonblocking algorithm
20250508 102239.558 INFO             PET3 index=  60                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20250508 102239.558 INFO             PET3 index=  61                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20250508 102239.558 INFO             PET3 index=  62                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_ring               - Force ring algorithm
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
tsp_ring       - Force generic transport ring algorithm
tsp_brucks     - Force generic transport based brucks algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
20250508 102239.558 INFO             PET3 index=  63                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20250508 102239.558 INFO             PET3 index=  64                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20250508 102239.558 INFO             PET3 index=  65                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
20250508 102239.558 INFO             PET3 index=  66                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET3 index=  67                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20250508 102239.558 INFO             PET3 index=  68                           MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL : k value for radix in brucks based iallgatherv
20250508 102239.558 INFO             PET3 index=  69                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_ring               - Force ring algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
tsp_ring             - Force generic transport ring algorithm
tsp_brucks           - Force generic transport based brucks algorithm
20250508 102239.558 INFO             PET3 index=  70                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET3 index=  71                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20250508 102239.558 INFO             PET3 index=  72                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20250508 102239.558 INFO             PET3 index=  73                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20250508 102239.558 INFO             PET3 index=  74                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks                    - Force brucks algorithm
k_brucks                  - Force Force radix k brucks algorithm
nb                        - Force nonblocking algorithm
pairwise                  - Force pairwise algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET3 index=  75                              MPIR_CVAR_ALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based alltoall
20250508 102239.558 INFO             PET3 index=  76                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET3 index=  77                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks            - Force brucks algorithm
sched_inplace           - Force inplace algorithm
sched_pairwise          - Force pairwise algorithm
sched_permuted_sendrecv - Force permuted sendrecv algorithm
tsp_ring            - Force generic transport based ring algorithm
tsp_brucks          - Force generic transport based brucks algorithm
tsp_scattered       - Force generic transport based scattered algorithm
20250508 102239.558 INFO             PET3 index=  78                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET3 index=  79                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET3 index=  80                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
pairwise_exchange - Force pairwise exchange algorithm
nb                - Force nonblocking algorithm
20250508 102239.558 INFO             PET3 index=  81                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_scattered       - Force generic transport based scattered algorithm
tsp_blocked         - Force generic transport blocked algorithm
tsp_inplace         - Force generic transport inplace algorithm
20250508 102239.558 INFO             PET3 index=  82                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET3 index=  83            MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.558 INFO             PET3 index=  84                   MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.558 INFO             PET3 index=  85                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET3 index=  86                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET3 index=  87                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_blocked   - Force generic transport based blocked algorithm
tsp_inplace   - Force generic transport based inplace algorithm
20250508 102239.558 INFO             PET3 index=  88                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET3 index=  89                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET3 index=  90                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20250508 102239.558 INFO             PET3 index=  91                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial              - Force binomial algorithm
nb                    - Force nonblocking algorithm
smp                   - Force smp algorithm
reduce_scatter_gather - Force reduce scatter gather algorithm
20250508 102239.558 INFO             PET3 index=  92                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_reduce_remote_send - Force local-reduce-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.558 INFO             PET3 index=  93                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20250508 102239.558 INFO             PET3 index=  94                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET3 index=  95                       MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET3 index=  96                             MPIR_CVAR_IREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET3 index=  97                          MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET3 index=  98                        MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET3 index=  99                        MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET3 index= 100                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET3 index= 101                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET3 index= 102                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET3 index= 103                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                   - Force smp algorithm
sched_binomial              - Force binomial algorithm
sched_reduce_scatter_gather - Force reduce scatter gather algorithm
tsp_tree                - Force Generic Transport Tree
tsp_ring                - Force Generic Transport Ring
20250508 102239.558 INFO             PET3 index= 104                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_reduce_remote_send - Force local-reduce-remote-send algorithm
20250508 102239.558 INFO             PET3 index= 105                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET3 index= 106                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20250508 102239.558 INFO             PET3 index= 107                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                       - Force nonblocking algorithm
smp                      - Force smp algorithm
recursive_doubling       - Force recursive doubling algorithm
reduce_scatter_allgather - Force reduce scatter allgather algorithm
tree                     - Force pipelined tree algorithm
recexch                  - Force generic transport recursive exchange algorithm
ring                     - Force ring algorithm
k_reduce_scatter_allgather - Force reduce scatter allgather algorithm
20250508 102239.558 INFO             PET3 index= 108                               MPIR_CVAR_ALLREDUCE_TREE_TYPE : Tree type for tree based allreduce knomial_1 is default as it supports both commutative and non-commutative reduce operations kary      - kary tree type knomial_1 - knomial_1 tree type (tree grows starting from the left of the root) knomial_2 - knomial_2 tree type (tree grows starting from the right of the root) topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET3 index= 109                               MPIR_CVAR_ALLREDUCE_TREE_KVAL : Indicates the branching factor for kary or knomial trees.
20250508 102239.558 INFO             PET3 index= 110                     MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET3 index= 111                           MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET3 index= 112                        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET3 index= 113                      MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET3 index= 114                      MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET3 index= 115                MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based allreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET3 index= 116                   MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET3 index= 117                            MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based allreduce
20250508 102239.558 INFO             PET3 index= 118               MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET3 index= 119                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                    - Force nonblocking algorithm
reduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20250508 102239.558 INFO             PET3 index= 120                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20250508 102239.558 INFO             PET3 index= 121                              MPIR_CVAR_IALLREDUCE_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.558 INFO             PET3 index= 122               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET3 index= 123                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET3 index= 124                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20250508 102239.558 INFO             PET3 index= 125                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_naive                      - Force naive algorithm
sched_smp                        - Force smp algorithm
sched_recursive_doubling         - Force recursive doubling algorithm
sched_reduce_scatter_allgather   - Force reduce scatter allgather algorithm
tsp_recexch_single_buffer    - Force generic transport recursive exchange with single buffer for receives
tsp_recexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
tsp_tree                     - Force generic transport tree algorithm
tsp_ring                     - Force generic transport ring algorithm
tsp_recexch_reduce_scatter_recexch_allgatherv  - Force generic transport recursive exchange with reduce scatter and allgatherv
20250508 102239.558 INFO             PET3 index= 126                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20250508 102239.558 INFO             PET3 index= 127          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20250508 102239.558 INFO             PET3 index= 128                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
noncommutative     - Force noncommutative algorithm
pairwise           - Force pairwise algorithm
recursive_doubling - Force recursive doubling algorithm
recursive_halving  - Force recursive halving algorithm
20250508 102239.558 INFO             PET3 index= 129                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET3 index= 130                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20250508 102239.558 INFO             PET3 index= 131                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET3 index= 132                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET3 index= 133              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
noncommutative     - Force noncommutative algorithm
recursive_doubling - Force recursive doubling algorithm
pairwise           - Force pairwise algorithm
recursive_halving  - Force recursive halving algorithm
nb                 - Force nonblocking algorithm
20250508 102239.558 INFO             PET3 index= 134              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET3 index= 135                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20250508 102239.558 INFO             PET3 index= 136             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET3 index= 137             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET3 index= 138                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
smp                - Force smp algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET3 index= 139                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                  - Force smp algorithm
sched_recursive_doubling   - Force recursive doubling algorithm
tsp_recursive_doubling - Force generic transport recursive doubling algorithm
20250508 102239.558 INFO             PET3 index= 140                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.558 INFO             PET3 index= 141                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
20250508 102239.559 INFO             PET3 index= 142                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET3 index= 143                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET3 index= 144               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 145               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 146               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 147               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 148              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 149              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 150                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 151                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 152                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 153                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 154                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 155                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 156               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 157               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 158                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 159                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET3 index= 160               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 161               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET3 index= 162                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 163                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 164                    MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 165                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 166                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 167                      MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 168                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 169                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 170                     MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 171                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 172                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 173                    MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 174                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 175                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 176                    MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 177                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 178                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 179                   MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 180                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 181                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 182                  MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 183                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 184                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 185                 MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 186                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 187                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 188                   MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 189                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 190                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 191                  MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 192                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 193                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 194                  MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 195                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 196                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 197                     MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 198                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 199                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 200                  MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 201                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 202                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 203             MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 204            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 205           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 206       MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 207                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 208                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 209                       MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 210                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 211                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 212                     MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 213              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 214             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 215         MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 216             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 217            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 218        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 219               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 220              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 221          MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 222              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 223             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 224         MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET3 index= 225              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET3 index= 226             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET3 index= 227         MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET3 index= 228                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20250508 102239.560 INFO             PET3 index= 229                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20250508 102239.560 INFO             PET3 index= 230                             MPIR_CVAR_IALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based ialltoall
20250508 102239.560 INFO             PET3 index= 231                   MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR : If set to true, the tsp based brucks algorithm will allocate dedicated send and receive buffers for every neighbor in the brucks algorithm. Otherwise, it would reuse a single buffer for sending and receiving data to/from neighbors
20250508 102239.560 INFO             PET3 index= 232             MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.560 INFO             PET3 index= 233                    MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.560 INFO             PET3 index= 234                                MPIR_CVAR_DEVICE_COLLECTIVES : Variable to select whether the device can override the
MPIR-level collective algorithms.
all     - Always prefer the device collectives
none    - Never pick the device collectives
percoll - Use the per-collective CVARs to decide
20250508 102239.560 INFO             PET3 index= 235                               MPIR_CVAR_COLLECTIVE_FALLBACK : Variable to control what the MPI library should do if the
user-specified collective algorithm does not work for the
arguments passed in by the user.
error   - throw an error
print   - print an error message and fallback to the internally selected algorithm
silent  - silently fallback to the internally selected algorithm
20250508 102239.560 INFO             PET3 index= 236                   MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.560 INFO             PET3 index= 237                                    MPIR_CVAR_HIERARCHY_DUMP : If set to true, each rank will dump the hierarchy data structure to a file named "hierarchy[rank]" in the current folder. If set to false, the hierarchy data structure will not be dumped.
20250508 102239.560 INFO             PET3 index= 238                                  MPIR_CVAR_COORDINATES_FILE : Defines the location of the input coordinates file.
20250508 102239.560 INFO             PET3 index= 239                                    MPIR_CVAR_COLL_TREE_DUMP : If set to true, each rank will dump the tree to a file named "colltree[rank].json" in the current folder. If set to false, the tree will not be dumped.
20250508 102239.560 INFO             PET3 index= 240                                  MPIR_CVAR_COORDINATES_DUMP : If set to true, rank 0 will dump the network coordinates to a file named "coords" in the current folder. If set to false, the network coordinates will not be dumped.
20250508 102239.560 INFO             PET3 index= 241                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20250508 102239.560 INFO             PET3 index= 242                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20250508 102239.560 INFO             PET3 index= 243                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20250508 102239.560 INFO             PET3 index= 244                                MPIR_CVAR_DATALOOP_FAST_SEEK : use a datatype-specialized algorithm to shortcut seeking to the correct location in a noncontiguous buffer
20250508 102239.560 INFO             PET3 index= 245                             MPIR_CVAR_YAKSA_COMPLEX_SUPPORT : This CVAR indicates that complex type reduction is not supported in yaksa.
20250508 102239.560 INFO             PET3 index= 246                                MPIR_CVAR_GPU_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET3 index= 247                           MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET3 index= 248                            MPIR_CVAR_ENABLE_YAKSA_REDUCTION : This cvar enables yaksa based reduction for local reduce.
20250508 102239.560 INFO             PET3 index= 249                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20250508 102239.560 INFO             PET3 index= 250                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20250508 102239.560 INFO             PET3 index= 251                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20250508 102239.560 INFO             PET3 index= 252                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20250508 102239.560 INFO             PET3 index= 253                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20250508 102239.560 INFO             PET3 index= 254                          MPIR_CVAR_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. Can be set to auto or comma-separated list of logical processors. When set to auto - MPICH will automatically select logical CPU cores to decide affinity of the progress threads. When set to comma-separated list of logical processors - In case of N progress threads per process, the first N logical processors from list will be assigned to threads of first local process, the next N logical processors from list - to second local process and so on. For example, thread affinity is "0,1,2,3", 2 progress threads per process and 2 processes per node. Progress threads of first local process will be pinned on logical processors "0,1", progress threads of second local process - on "2,3". Cannot work together with MPIR_CVAR_NUM_CLIQUES or MPIR_CVAR_ODD_EVEN_CLIQUES.
20250508 102239.560 INFO             PET3 index= 255                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20250508 102239.560 INFO             PET3 index= 256                                 MPIR_CVAR_COREDUMP_ON_ABORT : Call libc abort() to generate a corefile
20250508 102239.560 INFO             PET3 index= 257                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20250508 102239.560 INFO             PET3 index= 258                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20250508 102239.560 INFO             PET3 index= 259                                     MPIR_CVAR_DEBUG_SUMMARY : If true, print internal summary of various debug information, such as memory allocation by category. Each layer may print their own summary information. For example, ch4-ofi may print its provider capability settings.
20250508 102239.560 INFO             PET3 index= 260                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20250508 102239.560 INFO             PET3 index= 261                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20250508 102239.560 INFO             PET3 index= 262                    MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST : If true, mpl/ze will use immediate command list for copying
20250508 102239.560 INFO             PET3 index= 263                    MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES : If true, mpl/ze will use command queues in a round-robin fashion. If false, only command queues of index 0 will be used.
20250508 102239.560 INFO             PET3 index= 264                            MPIR_CVAR_NO_COLLECTIVE_FINALIZE : If true, prevent MPI_Finalize to invoke collective behavior such as barrier or communicating to other processes. Consequently, it may result in leaking memory or losing messages due to pre-mature exiting. The default is false, which may invoke collective behaviors at finalize.
20250508 102239.560 INFO             PET3 index= 265                                     MPIR_CVAR_FINALIZE_WAIT : If true, poll progress at MPI_Finalize until reference count on MPI_COMM_WORLD and MPI_COMM_SELF reaches zero. This may be necessary to prevent remote processes hanging if it has pending communication protocols, e.g. a rendezvous send.
20250508 102239.560 INFO             PET3 index= 266                                 MPIR_CVAR_REQUEST_ERR_FATAL : By default, MPI_Waitall, MPI_Testall, MPI_Waitsome, and MPI_Testsome return MPI_ERR_IN_STATUS when one of the request fails. If MPIR_CVAR_REQUEST_ERR_FATAL is set to true, these routines will return the error code of the request immediately. The default MPI_ERRS_ARE_FATAL error handler will dump a error stack in this case, which maybe more convenient for debugging. This cvar will also make nonblocking shched return error right away as it issues operations.
20250508 102239.560 INFO             PET3 index= 267                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during MPI_{Waitany,Waitsome} in terms of number of processed requests before polling.
20250508 102239.560 INFO             PET3 index= 268                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20250508 102239.560 INFO             PET3 index= 269                            MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT : Sets the timeout in seconds to dump outstanding requests when progress wait is not making progress for some time.
20250508 102239.560 INFO             PET3 index= 270                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20250508 102239.560 INFO             PET3 index= 271                                    MPIR_CVAR_QMPI_TOOL_LIST : Set the number and order of QMPI tools to be loaded by the MPI library when it is initialized.
20250508 102239.560 INFO             PET3 index= 272                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20250508 102239.560 INFO             PET3 index= 273                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20250508 102239.560 INFO             PET3 index= 274                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20250508 102239.560 INFO             PET3 index= 275                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20250508 102239.560 INFO             PET3 index= 276                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20250508 102239.560 INFO             PET3 index= 277                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20250508 102239.560 INFO             PET3 index= 278                                  MPIR_CVAR_CLIQUES_BY_BLOCK : Specify to divide processes into cliques by uniform blocks. The default is to divide in round-robin fashion. Used for debugging on a single machine.
20250508 102239.560 INFO             PET3 index= 279                                       MPIR_CVAR_PMI_VERSION : Variable to select runtime PMI version.
1        - PMI (default)
2        - PMI2
x        - PMIx
20250508 102239.560 INFO             PET3 index= 280                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20250508 102239.560 INFO             PET3 index= 281                                        MPIR_CVAR_ENABLE_GPU : Control MPICH GPU support. If set to 0, all GPU support is disabled and we do not query the buffer type internally because we assume no GPU buffer is use.
20250508 102239.560 INFO             PET3 index= 282                               MPIR_CVAR_GPU_HAS_WAIT_KERNEL : If set to 1, avoid allocate allocating GPU registered host buffers for temporary buffers. When stream workq and GPU wait kernels are in use, access APIs for GPU registered memory may cause deadlock.
20250508 102239.560 INFO             PET3 index= 283                               MPIR_CVAR_ENABLE_GPU_REGISTER : Control whether to actually register buffers with the GPU runtime in MPIR_gpu_register_host. This could lower the latency of certain GPU communication at the cost of some amount of GPU memory consumed by the MPI library. By default, registration is enabled.
20250508 102239.560 INFO             PET3 index= 284                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.560 INFO             PET3 index= 285                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20250508 102239.560 INFO             PET3 index= 286                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20250508 102239.560 INFO             PET3 index= 287                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20250508 102239.560 INFO             PET3 index= 288                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20250508 102239.560 INFO             PET3 index= 289                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20250508 102239.560 INFO             PET3 index= 290                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET3 index= 291                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET3 index= 292                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20250508 102239.560 INFO             PET3 index= 293                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20250508 102239.560 INFO             PET3 index= 294                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20250508 102239.560 INFO             PET3 index= 295                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.560 INFO             PET3 index= 296               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20250508 102239.560 INFO             PET3 index= 297                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20250508 102239.560 INFO             PET3 index= 298               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchronization approach.  Change this value if programs fail because they run out of requests or other internal resources
20250508 102239.560 INFO             PET3 index= 299                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be negative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20250508 102239.560 INFO             PET3 index= 300            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation available in the ending synchronization call.
20250508 102239.560 INFO             PET3 index= 301                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20250508 102239.560 INFO             PET3 index= 302                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive value.
20250508 102239.560 INFO             PET3 index= 303                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20250508 102239.560 INFO             PET3 index= 304                                    MPIR_CVAR_CH3_PG_VERBOSE : If set, print the PG state on finalize.
20250508 102239.560 INFO             PET3 index= 305                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET3 index= 306                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET3 index= 307                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET3 index= 308                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET3 index= 309           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediately.  Requires a positive value.
20250508 102239.560 INFO             PET3 index= 310                                  MPIR_CVAR_OFI_USE_PROVIDER : This variable is no longer supported. Use FI_PROVIDER instead to select libfabric providers.
20250508 102239.560 INFO             PET3 index= 311                               MPIR_CVAR_SINGLE_HOST_ENABLED : Set this variable to true to indicate that processes are launched on a single host. The current implication is to avoid the cxi provider to prevent the use of scarce hardware resources.
20250508 102239.560 INFO             PET3 index= 312                    MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE : For long message to be sent using pipeline rather than default RDMA read.
20250508 102239.561 INFO             PET3 index= 313                         MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir                        - Fallback to MPIR collectives
trigger_tree_tagged         - Force triggered ops based Tagged Tree
trigger_tree_rma            - Force triggered ops based RMA Tree
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_OFI_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET3 index= 314                                     MPIR_CVAR_OFI_SKIP_IPV6 : Skip IPv6 providers.
20250508 102239.561 INFO             PET3 index= 315                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20250508 102239.561 INFO             PET3 index= 316                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20250508 102239.561 INFO             PET3 index= 317                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20250508 102239.561 INFO             PET3 index= 318                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20250508 102239.561 INFO             PET3 index= 319                    MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS : If true, enable virtual addressing for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET3 index= 320                       MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED : If true, require all OFI memory regions must be backed by physical memory pages at the time the registration call is made. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET3 index= 321                   MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL : If true, memory registration call supports registering with NULL addresses.
20250508 102239.561 INFO             PET3 index= 322                        MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY : If true, enable provider supplied key for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET3 index= 323                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20250508 102239.561 INFO             PET3 index= 324                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20250508 102239.561 INFO             PET3 index= 325                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20250508 102239.561 INFO             PET3 index= 326                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20250508 102239.561 INFO             PET3 index= 327                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20250508 102239.561 INFO             PET3 index= 328                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20250508 102239.561 INFO             PET3 index= 329              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20250508 102239.561 INFO             PET3 index= 330                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20250508 102239.561 INFO             PET3 index= 331                               MPIR_CVAR_CH4_OFI_ENABLE_HMEM : If true, uses GPU direct RDMA support in the provider.
20250508 102239.561 INFO             PET3 index= 332                            MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM : If true, need to register the buffer to use GPU direct RDMA.
20250508 102239.561 INFO             PET3 index= 333                        MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD : The threshold to start using GPU direct RDMA.
20250508 102239.561 INFO             PET3 index= 334                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET3 index= 335                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET3 index= 336                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET3 index= 337                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET3 index= 338                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET3 index= 339                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20250508 102239.561 INFO             PET3 index= 340                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20250508 102239.561 INFO             PET3 index= 341                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20250508 102239.561 INFO             PET3 index= 342              MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS : Specifies the number of optimized memory regions supported by the provider. An optimized memory region is used for lower-overhead, unordered RMA operations. It uses a low-overhead RX path and additionally, a low-overhead packet format may be used to target an optimized memory region.
20250508 102239.561 INFO             PET3 index= 343                     MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL : Specifies the interval for manually flushing RMA operations when automatic progress is not enabled. It the underlying OFI provider supports auto data progress, this value is ignored. If the value is -1, this optimization will be turned off.
20250508 102239.561 INFO             PET3 index= 344                             MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX : Specifies the maximum number of iovecs to allocate for RMA operations to/from noncontiguous buffers.
20250508 102239.561 INFO             PET3 index= 345                        MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which OFI native path switches from eager to rendezvous mode. It does not affect the AM path eager limit. Having this gives a way to reliably test native non-path. If the number is positive, OFI will init the MPIDI_OFI_global.max_msg_size to the value of cvar. If the number is negative, OFI will init the MPIDI_OFI_globa.max_msg_size using whatever provider gives (which might be unlimited for socket provider).
20250508 102239.561 INFO             PET3 index= 346                                  MPIR_CVAR_CH4_OFI_MAX_NICS : If set to positive number, this cvar determines the maximum number of physical nics to use (if more than one is available). If the number is -1, underlying netmod or shmmod automatically uses an optimal number depending on what is detected on the system up to the limit determined by MPIDI_MAX_NICS (in ofi_types.h).
20250508 102239.561 INFO             PET3 index= 347                 MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING : If true, this cvar enables striping of large messages across multiple NICs.
20250508 102239.561 INFO             PET3 index= 348              MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD : Striping will happen for message sizes beyond this threshold.
20250508 102239.561 INFO             PET3 index= 349                  MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING : Multi-NIC hashing means to use more than one NIC to send and receive messages above a certain size.  If set to positive number, this feature will be turned on. If set to 0, this feature will be turned off. If the number is -1, MPICH automatically determines whether to use multi-nic hashing depending on what is detected on the system (e.g., number of NICs available, number of processes sharing the NICs).
20250508 102239.561 INFO             PET3 index= 350                     MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE : Controls the multirecv am buffer size. It is recommended to match this to the hugepage size so that the buffer can be allocated at the page boundary.
20250508 102239.561 INFO             PET3 index= 351                                  MPIR_CVAR_OFI_USE_MIN_NICS : If true and all nodes do not have the same number of NICs, MPICH will fall back to using the fewest number of NICs instead of returning an error.
20250508 102239.561 INFO             PET3 index= 352                          MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED : If true, enable OFI triggered ops for MPI collectives.
20250508 102239.561 INFO             PET3 index= 353                      MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the sender side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET3 index= 354                   MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the receiver side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET3 index= 355                       MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE : If true, enable pipeline for GPU data transfer. GPU pipeline does not support non-contiguous datatypes or mixed buffer types (i.e. GPU send buffer, host recv buffer). If GPU pipeline is enabled, the unsupported scenarios will cause undefined behavior if encountered.
20250508 102239.561 INFO             PET3 index= 356                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD : This is the threshold to start using GPU pipeline.
20250508 102239.561 INFO             PET3 index= 357                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU pipeline data transfer.
20250508 102239.561 INFO             PET3 index= 358        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU pipeline data transfer in each block/chunk of the pool.
20250508 102239.561 INFO             PET3 index= 359              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU pipeline data transfer
20250508 102239.561 INFO             PET3 index= 360              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the sender side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET3 index= 361              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the receiver side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET3 index= 362                      MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE : Avoid use fi_inject_write. For some provider, e.g. tcp;ofi_rxm, inject write may break the synchronization.
20250508 102239.561 INFO             PET3 index= 363                                       MPIR_CVAR_UCX_DT_RECV : Variable to select method for receiving noncontiguous data
true                - Use UCX datatype with pack/unpack callbacks
false               - MPICH will decide to pack/unpack at completion or use IOVs
based on the datatype
20250508 102239.561 INFO             PET3 index= 364                          MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE : By default, we will cache ipc handles using the specialized cache mechanism. If the
gpu-specific backend does not implement a specialized cache, then we will fallback to
the generic cache mechanism. Users can optionally force the generic cache mechanism or
disable ipc caching entirely.
generic - use the cache mechanism in the generic layer
specialized - use the cache mechanism in a gpu-specific mpl layer (if applicable)
disabled - disable caching completely
20250508 102239.561 INFO             PET3 index= 365                         MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD (in bytes), then enable GPU-based single copy protocol for intranode communication. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET3 index= 366                    MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE : If a send message size is less than or equal to MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE (in bytes), then enable GPU-basedfast memcpy. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET3 index= 367                       MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE : Variable to select implementation for ZE shareable IPC handle
pidfd - use pidfd_getfd syscall to implement shareable IPC handle
drmfd - force to use device fd-based shareable IPC handle
20250508 102239.561 INFO             PET3 index= 368                           MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE : By default, select engine type automatically
auto - select automatically
compute - use compute engine
copy_high_bandwidth - use high-bandwidth copy engine
copy_low_latency - use low-latency copy engine
20250508 102239.561 INFO             PET3 index= 369                   MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL : By default, use read protocol.
auto - select automatically
read - use read protocol
write - use write protocol if remote device is visible
20250508 102239.561 INFO             PET3 index= 370                           MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR : If an address is used more than once in the last ten send operations, map it for IPC use even if it is below the IPC threshold.
20250508 102239.561 INFO             PET3 index= 371                                  MPIR_CVAR_CH4_XPMEM_ENABLE : To manually disable XPMEM set to 0. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET3 index= 372                       MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD (in bytes), then enable XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET3 index= 373                       MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
ipc_read - Uses read-based collective with ipc
20250508 102239.561 INFO             PET3 index= 374                      MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET3 index= 375                      MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET3 index= 376                     MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET3 index= 377                   MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node allreduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET3 index= 378                     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node barrier
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET3 index= 379                    MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node alltoall
mpir           - Fallback to MPIR collectives (default)
ipc_read    - Uses read-based collective with ipc
20250508 102239.561 INFO             PET3 index= 380                              MPIR_CVAR_POSIX_POLL_FREQUENCY : This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.561 INFO             PET3 index= 381                 MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read bcast only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET3 index= 382              MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read alltoall only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET3 index= 383                         MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD : Use posix optimized collectives (release_gather) only when the total number of Bcast, Reduce, Barrier, and Allreduce calls on the node level communicator is more than this threshold.
20250508 102239.561 INFO             PET3 index= 384                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : If non-empty, this cvar specifies which shm posix eager module to use
20250508 102239.561 INFO             PET3 index= 385         MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.561 INFO             PET3 index= 386                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS : The number of cells used for the depth of the iqueue.
20250508 102239.561 INFO             PET3 index= 387                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE : Size of each cell.
20250508 102239.561 INFO             PET3 index= 388                           MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE : Maximum shared memory created per node for optimized intra-node collectives (in KB)
20250508 102239.561 INFO             PET3 index= 389                 MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the bcast buffer (in bytes)
20250508 102239.561 INFO             PET3 index= 390                         MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS : Number of cells the bcast buffer is divided into
20250508 102239.561 INFO             PET3 index= 391                MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the reduce buffer per rank (in bytes)
20250508 102239.561 INFO             PET3 index= 392                        MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS : Number of cells the reduce buffer is divided into, for each rank
20250508 102239.561 INFO             PET3 index= 393                         MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node bcast
20250508 102239.561 INFO             PET3 index= 394                         MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE : Tree type for intra-node bcast tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET3 index= 395                        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node reduce
20250508 102239.561 INFO             PET3 index= 396                        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE : Tree type for intra-node reduce tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET3 index= 397             MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES : Enable collective specific intra-node trees which leverage the memory hierarchy of a machine. Depends on hwloc to extract the binding information of each rank. Pick a leader rank per package (socket), then create a per_package tree for ranks on a same package, package leaders tree for package leaders. For Bcast - Assemble the per_package and package_leaders tree in such a way that leaders interact among themselves first before interacting with package local ranks. Both the package_leaders and per_package trees are left skewed (children are added from left to right, first child to be added is the first one to be processed in traversal) For Reduce - Assemble the per_package and package_leaders tree in such a way that a leader rank interacts with its package local ranks first, then with the other package leaders. Both the per_package and package_leaders tree is right skewed (children are added in reverse order, first child to be added is the last one to be processed in traversal) The tree radix and tree type of
20250508 102239.561 INFO             PET3 index= 398                               MPIR_CVAR_BARRIER_COMPOSITION : Select composition (inter_node + intra_node) for Barrier 0 Auto selection 1 NM + SHM 2 NM only
20250508 102239.561 INFO             PET3 index= 399                                 MPIR_CVAR_BCAST_COMPOSITION : Select composition (inter_node + intra_node) for Bcast 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.561 INFO             PET3 index= 400                             MPIR_CVAR_ALLREDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Allreduce 0 Auto selection 1 NM + SHM with reduce + bcast 2 NM only composition 3 SHM only composition 4 Multi leaders based inter node + intra node composition
20250508 102239.561 INFO             PET3 index= 401                             MPIR_CVAR_ALLGATHER_COMPOSITION : Select composition (inter_node + intra_node) for Allgather 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.561 INFO             PET3 index= 402                              MPIR_CVAR_ALLTOALL_COMPOSITION : Select composition (inter_node + intra_node) for Alltoall 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.561 INFO             PET3 index= 403                                MPIR_CVAR_REDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Reduce 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.561 INFO             PET3 index= 404                             MPIR_CVAR_ALLTOALL_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Alltoall (in bytes)
20250508 102239.562 INFO             PET3 index= 405                            MPIR_CVAR_ALLGATHER_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Allgather (in bytes)
20250508 102239.562 INFO             PET3 index= 406                                   MPIR_CVAR_NUM_MULTI_LEADS : Number of leader ranks per node to be used for multi-leaders based collective algorithms
20250508 102239.562 INFO             PET3 index= 407                          MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER : Shared memory region per node-leader for multi-leaders based composition for MPI_Allreduce (in bytes) If it is undefined by the user, it is set to the message size of the first call to the algorithm. Max shared memory size is limited to 4MB.
20250508 102239.562 INFO             PET3 index= 408                        MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER : Amount of data reduced in allreduce delta composition's reduce local step (in bytes). Smaller msg size per leader avoids cache misses and improves performance. Experiments indicate 512 to be the best value.
20250508 102239.562 INFO             PET3 index= 409                      MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS : number of offsets in the allreduce delta composition's local copy The value of 2 performed the best in our 2 NIC test cases.
20250508 102239.562 INFO             PET3 index= 410                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20250508 102239.562 INFO             PET3 index= 411                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20250508 102239.562 INFO             PET3 index= 412                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20250508 102239.562 INFO             PET3 index= 413                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20250508 102239.562 INFO             PET3 index= 414                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) lockless
20250508 102239.562 INFO             PET3 index= 415                                      MPIR_CVAR_CH4_NUM_VCIS : Sets the number of VCIs to be implicitly used (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET3 index= 416                                  MPIR_CVAR_CH4_RESERVE_VCIS : Sets the number of VCIs that user can explicitly allocate (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET3 index= 417               MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.562 INFO             PET3 index= 418                               MPIR_CVAR_CH4_IOV_DENSITY_MIN : Defines the threshold of high-density datatype. The density is calculated by (datatype_size / datatype_num_contig_blocks).
20250508 102239.562 INFO             PET3 index= 419                              MPIR_CVAR_CH4_PACK_BUFFER_SIZE : Specifies the number of buffers for packing/unpacking active messages in each block of the pool. The size here should be greater or equal to the max of the eager buffer limit of SHM and NETMOD.
20250508 102239.562 INFO             PET3 index= 420                    MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK : Specifies the number of buffers for packing/unpacking active messages in each block of the pool.
20250508 102239.562 INFO             PET3 index= 421                          MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS : Specifies the max number of buffers for packing/unpacking buffers in the pool. Use 0 for unlimited.
20250508 102239.562 INFO             PET3 index= 422                       MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU collectives data transfer.
20250508 102239.562 INFO             PET3 index= 423                MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU collectives data transfer in each block/chunk of the pool.
20250508 102239.562 INFO             PET3 index= 424                      MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU collectives data transfer.
20250508 102239.562 INFO             PET3 index= 425                               MPIR_CVAR_CH4_GLOBAL_PROGRESS : If on, poll global progress every once a while. With per-vci configuration, turning global progress off may improve the threading performance.
20250508 102239.562 INFO             PET3 index= 426                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.562 INFO             PET3 index= 427                           MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ : Enable stream enqueue operations via stream work queue. Requires progress thread on the corresponding MPIX stream. Reference: MPIX_Stream_progress and MPIX_Start_progress_thread.
20250508 102239.562 INFO             PET3 index= 428                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20250508 102239.562 INFO             PET3 index= 429                MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS : If true, allows RMA synchronization calls to dynamically reduce the frequency of internal progress polling for incoming RMA active messages received on the target process. The RMA synchronization call initially polls progress with a low frequency (defined by MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL) to reduce synchronization overhead. Once any RMA active message has been received, it will always poll progress once at every synchronization call to ensure prompt target-side progress. Effective only for passive target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}.
20250508 102239.562 INFO             PET3 index= 430                      MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL : Specifies a static interval of progress polling for incoming RMA active messages received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Invalid when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET3 index= 431             MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL : Specifies the interval of progress polling with low frequency for incoming RMA active message received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Used when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET3 index= 432            MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE : The genq shmem code allocates pools of cells on each process and, when needed, a cell is removed from the pool and passed to another process. This can happen by either removing a cell from the pool of the sending process or from the pool of the receiving process. This CVAR determines which pool to use. If true, the cell will come from the sender-side. If false, the cell will com from the receiver-side. There are specific advantages of using receiver-side cells when combined with the "avx" fast configure option, which allows MPICH to use AVX streaming copy intrintrinsics, when available, to avoid polluting the cache of the sender with the data being copied to the receiver. Using receiver-side cells does have the trade-off of requiring an MPMC lock for the free queue rather than an MPSC lock, which is used for sender-side cells. Initial performance analysis shows that using the MPMC lock in this case had no significant performance loss. By default, the queue will continue to use sender-side queues until the pe
20250508 102239.562 INFO             PET3 index= 433                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20250508 102239.562 INFO             PET3 index= 434                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20250508 102239.562 INFO             PET3 index= 435                             MPIR_CVAR_SHM_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20250508 102239.562 INFO             PET3 index= 436                                 MPIR_CVAR_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20250508 102239.562 INFO             PET3 index= 437                                MPIR_CVAR_ENABLE_HEAVY_YIELD : If enabled, use nanosleep to ensure other threads have a chance to grab the lock. Note: this may not work with some thread runtimes, e.g. non-preemptive user-level threads.
20250508 102239.562 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20250508 102239.562 INFO             PET3 main: --- VMK::log() start -------------------------------------
20250508 102239.562 INFO             PET3 main: vm located at: 0x125e06570
20250508 102239.562 INFO             PET3 main: mpionly=1 threadsflag=0
20250508 102239.562 INFO             PET3 main: ssiCount=1 localSsi=0
20250508 102239.562 INFO             PET3 main: devCount=0 ssiLocalDevCount=0
20250508 102239.562 INFO             PET3 main: petCount=6 ssiLocalPetCount=6
20250508 102239.562 INFO             PET3 main: localPet=3 mypthid=0x1edb24f40 ssiLocalPet=3 currentSsiPe=-1
20250508 102239.562 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.562 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.562 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20250508 102239.562 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.562 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20250508 102239.562 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.562 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20250508 102239.562 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.562 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20250508 102239.562 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.562 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20250508 102239.562 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.562 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20250508 102239.562 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20250508 102239.563 INFO             PET3 Executing 'userm1_setvm'
20250508 102239.563 INFO             PET3 Executing 'userm1_register'
20250508 102239.563 INFO             PET3 Executing 'userm2_setvm'
20250508 102239.563 DEBUG            PET3 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 DEBUG            PET3 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20250508 102239.563 INFO             PET3 model1: vm located at: 0x125e09e40
20250508 102239.563 INFO             PET3 model1: mpionly=1 threadsflag=0
20250508 102239.563 INFO             PET3 model1: ssiCount=1 localSsi=0
20250508 102239.563 INFO             PET3 model1: devCount=0 ssiLocalDevCount=0
20250508 102239.563 INFO             PET3 model1: petCount=6 ssiLocalPetCount=6
20250508 102239.563 INFO             PET3 model1: localPet=3 mypthid=0x1edb24f40 ssiLocalPet=3 currentSsiPe=-1
20250508 102239.563 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.563 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.563 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20250508 102239.563 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.563 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20250508 102239.563 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.563 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20250508 102239.563 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.563 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20250508 102239.563 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.563 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20250508 102239.563 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.563 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20250508 102239.563 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20250508 102239.568 INFO             PET3 Entering 'user1_run'
20250508 102239.568 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102239.841 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102240.081 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102240.303 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102240.530 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20250508 102240.756 INFO             PET3 Exiting 'user1_run'
20250508 102249.040 INFO             PET3  NUMBER_OF_PROCESSORS           6
20250508 102249.040 INFO             PET3  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20250508 102249.040 INFO             PET3  Finalizing ESMF
20250508 102249.040 INFO             PET3 ESMCI_IO_Handler.C:335 ESMCI::IO_Handler::finalize() 
20250508 102249.040 INFO             PET3 ESMCI_PIO_Handler.C:357 ESMCI::PIO_Handler::finalize() 
20250508 102249.040 INFO             PET3 ESMCI_IO_Handler.C:337 ESMCI::IO_Handler::finalize() after finalize, localrc = 0
20250508 102249.040 INFO             PET3 ESMCI_IO_Handler.C:360 ESMCI::IO_Handler::finalize() before return, localrc = 0
20250508 102239.555 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20250508 102239.556 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20250508 102239.556 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20250508 102239.556 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20250508 102239.556 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET4 Running with ESMF Version   : v8.9.0b08-5-g079ca68fdc
20250508 102239.556 INFO             PET4 ESMF library build date/time: "May  8 2025" "10:06:33"
20250508 102239.556 INFO             PET4 ESMF library build location : /Users/oehmke/ESMF_AutoTest/gfortranclang_12.2.0_14.0.0_mpich_g_develop/esmf
20250508 102239.556 INFO             PET4 ESMF_COMM                   : mpich
20250508 102239.556 INFO             PET4 ESMF_MOAB                   : enabled
20250508 102239.556 INFO             PET4 ESMF_LAPACK                 : enabled
20250508 102239.556 INFO             PET4 ESMF_NETCDF                 : enabled
20250508 102239.556 INFO             PET4 ESMF_PNETCDF                : disabled
20250508 102239.556 INFO             PET4 ESMF_PIO                    : enabled
20250508 102239.556 INFO             PET4 ESMF_YAMLCPP                : enabled
20250508 102239.556 INFO             PET4 --- VMK::logSystem() start -------------------------------
20250508 102239.556 INFO             PET4 esmfComm=mpich
20250508 102239.556 INFO             PET4 isPthreadsEnabled=1
20250508 102239.556 INFO             PET4 isOpenMPEnabled=0
20250508 102239.557 INFO             PET4 isOpenACCEnabled=0
20250508 102239.557 INFO             PET4 isSsiSharedMemoryEnabled=1
20250508 102239.557 INFO             PET4 isNvmlEnabled=0
20250508 102239.557 INFO             PET4 isNumaEnabled=0
20250508 102239.557 INFO             PET4 ssiCount=1 peCount=6
20250508 102239.557 INFO             PET4 PE=0 SSI=0 SSIPE=0
20250508 102239.557 INFO             PET4 PE=1 SSI=0 SSIPE=1
20250508 102239.557 INFO             PET4 PE=2 SSI=0 SSIPE=2
20250508 102239.557 INFO             PET4 PE=3 SSI=0 SSIPE=3
20250508 102239.557 INFO             PET4 PE=4 SSI=0 SSIPE=4
20250508 102239.557 INFO             PET4 PE=5 SSI=0 SSIPE=5
20250508 102239.557 INFO             PET4 ndevs=0 ndevsSSI=0
20250508 102239.557 INFO             PET4 
20250508 102239.557 INFO             PET4 --- VMK::logSystem() MPI Layer ---------------------------
20250508 102239.557 INFO             PET4 MPI_VERSION=4
20250508 102239.557 INFO             PET4 MPI_SUBVERSION=1
20250508 102239.557 INFO             PET4 MPICH_VERSION=4.2.3
20250508 102239.557 INFO             PET4 mpi_t_okay=1
20250508 102239.557 INFO             PET4 --- VMK::logSystem() MPI Tool Interface Control Vars ---
20250508 102239.557 INFO             PET4 index=   0                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb            - Force nonblocking algorithm
smp           - Force smp algorithm
k_dissemination - Force high radix dissemination algorithm
recexch       - Force recursive exchange algorithm
20250508 102239.557 INFO             PET4 index=   1                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
bcast - Force bcast algorithm
nb    - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=   2                               MPIR_CVAR_BARRIER_DISSEM_KVAL : k value for dissemination exchange based barrier algorithm
20250508 102239.557 INFO             PET4 index=   3                              MPIR_CVAR_BARRIER_RECEXCH_KVAL : k value for recursive exchange based allreduce based barrier
20250508 102239.557 INFO             PET4 index=   4                 MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.557 INFO             PET4 index=   5                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20250508 102239.557 INFO             PET4 index=   6                              MPIR_CVAR_IBARRIER_DISSEM_KVAL : k value for dissemination exchange based ibarrier
20250508 102239.557 INFO             PET4 index=   7                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
tsp_recexch - Force generic transport based recursive exchange algorithm
tsp_k_dissemination - Force generic transport based high-radix dissemination algorithm
20250508 102239.557 INFO             PET4 index=   8                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_bcast - Force bcast algorithm
20250508 102239.557 INFO             PET4 index=   9                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET4 index=  10                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET4 index=  11                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET4 index=  12                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes.
20250508 102239.557 INFO             PET4 index=  13                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial                                - Force Binomial Tree
nb                                      - Force nonblocking algorithm
smp                                     - Force smp algorithm
scatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgather
scatter_ring_allgather                  - Force Scatter Ring
pipelined_tree                          - Force tree-based pipelined algorithm
tree                                    - Force tree-based algorithm
20250508 102239.557 INFO             PET4 index=  14                                   MPIR_CVAR_BCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based bcast
20250508 102239.557 INFO             PET4 index=  15                                   MPIR_CVAR_BCAST_TREE_TYPE : Tree type for tree based bcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.557 INFO             PET4 index=  16                         MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.557 INFO             PET4 index=  17                               MPIR_CVAR_BCAST_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.557 INFO             PET4 index=  18                            MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.557 INFO             PET4 index=  19                          MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.557 INFO             PET4 index=  20                          MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.557 INFO             PET4 index=  21                             MPIR_CVAR_BCAST_IS_NON_BLOCKING : If set to true, MPI_Bcast will use non-blocking send.
20250508 102239.557 INFO             PET4 index=  22                    MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE : Indicates the chunk size for pipelined bcast.
20250508 102239.557 INFO             PET4 index=  23                               MPIR_CVAR_BCAST_RECV_PRE_POST : If set to true, MPI_Bcast will pre-post all the receives.
20250508 102239.557 INFO             PET4 index=  24                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                      - Force nonblocking algorithm
remote_send_local_bcast - Force remote-send-local-bcast algorithm
20250508 102239.557 INFO             PET4 index=  25                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20250508 102239.557 INFO             PET4 index=  26                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.557 INFO             PET4 index=  27                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET4 index=  28                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET4 index=  29                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial                             - Force Binomial algorithm
sched_smp                                  - Force smp algorithm
sched_scatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithm
sched_scatter_ring_allgather               - Force Scatter Ring Allgather algorithm
tsp_tree                               - Force Generic Transport Tree algorithm
tsp_scatterv_recexch_allgatherv        - Force Generic Transport Scatterv followed by Recursive Exchange Allgatherv algorithm
tsp_scatterv_ring_allgatherv           - Force Generic Transport Scatterv followed by Ring Allgatherv algorithm
tsp_ring                               - Force Generic Transport Ring algorithm
20250508 102239.557 INFO             PET4 index=  30                              MPIR_CVAR_IBCAST_SCATTERV_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET4 index=  31                    MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET4 index=  32                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_flat - Force flat algorithm
20250508 102239.557 INFO             PET4 index=  33                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20250508 102239.557 INFO             PET4 index=  34                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=  35                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                   - Force linear algorithm
local_gather_remote_send - Force local-gather-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=  36                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET4 index=  37                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20250508 102239.557 INFO             PET4 index=  38                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_long  - Force long inter algorithm
sched_short - Force short inter algorithm
20250508 102239.557 INFO             PET4 index=  39                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=  40                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=  41                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear         - Force linear algorithm
tsp_linear       - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET4 index=  42                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET4 index=  43                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20250508 102239.557 INFO             PET4 index=  44                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=  45                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                    - Force linear algorithm
nb                        - Force nonblocking algorithm
remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET4 index=  46                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET4 index=  47                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20250508 102239.557 INFO             PET4 index=  48                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear                    - Force linear algorithm
sched_remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET4 index=  49                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=  50                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET4 index=  51                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET4 index=  52                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET4 index=  53                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET4 index=  54                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET4 index=  55                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
k_brucks           - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
recexch_doubling   - Force recexch distance doubling algorithm
recexch_halving    - Force recexch distance halving algorithm
20250508 102239.558 INFO             PET4 index=  56                             MPIR_CVAR_ALLGATHER_BRUCKS_KVAL : radix (k) value for generic transport brucks based allgather
20250508 102239.558 INFO             PET4 index=  57                            MPIR_CVAR_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather
20250508 102239.558 INFO             PET4 index=  58               MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET4 index=  59                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
nb                        - Force nonblocking algorithm
20250508 102239.558 INFO             PET4 index=  60                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20250508 102239.558 INFO             PET4 index=  61                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20250508 102239.558 INFO             PET4 index=  62                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_ring               - Force ring algorithm
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
tsp_ring       - Force generic transport ring algorithm
tsp_brucks     - Force generic transport based brucks algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
20250508 102239.558 INFO             PET4 index=  63                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20250508 102239.558 INFO             PET4 index=  64                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20250508 102239.558 INFO             PET4 index=  65                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
20250508 102239.558 INFO             PET4 index=  66                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET4 index=  67                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20250508 102239.558 INFO             PET4 index=  68                           MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL : k value for radix in brucks based iallgatherv
20250508 102239.558 INFO             PET4 index=  69                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_ring               - Force ring algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
tsp_ring             - Force generic transport ring algorithm
tsp_brucks           - Force generic transport based brucks algorithm
20250508 102239.558 INFO             PET4 index=  70                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET4 index=  71                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20250508 102239.558 INFO             PET4 index=  72                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20250508 102239.558 INFO             PET4 index=  73                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20250508 102239.558 INFO             PET4 index=  74                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks                    - Force brucks algorithm
k_brucks                  - Force Force radix k brucks algorithm
nb                        - Force nonblocking algorithm
pairwise                  - Force pairwise algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET4 index=  75                              MPIR_CVAR_ALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based alltoall
20250508 102239.558 INFO             PET4 index=  76                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET4 index=  77                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks            - Force brucks algorithm
sched_inplace           - Force inplace algorithm
sched_pairwise          - Force pairwise algorithm
sched_permuted_sendrecv - Force permuted sendrecv algorithm
tsp_ring            - Force generic transport based ring algorithm
tsp_brucks          - Force generic transport based brucks algorithm
tsp_scattered       - Force generic transport based scattered algorithm
20250508 102239.558 INFO             PET4 index=  78                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET4 index=  79                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET4 index=  80                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
pairwise_exchange - Force pairwise exchange algorithm
nb                - Force nonblocking algorithm
20250508 102239.558 INFO             PET4 index=  81                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_scattered       - Force generic transport based scattered algorithm
tsp_blocked         - Force generic transport blocked algorithm
tsp_inplace         - Force generic transport inplace algorithm
20250508 102239.558 INFO             PET4 index=  82                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET4 index=  83            MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.558 INFO             PET4 index=  84                   MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.558 INFO             PET4 index=  85                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET4 index=  86                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET4 index=  87                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_blocked   - Force generic transport based blocked algorithm
tsp_inplace   - Force generic transport based inplace algorithm
20250508 102239.558 INFO             PET4 index=  88                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET4 index=  89                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET4 index=  90                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20250508 102239.558 INFO             PET4 index=  91                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial              - Force binomial algorithm
nb                    - Force nonblocking algorithm
smp                   - Force smp algorithm
reduce_scatter_gather - Force reduce scatter gather algorithm
20250508 102239.558 INFO             PET4 index=  92                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_reduce_remote_send - Force local-reduce-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.558 INFO             PET4 index=  93                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20250508 102239.558 INFO             PET4 index=  94                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET4 index=  95                       MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET4 index=  96                             MPIR_CVAR_IREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET4 index=  97                          MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET4 index=  98                        MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET4 index=  99                        MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET4 index= 100                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET4 index= 101                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET4 index= 102                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET4 index= 103                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                   - Force smp algorithm
sched_binomial              - Force binomial algorithm
sched_reduce_scatter_gather - Force reduce scatter gather algorithm
tsp_tree                - Force Generic Transport Tree
tsp_ring                - Force Generic Transport Ring
20250508 102239.558 INFO             PET4 index= 104                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_reduce_remote_send - Force local-reduce-remote-send algorithm
20250508 102239.558 INFO             PET4 index= 105                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET4 index= 106                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20250508 102239.558 INFO             PET4 index= 107                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                       - Force nonblocking algorithm
smp                      - Force smp algorithm
recursive_doubling       - Force recursive doubling algorithm
reduce_scatter_allgather - Force reduce scatter allgather algorithm
tree                     - Force pipelined tree algorithm
recexch                  - Force generic transport recursive exchange algorithm
ring                     - Force ring algorithm
k_reduce_scatter_allgather - Force reduce scatter allgather algorithm
20250508 102239.558 INFO             PET4 index= 108                               MPIR_CVAR_ALLREDUCE_TREE_TYPE : Tree type for tree based allreduce knomial_1 is default as it supports both commutative and non-commutative reduce operations kary      - kary tree type knomial_1 - knomial_1 tree type (tree grows starting from the left of the root) knomial_2 - knomial_2 tree type (tree grows starting from the right of the root) topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET4 index= 109                               MPIR_CVAR_ALLREDUCE_TREE_KVAL : Indicates the branching factor for kary or knomial trees.
20250508 102239.558 INFO             PET4 index= 110                     MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET4 index= 111                           MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET4 index= 112                        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET4 index= 113                      MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET4 index= 114                      MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET4 index= 115                MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based allreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET4 index= 116                   MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET4 index= 117                            MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based allreduce
20250508 102239.558 INFO             PET4 index= 118               MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET4 index= 119                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                    - Force nonblocking algorithm
reduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20250508 102239.558 INFO             PET4 index= 120                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20250508 102239.558 INFO             PET4 index= 121                              MPIR_CVAR_IALLREDUCE_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.558 INFO             PET4 index= 122               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET4 index= 123                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET4 index= 124                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20250508 102239.558 INFO             PET4 index= 125                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_naive                      - Force naive algorithm
sched_smp                        - Force smp algorithm
sched_recursive_doubling         - Force recursive doubling algorithm
sched_reduce_scatter_allgather   - Force reduce scatter allgather algorithm
tsp_recexch_single_buffer    - Force generic transport recursive exchange with single buffer for receives
tsp_recexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
tsp_tree                     - Force generic transport tree algorithm
tsp_ring                     - Force generic transport ring algorithm
tsp_recexch_reduce_scatter_recexch_allgatherv  - Force generic transport recursive exchange with reduce scatter and allgatherv
20250508 102239.558 INFO             PET4 index= 126                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20250508 102239.558 INFO             PET4 index= 127          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20250508 102239.558 INFO             PET4 index= 128                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
noncommutative     - Force noncommutative algorithm
pairwise           - Force pairwise algorithm
recursive_doubling - Force recursive doubling algorithm
recursive_halving  - Force recursive halving algorithm
20250508 102239.558 INFO             PET4 index= 129                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET4 index= 130                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20250508 102239.558 INFO             PET4 index= 131                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET4 index= 132                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET4 index= 133              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
noncommutative     - Force noncommutative algorithm
recursive_doubling - Force recursive doubling algorithm
pairwise           - Force pairwise algorithm
recursive_halving  - Force recursive halving algorithm
nb                 - Force nonblocking algorithm
20250508 102239.558 INFO             PET4 index= 134              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET4 index= 135                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20250508 102239.558 INFO             PET4 index= 136             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET4 index= 137             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.559 INFO             PET4 index= 138                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
smp                - Force smp algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.559 INFO             PET4 index= 139                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                  - Force smp algorithm
sched_recursive_doubling   - Force recursive doubling algorithm
tsp_recursive_doubling - Force generic transport recursive doubling algorithm
20250508 102239.559 INFO             PET4 index= 140                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.559 INFO             PET4 index= 141                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
20250508 102239.559 INFO             PET4 index= 142                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET4 index= 143                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET4 index= 144               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 145               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 146               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 147               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 148              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 149              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 150                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 151                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 152                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 153                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 154                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 155                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 156               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 157               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 158                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 159                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET4 index= 160               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 161               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET4 index= 162                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 163                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 164                    MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 165                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 166                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 167                      MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 168                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 169                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 170                     MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 171                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 172                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 173                    MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 174                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 175                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 176                    MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 177                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 178                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 179                   MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 180                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 181                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 182                  MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 183                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 184                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 185                 MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 186                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 187                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 188                   MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 189                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 190                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 191                  MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 192                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 193                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 194                  MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 195                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 196                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 197                     MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 198                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 199                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 200                  MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 201                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 202                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 203             MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 204            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 205           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 206       MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 207                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 208                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 209                       MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 210                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 211                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 212                     MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 213              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 214             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 215         MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 216             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 217            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 218        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET4 index= 219               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 220              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 221          MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 222              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 223             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 224         MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 225              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 226             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 227         MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET4 index= 228                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20250508 102239.560 INFO             PET4 index= 229                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20250508 102239.560 INFO             PET4 index= 230                             MPIR_CVAR_IALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based ialltoall
20250508 102239.560 INFO             PET4 index= 231                   MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR : If set to true, the tsp based brucks algorithm will allocate dedicated send and receive buffers for every neighbor in the brucks algorithm. Otherwise, it would reuse a single buffer for sending and receiving data to/from neighbors
20250508 102239.560 INFO             PET4 index= 232             MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.560 INFO             PET4 index= 233                    MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.560 INFO             PET4 index= 234                                MPIR_CVAR_DEVICE_COLLECTIVES : Variable to select whether the device can override the
MPIR-level collective algorithms.
all     - Always prefer the device collectives
none    - Never pick the device collectives
percoll - Use the per-collective CVARs to decide
20250508 102239.560 INFO             PET4 index= 235                               MPIR_CVAR_COLLECTIVE_FALLBACK : Variable to control what the MPI library should do if the
user-specified collective algorithm does not work for the
arguments passed in by the user.
error   - throw an error
print   - print an error message and fallback to the internally selected algorithm
silent  - silently fallback to the internally selected algorithm
20250508 102239.560 INFO             PET4 index= 236                   MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.560 INFO             PET4 index= 237                                    MPIR_CVAR_HIERARCHY_DUMP : If set to true, each rank will dump the hierarchy data structure to a file named "hierarchy[rank]" in the current folder. If set to false, the hierarchy data structure will not be dumped.
20250508 102239.560 INFO             PET4 index= 238                                  MPIR_CVAR_COORDINATES_FILE : Defines the location of the input coordinates file.
20250508 102239.560 INFO             PET4 index= 239                                    MPIR_CVAR_COLL_TREE_DUMP : If set to true, each rank will dump the tree to a file named "colltree[rank].json" in the current folder. If set to false, the tree will not be dumped.
20250508 102239.560 INFO             PET4 index= 240                                  MPIR_CVAR_COORDINATES_DUMP : If set to true, rank 0 will dump the network coordinates to a file named "coords" in the current folder. If set to false, the network coordinates will not be dumped.
20250508 102239.560 INFO             PET4 index= 241                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20250508 102239.560 INFO             PET4 index= 242                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20250508 102239.560 INFO             PET4 index= 243                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20250508 102239.560 INFO             PET4 index= 244                                MPIR_CVAR_DATALOOP_FAST_SEEK : use a datatype-specialized algorithm to shortcut seeking to the correct location in a noncontiguous buffer
20250508 102239.560 INFO             PET4 index= 245                             MPIR_CVAR_YAKSA_COMPLEX_SUPPORT : This CVAR indicates that complex type reduction is not supported in yaksa.
20250508 102239.560 INFO             PET4 index= 246                                MPIR_CVAR_GPU_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET4 index= 247                           MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET4 index= 248                            MPIR_CVAR_ENABLE_YAKSA_REDUCTION : This cvar enables yaksa based reduction for local reduce.
20250508 102239.560 INFO             PET4 index= 249                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20250508 102239.560 INFO             PET4 index= 250                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20250508 102239.560 INFO             PET4 index= 251                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20250508 102239.560 INFO             PET4 index= 252                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20250508 102239.560 INFO             PET4 index= 253                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20250508 102239.560 INFO             PET4 index= 254                          MPIR_CVAR_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. Can be set to auto or comma-separated list of logical processors. When set to auto - MPICH will automatically select logical CPU cores to decide affinity of the progress threads. When set to comma-separated list of logical processors - In case of N progress threads per process, the first N logical processors from list will be assigned to threads of first local process, the next N logical processors from list - to second local process and so on. For example, thread affinity is "0,1,2,3", 2 progress threads per process and 2 processes per node. Progress threads of first local process will be pinned on logical processors "0,1", progress threads of second local process - on "2,3". Cannot work together with MPIR_CVAR_NUM_CLIQUES or MPIR_CVAR_ODD_EVEN_CLIQUES.
20250508 102239.560 INFO             PET4 index= 255                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20250508 102239.560 INFO             PET4 index= 256                                 MPIR_CVAR_COREDUMP_ON_ABORT : Call libc abort() to generate a corefile
20250508 102239.560 INFO             PET4 index= 257                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20250508 102239.560 INFO             PET4 index= 258                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20250508 102239.560 INFO             PET4 index= 259                                     MPIR_CVAR_DEBUG_SUMMARY : If true, print internal summary of various debug information, such as memory allocation by category. Each layer may print their own summary information. For example, ch4-ofi may print its provider capability settings.
20250508 102239.560 INFO             PET4 index= 260                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20250508 102239.560 INFO             PET4 index= 261                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20250508 102239.560 INFO             PET4 index= 262                    MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST : If true, mpl/ze will use immediate command list for copying
20250508 102239.560 INFO             PET4 index= 263                    MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES : If true, mpl/ze will use command queues in a round-robin fashion. If false, only command queues of index 0 will be used.
20250508 102239.560 INFO             PET4 index= 264                            MPIR_CVAR_NO_COLLECTIVE_FINALIZE : If true, prevent MPI_Finalize to invoke collective behavior such as barrier or communicating to other processes. Consequently, it may result in leaking memory or losing messages due to pre-mature exiting. The default is false, which may invoke collective behaviors at finalize.
20250508 102239.560 INFO             PET4 index= 265                                     MPIR_CVAR_FINALIZE_WAIT : If true, poll progress at MPI_Finalize until reference count on MPI_COMM_WORLD and MPI_COMM_SELF reaches zero. This may be necessary to prevent remote processes hanging if it has pending communication protocols, e.g. a rendezvous send.
20250508 102239.560 INFO             PET4 index= 266                                 MPIR_CVAR_REQUEST_ERR_FATAL : By default, MPI_Waitall, MPI_Testall, MPI_Waitsome, and MPI_Testsome return MPI_ERR_IN_STATUS when one of the request fails. If MPIR_CVAR_REQUEST_ERR_FATAL is set to true, these routines will return the error code of the request immediately. The default MPI_ERRS_ARE_FATAL error handler will dump a error stack in this case, which maybe more convenient for debugging. This cvar will also make nonblocking shched return error right away as it issues operations.
20250508 102239.560 INFO             PET4 index= 267                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during MPI_{Waitany,Waitsome} in terms of number of processed requests before polling.
20250508 102239.560 INFO             PET4 index= 268                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20250508 102239.560 INFO             PET4 index= 269                            MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT : Sets the timeout in seconds to dump outstanding requests when progress wait is not making progress for some time.
20250508 102239.560 INFO             PET4 index= 270                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20250508 102239.560 INFO             PET4 index= 271                                    MPIR_CVAR_QMPI_TOOL_LIST : Set the number and order of QMPI tools to be loaded by the MPI library when it is initialized.
20250508 102239.560 INFO             PET4 index= 272                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20250508 102239.560 INFO             PET4 index= 273                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20250508 102239.560 INFO             PET4 index= 274                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20250508 102239.560 INFO             PET4 index= 275                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20250508 102239.560 INFO             PET4 index= 276                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20250508 102239.560 INFO             PET4 index= 277                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20250508 102239.560 INFO             PET4 index= 278                                  MPIR_CVAR_CLIQUES_BY_BLOCK : Specify to divide processes into cliques by uniform blocks. The default is to divide in round-robin fashion. Used for debugging on a single machine.
20250508 102239.560 INFO             PET4 index= 279                                       MPIR_CVAR_PMI_VERSION : Variable to select runtime PMI version.
1        - PMI (default)
2        - PMI2
x        - PMIx
20250508 102239.560 INFO             PET4 index= 280                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20250508 102239.560 INFO             PET4 index= 281                                        MPIR_CVAR_ENABLE_GPU : Control MPICH GPU support. If set to 0, all GPU support is disabled and we do not query the buffer type internally because we assume no GPU buffer is use.
20250508 102239.560 INFO             PET4 index= 282                               MPIR_CVAR_GPU_HAS_WAIT_KERNEL : If set to 1, avoid allocate allocating GPU registered host buffers for temporary buffers. When stream workq and GPU wait kernels are in use, access APIs for GPU registered memory may cause deadlock.
20250508 102239.560 INFO             PET4 index= 283                               MPIR_CVAR_ENABLE_GPU_REGISTER : Control whether to actually register buffers with the GPU runtime in MPIR_gpu_register_host. This could lower the latency of certain GPU communication at the cost of some amount of GPU memory consumed by the MPI library. By default, registration is enabled.
20250508 102239.560 INFO             PET4 index= 284                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.560 INFO             PET4 index= 285                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20250508 102239.560 INFO             PET4 index= 286                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20250508 102239.560 INFO             PET4 index= 287                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20250508 102239.560 INFO             PET4 index= 288                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20250508 102239.560 INFO             PET4 index= 289                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20250508 102239.560 INFO             PET4 index= 290                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET4 index= 291                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET4 index= 292                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20250508 102239.560 INFO             PET4 index= 293                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20250508 102239.560 INFO             PET4 index= 294                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20250508 102239.560 INFO             PET4 index= 295                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.560 INFO             PET4 index= 296               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20250508 102239.560 INFO             PET4 index= 297                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20250508 102239.560 INFO             PET4 index= 298               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchronization approach.  Change this value if programs fail because they run out of requests or other internal resources
20250508 102239.560 INFO             PET4 index= 299                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be negative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20250508 102239.560 INFO             PET4 index= 300            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation available in the ending synchronization call.
20250508 102239.560 INFO             PET4 index= 301                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20250508 102239.560 INFO             PET4 index= 302                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive value.
20250508 102239.560 INFO             PET4 index= 303                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20250508 102239.561 INFO             PET4 index= 304                                    MPIR_CVAR_CH3_PG_VERBOSE : If set, print the PG state on finalize.
20250508 102239.561 INFO             PET4 index= 305                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.561 INFO             PET4 index= 306                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.561 INFO             PET4 index= 307                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.561 INFO             PET4 index= 308                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.561 INFO             PET4 index= 309           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediately.  Requires a positive value.
20250508 102239.561 INFO             PET4 index= 310                                  MPIR_CVAR_OFI_USE_PROVIDER : This variable is no longer supported. Use FI_PROVIDER instead to select libfabric providers.
20250508 102239.561 INFO             PET4 index= 311                               MPIR_CVAR_SINGLE_HOST_ENABLED : Set this variable to true to indicate that processes are launched on a single host. The current implication is to avoid the cxi provider to prevent the use of scarce hardware resources.
20250508 102239.561 INFO             PET4 index= 312                    MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE : For long message to be sent using pipeline rather than default RDMA read.
20250508 102239.561 INFO             PET4 index= 313                         MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir                        - Fallback to MPIR collectives
trigger_tree_tagged         - Force triggered ops based Tagged Tree
trigger_tree_rma            - Force triggered ops based RMA Tree
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_OFI_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET4 index= 314                                     MPIR_CVAR_OFI_SKIP_IPV6 : Skip IPv6 providers.
20250508 102239.561 INFO             PET4 index= 315                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20250508 102239.561 INFO             PET4 index= 316                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20250508 102239.561 INFO             PET4 index= 317                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20250508 102239.561 INFO             PET4 index= 318                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20250508 102239.561 INFO             PET4 index= 319                    MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS : If true, enable virtual addressing for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET4 index= 320                       MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED : If true, require all OFI memory regions must be backed by physical memory pages at the time the registration call is made. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET4 index= 321                   MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL : If true, memory registration call supports registering with NULL addresses.
20250508 102239.561 INFO             PET4 index= 322                        MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY : If true, enable provider supplied key for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET4 index= 323                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20250508 102239.561 INFO             PET4 index= 324                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20250508 102239.561 INFO             PET4 index= 325                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20250508 102239.561 INFO             PET4 index= 326                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20250508 102239.561 INFO             PET4 index= 327                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20250508 102239.561 INFO             PET4 index= 328                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20250508 102239.561 INFO             PET4 index= 329              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20250508 102239.561 INFO             PET4 index= 330                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20250508 102239.561 INFO             PET4 index= 331                               MPIR_CVAR_CH4_OFI_ENABLE_HMEM : If true, uses GPU direct RDMA support in the provider.
20250508 102239.561 INFO             PET4 index= 332                            MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM : If true, need to register the buffer to use GPU direct RDMA.
20250508 102239.561 INFO             PET4 index= 333                        MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD : The threshold to start using GPU direct RDMA.
20250508 102239.561 INFO             PET4 index= 334                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET4 index= 335                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET4 index= 336                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET4 index= 337                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET4 index= 338                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET4 index= 339                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20250508 102239.561 INFO             PET4 index= 340                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20250508 102239.561 INFO             PET4 index= 341                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20250508 102239.561 INFO             PET4 index= 342              MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS : Specifies the number of optimized memory regions supported by the provider. An optimized memory region is used for lower-overhead, unordered RMA operations. It uses a low-overhead RX path and additionally, a low-overhead packet format may be used to target an optimized memory region.
20250508 102239.561 INFO             PET4 index= 343                     MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL : Specifies the interval for manually flushing RMA operations when automatic progress is not enabled. It the underlying OFI provider supports auto data progress, this value is ignored. If the value is -1, this optimization will be turned off.
20250508 102239.561 INFO             PET4 index= 344                             MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX : Specifies the maximum number of iovecs to allocate for RMA operations to/from noncontiguous buffers.
20250508 102239.561 INFO             PET4 index= 345                        MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which OFI native path switches from eager to rendezvous mode. It does not affect the AM path eager limit. Having this gives a way to reliably test native non-path. If the number is positive, OFI will init the MPIDI_OFI_global.max_msg_size to the value of cvar. If the number is negative, OFI will init the MPIDI_OFI_globa.max_msg_size using whatever provider gives (which might be unlimited for socket provider).
20250508 102239.561 INFO             PET4 index= 346                                  MPIR_CVAR_CH4_OFI_MAX_NICS : If set to positive number, this cvar determines the maximum number of physical nics to use (if more than one is available). If the number is -1, underlying netmod or shmmod automatically uses an optimal number depending on what is detected on the system up to the limit determined by MPIDI_MAX_NICS (in ofi_types.h).
20250508 102239.561 INFO             PET4 index= 347                 MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING : If true, this cvar enables striping of large messages across multiple NICs.
20250508 102239.561 INFO             PET4 index= 348              MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD : Striping will happen for message sizes beyond this threshold.
20250508 102239.561 INFO             PET4 index= 349                  MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING : Multi-NIC hashing means to use more than one NIC to send and receive messages above a certain size.  If set to positive number, this feature will be turned on. If set to 0, this feature will be turned off. If the number is -1, MPICH automatically determines whether to use multi-nic hashing depending on what is detected on the system (e.g., number of NICs available, number of processes sharing the NICs).
20250508 102239.561 INFO             PET4 index= 350                     MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE : Controls the multirecv am buffer size. It is recommended to match this to the hugepage size so that the buffer can be allocated at the page boundary.
20250508 102239.561 INFO             PET4 index= 351                                  MPIR_CVAR_OFI_USE_MIN_NICS : If true and all nodes do not have the same number of NICs, MPICH will fall back to using the fewest number of NICs instead of returning an error.
20250508 102239.561 INFO             PET4 index= 352                          MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED : If true, enable OFI triggered ops for MPI collectives.
20250508 102239.561 INFO             PET4 index= 353                      MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the sender side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET4 index= 354                   MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the receiver side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET4 index= 355                       MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE : If true, enable pipeline for GPU data transfer. GPU pipeline does not support non-contiguous datatypes or mixed buffer types (i.e. GPU send buffer, host recv buffer). If GPU pipeline is enabled, the unsupported scenarios will cause undefined behavior if encountered.
20250508 102239.561 INFO             PET4 index= 356                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD : This is the threshold to start using GPU pipeline.
20250508 102239.561 INFO             PET4 index= 357                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU pipeline data transfer.
20250508 102239.561 INFO             PET4 index= 358        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU pipeline data transfer in each block/chunk of the pool.
20250508 102239.561 INFO             PET4 index= 359              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU pipeline data transfer
20250508 102239.561 INFO             PET4 index= 360              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the sender side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET4 index= 361              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the receiver side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET4 index= 362                      MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE : Avoid use fi_inject_write. For some provider, e.g. tcp;ofi_rxm, inject write may break the synchronization.
20250508 102239.561 INFO             PET4 index= 363                                       MPIR_CVAR_UCX_DT_RECV : Variable to select method for receiving noncontiguous data
true                - Use UCX datatype with pack/unpack callbacks
false               - MPICH will decide to pack/unpack at completion or use IOVs
based on the datatype
20250508 102239.561 INFO             PET4 index= 364                          MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE : By default, we will cache ipc handles using the specialized cache mechanism. If the
gpu-specific backend does not implement a specialized cache, then we will fallback to
the generic cache mechanism. Users can optionally force the generic cache mechanism or
disable ipc caching entirely.
generic - use the cache mechanism in the generic layer
specialized - use the cache mechanism in a gpu-specific mpl layer (if applicable)
disabled - disable caching completely
20250508 102239.561 INFO             PET4 index= 365                         MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD (in bytes), then enable GPU-based single copy protocol for intranode communication. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET4 index= 366                    MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE : If a send message size is less than or equal to MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE (in bytes), then enable GPU-basedfast memcpy. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET4 index= 367                       MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE : Variable to select implementation for ZE shareable IPC handle
pidfd - use pidfd_getfd syscall to implement shareable IPC handle
drmfd - force to use device fd-based shareable IPC handle
20250508 102239.561 INFO             PET4 index= 368                           MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE : By default, select engine type automatically
auto - select automatically
compute - use compute engine
copy_high_bandwidth - use high-bandwidth copy engine
copy_low_latency - use low-latency copy engine
20250508 102239.561 INFO             PET4 index= 369                   MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL : By default, use read protocol.
auto - select automatically
read - use read protocol
write - use write protocol if remote device is visible
20250508 102239.561 INFO             PET4 index= 370                           MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR : If an address is used more than once in the last ten send operations, map it for IPC use even if it is below the IPC threshold.
20250508 102239.561 INFO             PET4 index= 371                                  MPIR_CVAR_CH4_XPMEM_ENABLE : To manually disable XPMEM set to 0. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET4 index= 372                       MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD (in bytes), then enable XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET4 index= 373                       MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
ipc_read - Uses read-based collective with ipc
20250508 102239.561 INFO             PET4 index= 374                      MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET4 index= 375                      MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET4 index= 376                     MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET4 index= 377                   MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node allreduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET4 index= 378                     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node barrier
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET4 index= 379                    MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node alltoall
mpir           - Fallback to MPIR collectives (default)
ipc_read    - Uses read-based collective with ipc
20250508 102239.561 INFO             PET4 index= 380                              MPIR_CVAR_POSIX_POLL_FREQUENCY : This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.561 INFO             PET4 index= 381                 MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read bcast only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET4 index= 382              MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read alltoall only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET4 index= 383                         MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD : Use posix optimized collectives (release_gather) only when the total number of Bcast, Reduce, Barrier, and Allreduce calls on the node level communicator is more than this threshold.
20250508 102239.561 INFO             PET4 index= 384                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : If non-empty, this cvar specifies which shm posix eager module to use
20250508 102239.561 INFO             PET4 index= 385         MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.561 INFO             PET4 index= 386                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS : The number of cells used for the depth of the iqueue.
20250508 102239.561 INFO             PET4 index= 387                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE : Size of each cell.
20250508 102239.561 INFO             PET4 index= 388                           MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE : Maximum shared memory created per node for optimized intra-node collectives (in KB)
20250508 102239.561 INFO             PET4 index= 389                 MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the bcast buffer (in bytes)
20250508 102239.561 INFO             PET4 index= 390                         MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS : Number of cells the bcast buffer is divided into
20250508 102239.561 INFO             PET4 index= 391                MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the reduce buffer per rank (in bytes)
20250508 102239.561 INFO             PET4 index= 392                        MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS : Number of cells the reduce buffer is divided into, for each rank
20250508 102239.561 INFO             PET4 index= 393                         MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node bcast
20250508 102239.561 INFO             PET4 index= 394                         MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE : Tree type for intra-node bcast tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET4 index= 395                        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node reduce
20250508 102239.561 INFO             PET4 index= 396                        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE : Tree type for intra-node reduce tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.562 INFO             PET4 index= 397             MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES : Enable collective specific intra-node trees which leverage the memory hierarchy of a machine. Depends on hwloc to extract the binding information of each rank. Pick a leader rank per package (socket), then create a per_package tree for ranks on a same package, package leaders tree for package leaders. For Bcast - Assemble the per_package and package_leaders tree in such a way that leaders interact among themselves first before interacting with package local ranks. Both the package_leaders and per_package trees are left skewed (children are added from left to right, first child to be added is the first one to be processed in traversal) For Reduce - Assemble the per_package and package_leaders tree in such a way that a leader rank interacts with its package local ranks first, then with the other package leaders. Both the per_package and package_leaders tree is right skewed (children are added in reverse order, first child to be added is the last one to be processed in traversal) The tree radix and tree type of
20250508 102239.562 INFO             PET4 index= 398                               MPIR_CVAR_BARRIER_COMPOSITION : Select composition (inter_node + intra_node) for Barrier 0 Auto selection 1 NM + SHM 2 NM only
20250508 102239.562 INFO             PET4 index= 399                                 MPIR_CVAR_BCAST_COMPOSITION : Select composition (inter_node + intra_node) for Bcast 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.562 INFO             PET4 index= 400                             MPIR_CVAR_ALLREDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Allreduce 0 Auto selection 1 NM + SHM with reduce + bcast 2 NM only composition 3 SHM only composition 4 Multi leaders based inter node + intra node composition
20250508 102239.562 INFO             PET4 index= 401                             MPIR_CVAR_ALLGATHER_COMPOSITION : Select composition (inter_node + intra_node) for Allgather 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.562 INFO             PET4 index= 402                              MPIR_CVAR_ALLTOALL_COMPOSITION : Select composition (inter_node + intra_node) for Alltoall 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.562 INFO             PET4 index= 403                                MPIR_CVAR_REDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Reduce 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.562 INFO             PET4 index= 404                             MPIR_CVAR_ALLTOALL_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Alltoall (in bytes)
20250508 102239.562 INFO             PET4 index= 405                            MPIR_CVAR_ALLGATHER_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Allgather (in bytes)
20250508 102239.562 INFO             PET4 index= 406                                   MPIR_CVAR_NUM_MULTI_LEADS : Number of leader ranks per node to be used for multi-leaders based collective algorithms
20250508 102239.562 INFO             PET4 index= 407                          MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER : Shared memory region per node-leader for multi-leaders based composition for MPI_Allreduce (in bytes) If it is undefined by the user, it is set to the message size of the first call to the algorithm. Max shared memory size is limited to 4MB.
20250508 102239.562 INFO             PET4 index= 408                        MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER : Amount of data reduced in allreduce delta composition's reduce local step (in bytes). Smaller msg size per leader avoids cache misses and improves performance. Experiments indicate 512 to be the best value.
20250508 102239.562 INFO             PET4 index= 409                      MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS : number of offsets in the allreduce delta composition's local copy The value of 2 performed the best in our 2 NIC test cases.
20250508 102239.562 INFO             PET4 index= 410                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20250508 102239.562 INFO             PET4 index= 411                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20250508 102239.562 INFO             PET4 index= 412                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20250508 102239.562 INFO             PET4 index= 413                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20250508 102239.562 INFO             PET4 index= 414                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) lockless
20250508 102239.562 INFO             PET4 index= 415                                      MPIR_CVAR_CH4_NUM_VCIS : Sets the number of VCIs to be implicitly used (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET4 index= 416                                  MPIR_CVAR_CH4_RESERVE_VCIS : Sets the number of VCIs that user can explicitly allocate (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET4 index= 417               MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.562 INFO             PET4 index= 418                               MPIR_CVAR_CH4_IOV_DENSITY_MIN : Defines the threshold of high-density datatype. The density is calculated by (datatype_size / datatype_num_contig_blocks).
20250508 102239.562 INFO             PET4 index= 419                              MPIR_CVAR_CH4_PACK_BUFFER_SIZE : Specifies the number of buffers for packing/unpacking active messages in each block of the pool. The size here should be greater or equal to the max of the eager buffer limit of SHM and NETMOD.
20250508 102239.562 INFO             PET4 index= 420                    MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK : Specifies the number of buffers for packing/unpacking active messages in each block of the pool.
20250508 102239.562 INFO             PET4 index= 421                          MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS : Specifies the max number of buffers for packing/unpacking buffers in the pool. Use 0 for unlimited.
20250508 102239.562 INFO             PET4 index= 422                       MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU collectives data transfer.
20250508 102239.562 INFO             PET4 index= 423                MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU collectives data transfer in each block/chunk of the pool.
20250508 102239.562 INFO             PET4 index= 424                      MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU collectives data transfer.
20250508 102239.562 INFO             PET4 index= 425                               MPIR_CVAR_CH4_GLOBAL_PROGRESS : If on, poll global progress every once a while. With per-vci configuration, turning global progress off may improve the threading performance.
20250508 102239.562 INFO             PET4 index= 426                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.562 INFO             PET4 index= 427                           MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ : Enable stream enqueue operations via stream work queue. Requires progress thread on the corresponding MPIX stream. Reference: MPIX_Stream_progress and MPIX_Start_progress_thread.
20250508 102239.562 INFO             PET4 index= 428                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20250508 102239.562 INFO             PET4 index= 429                MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS : If true, allows RMA synchronization calls to dynamically reduce the frequency of internal progress polling for incoming RMA active messages received on the target process. The RMA synchronization call initially polls progress with a low frequency (defined by MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL) to reduce synchronization overhead. Once any RMA active message has been received, it will always poll progress once at every synchronization call to ensure prompt target-side progress. Effective only for passive target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}.
20250508 102239.562 INFO             PET4 index= 430                      MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL : Specifies a static interval of progress polling for incoming RMA active messages received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Invalid when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET4 index= 431             MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL : Specifies the interval of progress polling with low frequency for incoming RMA active message received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Used when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET4 index= 432            MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE : The genq shmem code allocates pools of cells on each process and, when needed, a cell is removed from the pool and passed to another process. This can happen by either removing a cell from the pool of the sending process or from the pool of the receiving process. This CVAR determines which pool to use. If true, the cell will come from the sender-side. If false, the cell will com from the receiver-side. There are specific advantages of using receiver-side cells when combined with the "avx" fast configure option, which allows MPICH to use AVX streaming copy intrintrinsics, when available, to avoid polluting the cache of the sender with the data being copied to the receiver. Using receiver-side cells does have the trade-off of requiring an MPMC lock for the free queue rather than an MPSC lock, which is used for sender-side cells. Initial performance analysis shows that using the MPMC lock in this case had no significant performance loss. By default, the queue will continue to use sender-side queues until the pe
20250508 102239.562 INFO             PET4 index= 433                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20250508 102239.562 INFO             PET4 index= 434                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20250508 102239.562 INFO             PET4 index= 435                             MPIR_CVAR_SHM_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20250508 102239.562 INFO             PET4 index= 436                                 MPIR_CVAR_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20250508 102239.562 INFO             PET4 index= 437                                MPIR_CVAR_ENABLE_HEAVY_YIELD : If enabled, use nanosleep to ensure other threads have a chance to grab the lock. Note: this may not work with some thread runtimes, e.g. non-preemptive user-level threads.
20250508 102239.562 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20250508 102239.562 INFO             PET4 main: --- VMK::log() start -------------------------------------
20250508 102239.562 INFO             PET4 main: vm located at: 0x139e06570
20250508 102239.562 INFO             PET4 main: mpionly=1 threadsflag=0
20250508 102239.562 INFO             PET4 main: ssiCount=1 localSsi=0
20250508 102239.562 INFO             PET4 main: devCount=0 ssiLocalDevCount=0
20250508 102239.562 INFO             PET4 main: petCount=6 ssiLocalPetCount=6
20250508 102239.562 INFO             PET4 main: localPet=4 mypthid=0x1edb24f40 ssiLocalPet=4 currentSsiPe=-1
20250508 102239.562 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.562 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.562 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20250508 102239.562 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.562 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20250508 102239.562 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.562 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20250508 102239.562 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.562 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20250508 102239.562 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.562 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20250508 102239.562 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.562 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20250508 102239.562 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20250508 102239.562 INFO             PET4 Executing 'userm1_setvm'
20250508 102239.563 INFO             PET4 Executing 'userm1_register'
20250508 102239.563 INFO             PET4 Executing 'userm2_setvm'
20250508 102239.563 DEBUG            PET4 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 DEBUG            PET4 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20250508 102239.563 INFO             PET4 model1: vm located at: 0x119e04e80
20250508 102239.563 INFO             PET4 model1: mpionly=1 threadsflag=0
20250508 102239.563 INFO             PET4 model1: ssiCount=1 localSsi=0
20250508 102239.563 INFO             PET4 model1: devCount=0 ssiLocalDevCount=0
20250508 102239.563 INFO             PET4 model1: petCount=6 ssiLocalPetCount=6
20250508 102239.563 INFO             PET4 model1: localPet=4 mypthid=0x1edb24f40 ssiLocalPet=4 currentSsiPe=-1
20250508 102239.563 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.563 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.563 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20250508 102239.563 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.563 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20250508 102239.563 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.563 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20250508 102239.563 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.563 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20250508 102239.563 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.563 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20250508 102239.563 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.563 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20250508 102239.563 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20250508 102239.568 INFO             PET4 Entering 'user1_run'
20250508 102239.568 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102239.838 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102240.068 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102240.292 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102240.520 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20250508 102240.745 INFO             PET4 Exiting 'user1_run'
20250508 102249.040 INFO             PET4  NUMBER_OF_PROCESSORS           6
20250508 102249.040 INFO             PET4  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20250508 102249.040 INFO             PET4  Finalizing ESMF
20250508 102249.040 INFO             PET4 ESMCI_IO_Handler.C:335 ESMCI::IO_Handler::finalize() 
20250508 102249.040 INFO             PET4 ESMCI_PIO_Handler.C:357 ESMCI::PIO_Handler::finalize() 
20250508 102249.040 INFO             PET4 ESMCI_IO_Handler.C:337 ESMCI::IO_Handler::finalize() after finalize, localrc = 0
20250508 102249.040 INFO             PET4 ESMCI_IO_Handler.C:360 ESMCI::IO_Handler::finalize() before return, localrc = 0
20250508 102239.555 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20250508 102239.556 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20250508 102239.556 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20250508 102239.556 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20250508 102239.556 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20250508 102239.556 INFO             PET5 Running with ESMF Version   : v8.9.0b08-5-g079ca68fdc
20250508 102239.556 INFO             PET5 ESMF library build date/time: "May  8 2025" "10:06:33"
20250508 102239.556 INFO             PET5 ESMF library build location : /Users/oehmke/ESMF_AutoTest/gfortranclang_12.2.0_14.0.0_mpich_g_develop/esmf
20250508 102239.556 INFO             PET5 ESMF_COMM                   : mpich
20250508 102239.556 INFO             PET5 ESMF_MOAB                   : enabled
20250508 102239.556 INFO             PET5 ESMF_LAPACK                 : enabled
20250508 102239.556 INFO             PET5 ESMF_NETCDF                 : enabled
20250508 102239.556 INFO             PET5 ESMF_PNETCDF                : disabled
20250508 102239.556 INFO             PET5 ESMF_PIO                    : enabled
20250508 102239.556 INFO             PET5 ESMF_YAMLCPP                : enabled
20250508 102239.556 INFO             PET5 --- VMK::logSystem() start -------------------------------
20250508 102239.556 INFO             PET5 esmfComm=mpich
20250508 102239.556 INFO             PET5 isPthreadsEnabled=1
20250508 102239.556 INFO             PET5 isOpenMPEnabled=0
20250508 102239.557 INFO             PET5 isOpenACCEnabled=0
20250508 102239.557 INFO             PET5 isSsiSharedMemoryEnabled=1
20250508 102239.557 INFO             PET5 isNvmlEnabled=0
20250508 102239.557 INFO             PET5 isNumaEnabled=0
20250508 102239.557 INFO             PET5 ssiCount=1 peCount=6
20250508 102239.557 INFO             PET5 PE=0 SSI=0 SSIPE=0
20250508 102239.557 INFO             PET5 PE=1 SSI=0 SSIPE=1
20250508 102239.557 INFO             PET5 PE=2 SSI=0 SSIPE=2
20250508 102239.557 INFO             PET5 PE=3 SSI=0 SSIPE=3
20250508 102239.557 INFO             PET5 PE=4 SSI=0 SSIPE=4
20250508 102239.557 INFO             PET5 PE=5 SSI=0 SSIPE=5
20250508 102239.557 INFO             PET5 ndevs=0 ndevsSSI=0
20250508 102239.557 INFO             PET5 
20250508 102239.557 INFO             PET5 --- VMK::logSystem() MPI Layer ---------------------------
20250508 102239.557 INFO             PET5 MPI_VERSION=4
20250508 102239.557 INFO             PET5 MPI_SUBVERSION=1
20250508 102239.557 INFO             PET5 MPICH_VERSION=4.2.3
20250508 102239.557 INFO             PET5 mpi_t_okay=1
20250508 102239.557 INFO             PET5 --- VMK::logSystem() MPI Tool Interface Control Vars ---
20250508 102239.557 INFO             PET5 index=   0                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb            - Force nonblocking algorithm
smp           - Force smp algorithm
k_dissemination - Force high radix dissemination algorithm
recexch       - Force recursive exchange algorithm
20250508 102239.557 INFO             PET5 index=   1                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
bcast - Force bcast algorithm
nb    - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=   2                               MPIR_CVAR_BARRIER_DISSEM_KVAL : k value for dissemination exchange based barrier algorithm
20250508 102239.557 INFO             PET5 index=   3                              MPIR_CVAR_BARRIER_RECEXCH_KVAL : k value for recursive exchange based allreduce based barrier
20250508 102239.557 INFO             PET5 index=   4                 MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.557 INFO             PET5 index=   5                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20250508 102239.557 INFO             PET5 index=   6                              MPIR_CVAR_IBARRIER_DISSEM_KVAL : k value for dissemination exchange based ibarrier
20250508 102239.557 INFO             PET5 index=   7                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
tsp_recexch - Force generic transport based recursive exchange algorithm
tsp_k_dissemination - Force generic transport based high-radix dissemination algorithm
20250508 102239.557 INFO             PET5 index=   8                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_bcast - Force bcast algorithm
20250508 102239.557 INFO             PET5 index=   9                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET5 index=  10                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20250508 102239.557 INFO             PET5 index=  11                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20250508 102239.557 INFO             PET5 index=  12                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes.
20250508 102239.557 INFO             PET5 index=  13                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial                                - Force Binomial Tree
nb                                      - Force nonblocking algorithm
smp                                     - Force smp algorithm
scatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgather
scatter_ring_allgather                  - Force Scatter Ring
pipelined_tree                          - Force tree-based pipelined algorithm
tree                                    - Force tree-based algorithm
20250508 102239.557 INFO             PET5 index=  14                                   MPIR_CVAR_BCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based bcast
20250508 102239.557 INFO             PET5 index=  15                                   MPIR_CVAR_BCAST_TREE_TYPE : Tree type for tree based bcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.557 INFO             PET5 index=  16                         MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.557 INFO             PET5 index=  17                               MPIR_CVAR_BCAST_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.557 INFO             PET5 index=  18                            MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.557 INFO             PET5 index=  19                          MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.557 INFO             PET5 index=  20                          MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.557 INFO             PET5 index=  21                             MPIR_CVAR_BCAST_IS_NON_BLOCKING : If set to true, MPI_Bcast will use non-blocking send.
20250508 102239.557 INFO             PET5 index=  22                    MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE : Indicates the chunk size for pipelined bcast.
20250508 102239.557 INFO             PET5 index=  23                               MPIR_CVAR_BCAST_RECV_PRE_POST : If set to true, MPI_Bcast will pre-post all the receives.
20250508 102239.557 INFO             PET5 index=  24                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                      - Force nonblocking algorithm
remote_send_local_bcast - Force remote-send-local-bcast algorithm
20250508 102239.557 INFO             PET5 index=  25                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20250508 102239.557 INFO             PET5 index=  26                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.557 INFO             PET5 index=  27                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET5 index=  28                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.557 INFO             PET5 index=  29                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial                             - Force Binomial algorithm
sched_smp                                  - Force smp algorithm
sched_scatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithm
sched_scatter_ring_allgather               - Force Scatter Ring Allgather algorithm
tsp_tree                               - Force Generic Transport Tree algorithm
tsp_scatterv_recexch_allgatherv        - Force Generic Transport Scatterv followed by Recursive Exchange Allgatherv algorithm
tsp_scatterv_ring_allgatherv           - Force Generic Transport Scatterv followed by Ring Allgatherv algorithm
tsp_ring                               - Force Generic Transport Ring algorithm
20250508 102239.557 INFO             PET5 index=  30                              MPIR_CVAR_IBCAST_SCATTERV_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET5 index=  31                    MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20250508 102239.557 INFO             PET5 index=  32                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_flat - Force flat algorithm
20250508 102239.557 INFO             PET5 index=  33                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20250508 102239.557 INFO             PET5 index=  34                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=  35                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                   - Force linear algorithm
local_gather_remote_send - Force local-gather-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=  36                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET5 index=  37                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20250508 102239.557 INFO             PET5 index=  38                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_long  - Force long inter algorithm
sched_short - Force short inter algorithm
20250508 102239.557 INFO             PET5 index=  39                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=  40                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=  41                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear         - Force linear algorithm
tsp_linear       - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET5 index=  42                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET5 index=  43                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20250508 102239.557 INFO             PET5 index=  44                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial - Force binomial algorithm
nb       - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=  45                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear                    - Force linear algorithm
nb                        - Force nonblocking algorithm
remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET5 index=  46                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_binomial     - Force binomial algorithm
tsp_tree       - Force genetric transport based tree algorithm
20250508 102239.557 INFO             PET5 index=  47                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20250508 102239.557 INFO             PET5 index=  48                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear                    - Force linear algorithm
sched_remote_send_local_scatter - Force remote-send-local-scatter algorithm
20250508 102239.557 INFO             PET5 index=  49                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=  50                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
linear - Force linear algorithm
nb     - Force nonblocking algorithm
20250508 102239.557 INFO             PET5 index=  51                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET5 index=  52                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear - Force linear algorithm
tsp_linear - Force generic transport based linear algorithm
20250508 102239.557 INFO             PET5 index=  53                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20250508 102239.558 INFO             PET5 index=  54                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20250508 102239.558 INFO             PET5 index=  55                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
k_brucks           - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
recexch_doubling   - Force recexch distance doubling algorithm
recexch_halving    - Force recexch distance halving algorithm
20250508 102239.558 INFO             PET5 index=  56                             MPIR_CVAR_ALLGATHER_BRUCKS_KVAL : radix (k) value for generic transport brucks based allgather
20250508 102239.558 INFO             PET5 index=  57                            MPIR_CVAR_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather
20250508 102239.558 INFO             PET5 index=  58               MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET5 index=  59                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
nb                        - Force nonblocking algorithm
20250508 102239.558 INFO             PET5 index=  60                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20250508 102239.558 INFO             PET5 index=  61                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20250508 102239.558 INFO             PET5 index=  62                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_ring               - Force ring algorithm
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
tsp_ring       - Force generic transport ring algorithm
tsp_brucks     - Force generic transport based brucks algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
20250508 102239.558 INFO             PET5 index=  63                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20250508 102239.558 INFO             PET5 index=  64                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20250508 102239.558 INFO             PET5 index=  65                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks             - Force brucks algorithm
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
ring               - Force ring algorithm
20250508 102239.558 INFO             PET5 index=  66                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET5 index=  67                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20250508 102239.558 INFO             PET5 index=  68                           MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL : k value for radix in brucks based iallgatherv
20250508 102239.558 INFO             PET5 index=  69                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks             - Force brucks algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_ring               - Force ring algorithm
tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase
tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase
tsp_ring             - Force generic transport ring algorithm
tsp_brucks           - Force generic transport based brucks algorithm
20250508 102239.558 INFO             PET5 index=  70                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20250508 102239.558 INFO             PET5 index=  71                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20250508 102239.558 INFO             PET5 index=  72                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20250508 102239.558 INFO             PET5 index=  73                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20250508 102239.558 INFO             PET5 index=  74                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
brucks                    - Force brucks algorithm
k_brucks                  - Force Force radix k brucks algorithm
nb                        - Force nonblocking algorithm
pairwise                  - Force pairwise algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET5 index=  75                              MPIR_CVAR_ALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based alltoall
20250508 102239.558 INFO             PET5 index=  76                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET5 index=  77                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_brucks            - Force brucks algorithm
sched_inplace           - Force inplace algorithm
sched_pairwise          - Force pairwise algorithm
sched_permuted_sendrecv - Force permuted sendrecv algorithm
tsp_ring            - Force generic transport based ring algorithm
tsp_brucks          - Force generic transport based brucks algorithm
tsp_scattered       - Force generic transport based scattered algorithm
20250508 102239.558 INFO             PET5 index=  78                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET5 index=  79                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET5 index=  80                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
pairwise_exchange - Force pairwise exchange algorithm
nb                - Force nonblocking algorithm
20250508 102239.558 INFO             PET5 index=  81                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_scattered       - Force generic transport based scattered algorithm
tsp_blocked         - Force generic transport blocked algorithm
tsp_inplace         - Force generic transport inplace algorithm
20250508 102239.558 INFO             PET5 index=  82                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET5 index=  83            MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.558 INFO             PET5 index=  84                   MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.558 INFO             PET5 index=  85                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                        - Force nonblocking algorithm
pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm
scattered                 - Force scattered algorithm
20250508 102239.558 INFO             PET5 index=  86                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                - Force nonblocking algorithm
pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET5 index=  87                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_blocked           - Force blocked algorithm
sched_inplace           - Force inplace algorithm
tsp_blocked   - Force generic transport based blocked algorithm
tsp_inplace   - Force generic transport based inplace algorithm
20250508 102239.558 INFO             PET5 index=  88                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_pairwise_exchange - Force pairwise exchange algorithm
20250508 102239.558 INFO             PET5 index=  89                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET5 index=  90                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20250508 102239.558 INFO             PET5 index=  91                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
binomial              - Force binomial algorithm
nb                    - Force nonblocking algorithm
smp                   - Force smp algorithm
reduce_scatter_gather - Force reduce scatter gather algorithm
20250508 102239.558 INFO             PET5 index=  92                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
local_reduce_remote_send - Force local-reduce-remote-send algorithm
nb                       - Force nonblocking algorithm
20250508 102239.558 INFO             PET5 index=  93                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20250508 102239.558 INFO             PET5 index=  94                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET5 index=  95                       MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET5 index=  96                             MPIR_CVAR_IREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET5 index=  97                          MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET5 index=  98                        MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET5 index=  99                        MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET5 index= 100                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET5 index= 101                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET5 index= 102                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET5 index= 103                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                   - Force smp algorithm
sched_binomial              - Force binomial algorithm
sched_reduce_scatter_gather - Force reduce scatter gather algorithm
tsp_tree                - Force Generic Transport Tree
tsp_ring                - Force Generic Transport Ring
20250508 102239.558 INFO             PET5 index= 104                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_local_reduce_remote_send - Force local-reduce-remote-send algorithm
20250508 102239.558 INFO             PET5 index= 105                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20250508 102239.558 INFO             PET5 index= 106                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20250508 102239.558 INFO             PET5 index= 107                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                       - Force nonblocking algorithm
smp                      - Force smp algorithm
recursive_doubling       - Force recursive doubling algorithm
reduce_scatter_allgather - Force reduce scatter allgather algorithm
tree                     - Force pipelined tree algorithm
recexch                  - Force generic transport recursive exchange algorithm
ring                     - Force ring algorithm
k_reduce_scatter_allgather - Force reduce scatter allgather algorithm
20250508 102239.558 INFO             PET5 index= 108                               MPIR_CVAR_ALLREDUCE_TREE_TYPE : Tree type for tree based allreduce knomial_1 is default as it supports both commutative and non-commutative reduce operations kary      - kary tree type knomial_1 - knomial_1 tree type (tree grows starting from the left of the root) knomial_2 - knomial_2 tree type (tree grows starting from the right of the root) topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type
20250508 102239.558 INFO             PET5 index= 109                               MPIR_CVAR_ALLREDUCE_TREE_KVAL : Indicates the branching factor for kary or knomial trees.
20250508 102239.558 INFO             PET5 index= 110                     MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE : This cvar controls if the leaders are reordered based on the number of ranks in each group.
20250508 102239.558 INFO             PET5 index= 111                           MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD : This cvar controls the size of the overhead.
20250508 102239.558 INFO             PET5 index= 112                        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS : This cvar controls the latency between different groups.
20250508 102239.558 INFO             PET5 index= 113                      MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES : This cvar controls the latency between different switches in the same groups.
20250508 102239.558 INFO             PET5 index= 114                      MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES : This cvar controls the latency in the same switch.
20250508 102239.558 INFO             PET5 index= 115                MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based allreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET5 index= 116                   MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET5 index= 117                            MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based allreduce
20250508 102239.558 INFO             PET5 index= 118               MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV : This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.
20250508 102239.558 INFO             PET5 index= 119                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                    - Force nonblocking algorithm
reduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20250508 102239.558 INFO             PET5 index= 120                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20250508 102239.558 INFO             PET5 index= 121                              MPIR_CVAR_IALLREDUCE_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20250508 102239.558 INFO             PET5 index= 122               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce. Default value is 0, that is, no pipelining by default
20250508 102239.558 INFO             PET5 index= 123                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20250508 102239.558 INFO             PET5 index= 124                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20250508 102239.558 INFO             PET5 index= 125                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_naive                      - Force naive algorithm
sched_smp                        - Force smp algorithm
sched_recursive_doubling         - Force recursive doubling algorithm
sched_reduce_scatter_allgather   - Force reduce scatter allgather algorithm
tsp_recexch_single_buffer    - Force generic transport recursive exchange with single buffer for receives
tsp_recexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
tsp_tree                     - Force generic transport tree algorithm
tsp_ring                     - Force generic transport ring algorithm
tsp_recexch_reduce_scatter_recexch_allgatherv  - Force generic transport recursive exchange with reduce scatter and allgatherv
20250508 102239.558 INFO             PET5 index= 126                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20250508 102239.558 INFO             PET5 index= 127          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20250508 102239.558 INFO             PET5 index= 128                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
noncommutative     - Force noncommutative algorithm
pairwise           - Force pairwise algorithm
recursive_doubling - Force recursive doubling algorithm
recursive_halving  - Force recursive halving algorithm
20250508 102239.558 INFO             PET5 index= 129                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET5 index= 130                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20250508 102239.558 INFO             PET5 index= 131                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET5 index= 132                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET5 index= 133              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
noncommutative     - Force noncommutative algorithm
recursive_doubling - Force recursive doubling algorithm
pairwise           - Force pairwise algorithm
recursive_halving  - Force recursive halving algorithm
nb                 - Force nonblocking algorithm
20250508 102239.558 INFO             PET5 index= 134              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                          - Force nonblocking algorithm
remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20250508 102239.558 INFO             PET5 index= 135                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20250508 102239.558 INFO             PET5 index= 136             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_noncommutative     - Force noncommutative algorithm
sched_recursive_doubling - Force recursive doubling algorithm
sched_pairwise           - Force pairwise algorithm
sched_recursive_halving  - Force recursive halving algorithm
tsp_recexch          - Force generic transport recursive exchange algorithm
20250508 102239.558 INFO             PET5 index= 137             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20250508 102239.558 INFO             PET5 index= 138                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
smp                - Force smp algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.559 INFO             PET5 index= 139                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_smp                  - Force smp algorithm
sched_recursive_doubling   - Force recursive doubling algorithm
tsp_recursive_doubling - Force generic transport recursive doubling algorithm
20250508 102239.559 INFO             PET5 index= 140                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb                 - Force nonblocking algorithm
recursive_doubling - Force recursive doubling algorithm
20250508 102239.559 INFO             PET5 index= 141                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_recursive_doubling - Force recursive doubling algorithm
20250508 102239.559 INFO             PET5 index= 142                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET5 index= 143                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nonblocking algorithm
20250508 102239.559 INFO             PET5 index= 144               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 145               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear    - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 146               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 147               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 148              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 149              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 150                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 151                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 152                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 153                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 154                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 155                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 156               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 157               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear  - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 158                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 159                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
nb   - Force nb algorithm
20250508 102239.559 INFO             PET5 index= 160               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 161               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithm
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)
sched_auto - Internal algorithm selection for sched-based algorithms
sched_linear          - Force linear algorithm
tsp_linear        - Force generic transport based linear algorithm
20250508 102239.559 INFO             PET5 index= 162                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 163                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 164                    MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 165                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 166                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 167                      MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Bcast_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 168                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 169                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 170                     MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 171                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 172                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 173                    MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Gatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 174                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 175                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 176                    MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 177                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 178                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 179                   MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scatterv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 180                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 181                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 182                  MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 183                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 184                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 185                 MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 186                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 187                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 188                   MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 189                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 190                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 191                  MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 192                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 193                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 194                  MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 195                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 196                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 197                     MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 198                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 199                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 200                  MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Allreduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 201                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 202                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 203             MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 204            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 205           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 206       MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Reduce_scatter_block_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 207                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 208                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 209                       MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Scan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 210                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 211                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 212                     MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Exscan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 213              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 214             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 215         MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 216             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 217            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 218        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 219               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.559 INFO             PET5 index= 220              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 221          MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 222              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 223             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 224         MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 225              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 226             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 227         MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE : This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to "percoll".  If set to true, MPI_Neighbor_alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.
20250508 102239.560 INFO             PET5 index= 228                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20250508 102239.560 INFO             PET5 index= 229                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20250508 102239.560 INFO             PET5 index= 230                             MPIR_CVAR_IALLTOALL_BRUCKS_KVAL : radix (k) value for generic transport brucks based ialltoall
20250508 102239.560 INFO             PET5 index= 231                   MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR : If set to true, the tsp based brucks algorithm will allocate dedicated send and receive buffers for every neighbor in the brucks algorithm. Otherwise, it would reuse a single buffer for sending and receiving data to/from neighbors
20250508 102239.560 INFO             PET5 index= 232             MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS : Maximum number of outstanding sends and recvs posted at a time
20250508 102239.560 INFO             PET5 index= 233                    MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE : Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size
20250508 102239.560 INFO             PET5 index= 234                                MPIR_CVAR_DEVICE_COLLECTIVES : Variable to select whether the device can override the
MPIR-level collective algorithms.
all     - Always prefer the device collectives
none    - Never pick the device collectives
percoll - Use the per-collective CVARs to decide
20250508 102239.560 INFO             PET5 index= 235                               MPIR_CVAR_COLLECTIVE_FALLBACK : Variable to control what the MPI library should do if the
user-specified collective algorithm does not work for the
arguments passed in by the user.
error   - throw an error
print   - print an error message and fallback to the internally selected algorithm
silent  - silently fallback to the internally selected algorithm
20250508 102239.560 INFO             PET5 index= 236                   MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.560 INFO             PET5 index= 237                                    MPIR_CVAR_HIERARCHY_DUMP : If set to true, each rank will dump the hierarchy data structure to a file named "hierarchy[rank]" in the current folder. If set to false, the hierarchy data structure will not be dumped.
20250508 102239.560 INFO             PET5 index= 238                                  MPIR_CVAR_COORDINATES_FILE : Defines the location of the input coordinates file.
20250508 102239.560 INFO             PET5 index= 239                                    MPIR_CVAR_COLL_TREE_DUMP : If set to true, each rank will dump the tree to a file named "colltree[rank].json" in the current folder. If set to false, the tree will not be dumped.
20250508 102239.560 INFO             PET5 index= 240                                  MPIR_CVAR_COORDINATES_DUMP : If set to true, rank 0 will dump the network coordinates to a file named "coords" in the current folder. If set to false, the network coordinates will not be dumped.
20250508 102239.560 INFO             PET5 index= 241                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20250508 102239.560 INFO             PET5 index= 242                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20250508 102239.560 INFO             PET5 index= 243                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20250508 102239.560 INFO             PET5 index= 244                                MPIR_CVAR_DATALOOP_FAST_SEEK : use a datatype-specialized algorithm to shortcut seeking to the correct location in a noncontiguous buffer
20250508 102239.560 INFO             PET5 index= 245                             MPIR_CVAR_YAKSA_COMPLEX_SUPPORT : This CVAR indicates that complex type reduction is not supported in yaksa.
20250508 102239.560 INFO             PET5 index= 246                                MPIR_CVAR_GPU_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET5 index= 247                           MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT : This CVAR indicates that double type is not supported on the GPU.
20250508 102239.560 INFO             PET5 index= 248                            MPIR_CVAR_ENABLE_YAKSA_REDUCTION : This cvar enables yaksa based reduction for local reduce.
20250508 102239.560 INFO             PET5 index= 249                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20250508 102239.560 INFO             PET5 index= 250                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20250508 102239.560 INFO             PET5 index= 251                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20250508 102239.560 INFO             PET5 index= 252                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20250508 102239.560 INFO             PET5 index= 253                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20250508 102239.560 INFO             PET5 index= 254                          MPIR_CVAR_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. Can be set to auto or comma-separated list of logical processors. When set to auto - MPICH will automatically select logical CPU cores to decide affinity of the progress threads. When set to comma-separated list of logical processors - In case of N progress threads per process, the first N logical processors from list will be assigned to threads of first local process, the next N logical processors from list - to second local process and so on. For example, thread affinity is "0,1,2,3", 2 progress threads per process and 2 processes per node. Progress threads of first local process will be pinned on logical processors "0,1", progress threads of second local process - on "2,3". Cannot work together with MPIR_CVAR_NUM_CLIQUES or MPIR_CVAR_ODD_EVEN_CLIQUES.
20250508 102239.560 INFO             PET5 index= 255                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20250508 102239.560 INFO             PET5 index= 256                                 MPIR_CVAR_COREDUMP_ON_ABORT : Call libc abort() to generate a corefile
20250508 102239.560 INFO             PET5 index= 257                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20250508 102239.560 INFO             PET5 index= 258                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20250508 102239.560 INFO             PET5 index= 259                                     MPIR_CVAR_DEBUG_SUMMARY : If true, print internal summary of various debug information, such as memory allocation by category. Each layer may print their own summary information. For example, ch4-ofi may print its provider capability settings.
20250508 102239.560 INFO             PET5 index= 260                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20250508 102239.560 INFO             PET5 index= 261                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20250508 102239.560 INFO             PET5 index= 262                    MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST : If true, mpl/ze will use immediate command list for copying
20250508 102239.560 INFO             PET5 index= 263                    MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES : If true, mpl/ze will use command queues in a round-robin fashion. If false, only command queues of index 0 will be used.
20250508 102239.560 INFO             PET5 index= 264                            MPIR_CVAR_NO_COLLECTIVE_FINALIZE : If true, prevent MPI_Finalize to invoke collective behavior such as barrier or communicating to other processes. Consequently, it may result in leaking memory or losing messages due to pre-mature exiting. The default is false, which may invoke collective behaviors at finalize.
20250508 102239.560 INFO             PET5 index= 265                                     MPIR_CVAR_FINALIZE_WAIT : If true, poll progress at MPI_Finalize until reference count on MPI_COMM_WORLD and MPI_COMM_SELF reaches zero. This may be necessary to prevent remote processes hanging if it has pending communication protocols, e.g. a rendezvous send.
20250508 102239.560 INFO             PET5 index= 266                                 MPIR_CVAR_REQUEST_ERR_FATAL : By default, MPI_Waitall, MPI_Testall, MPI_Waitsome, and MPI_Testsome return MPI_ERR_IN_STATUS when one of the request fails. If MPIR_CVAR_REQUEST_ERR_FATAL is set to true, these routines will return the error code of the request immediately. The default MPI_ERRS_ARE_FATAL error handler will dump a error stack in this case, which maybe more convenient for debugging. This cvar will also make nonblocking shched return error right away as it issues operations.
20250508 102239.560 INFO             PET5 index= 267                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during MPI_{Waitany,Waitsome} in terms of number of processed requests before polling.
20250508 102239.560 INFO             PET5 index= 268                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20250508 102239.560 INFO             PET5 index= 269                            MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT : Sets the timeout in seconds to dump outstanding requests when progress wait is not making progress for some time.
20250508 102239.560 INFO             PET5 index= 270                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20250508 102239.560 INFO             PET5 index= 271                                    MPIR_CVAR_QMPI_TOOL_LIST : Set the number and order of QMPI tools to be loaded by the MPI library when it is initialized.
20250508 102239.560 INFO             PET5 index= 272                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20250508 102239.560 INFO             PET5 index= 273                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20250508 102239.560 INFO             PET5 index= 274                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20250508 102239.560 INFO             PET5 index= 275                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20250508 102239.560 INFO             PET5 index= 276                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20250508 102239.560 INFO             PET5 index= 277                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20250508 102239.560 INFO             PET5 index= 278                                  MPIR_CVAR_CLIQUES_BY_BLOCK : Specify to divide processes into cliques by uniform blocks. The default is to divide in round-robin fashion. Used for debugging on a single machine.
20250508 102239.560 INFO             PET5 index= 279                                       MPIR_CVAR_PMI_VERSION : Variable to select runtime PMI version.
1        - PMI (default)
2        - PMI2
x        - PMIx
20250508 102239.560 INFO             PET5 index= 280                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20250508 102239.560 INFO             PET5 index= 281                                        MPIR_CVAR_ENABLE_GPU : Control MPICH GPU support. If set to 0, all GPU support is disabled and we do not query the buffer type internally because we assume no GPU buffer is use.
20250508 102239.560 INFO             PET5 index= 282                               MPIR_CVAR_GPU_HAS_WAIT_KERNEL : If set to 1, avoid allocate allocating GPU registered host buffers for temporary buffers. When stream workq and GPU wait kernels are in use, access APIs for GPU registered memory may cause deadlock.
20250508 102239.560 INFO             PET5 index= 283                               MPIR_CVAR_ENABLE_GPU_REGISTER : Control whether to actually register buffers with the GPU runtime in MPIR_gpu_register_host. This could lower the latency of certain GPU communication at the cost of some amount of GPU memory consumed by the MPI library. By default, registration is enabled.
20250508 102239.560 INFO             PET5 index= 284                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.560 INFO             PET5 index= 285                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20250508 102239.560 INFO             PET5 index= 286                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20250508 102239.560 INFO             PET5 index= 287                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20250508 102239.560 INFO             PET5 index= 288                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20250508 102239.560 INFO             PET5 index= 289                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20250508 102239.560 INFO             PET5 index= 290                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET5 index= 291                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20250508 102239.560 INFO             PET5 index= 292                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20250508 102239.560 INFO             PET5 index= 293                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20250508 102239.560 INFO             PET5 index= 294                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20250508 102239.560 INFO             PET5 index= 295                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.560 INFO             PET5 index= 296               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20250508 102239.560 INFO             PET5 index= 297                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20250508 102239.560 INFO             PET5 index= 298               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchronization approach.  Change this value if programs fail because they run out of requests or other internal resources
20250508 102239.560 INFO             PET5 index= 299                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be negative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20250508 102239.560 INFO             PET5 index= 300            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation available in the ending synchronization call.
20250508 102239.560 INFO             PET5 index= 301                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20250508 102239.560 INFO             PET5 index= 302                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive value.
20250508 102239.560 INFO             PET5 index= 303                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20250508 102239.560 INFO             PET5 index= 304                                    MPIR_CVAR_CH3_PG_VERBOSE : If set, print the PG state on finalize.
20250508 102239.560 INFO             PET5 index= 305                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.560 INFO             PET5 index= 306                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20250508 102239.561 INFO             PET5 index= 307                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.561 INFO             PET5 index= 308                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20250508 102239.561 INFO             PET5 index= 309           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediately.  Requires a positive value.
20250508 102239.561 INFO             PET5 index= 310                                  MPIR_CVAR_OFI_USE_PROVIDER : This variable is no longer supported. Use FI_PROVIDER instead to select libfabric providers.
20250508 102239.561 INFO             PET5 index= 311                               MPIR_CVAR_SINGLE_HOST_ENABLED : Set this variable to true to indicate that processes are launched on a single host. The current implication is to avoid the cxi provider to prevent the use of scarce hardware resources.
20250508 102239.561 INFO             PET5 index= 312                    MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE : For long message to be sent using pipeline rather than default RDMA read.
20250508 102239.561 INFO             PET5 index= 313                         MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir                        - Fallback to MPIR collectives
trigger_tree_tagged         - Force triggered ops based Tagged Tree
trigger_tree_rma            - Force triggered ops based RMA Tree
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_OFI_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET5 index= 314                                     MPIR_CVAR_OFI_SKIP_IPV6 : Skip IPv6 providers.
20250508 102239.561 INFO             PET5 index= 315                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20250508 102239.561 INFO             PET5 index= 316                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20250508 102239.561 INFO             PET5 index= 317                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20250508 102239.561 INFO             PET5 index= 318                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20250508 102239.561 INFO             PET5 index= 319                    MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS : If true, enable virtual addressing for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET5 index= 320                       MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED : If true, require all OFI memory regions must be backed by physical memory pages at the time the registration call is made. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET5 index= 321                   MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL : If true, memory registration call supports registering with NULL addresses.
20250508 102239.561 INFO             PET5 index= 322                        MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY : If true, enable provider supplied key for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.
20250508 102239.561 INFO             PET5 index= 323                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20250508 102239.561 INFO             PET5 index= 324                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20250508 102239.561 INFO             PET5 index= 325                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20250508 102239.561 INFO             PET5 index= 326                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20250508 102239.561 INFO             PET5 index= 327                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20250508 102239.561 INFO             PET5 index= 328                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20250508 102239.561 INFO             PET5 index= 329              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20250508 102239.561 INFO             PET5 index= 330                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20250508 102239.561 INFO             PET5 index= 331                               MPIR_CVAR_CH4_OFI_ENABLE_HMEM : If true, uses GPU direct RDMA support in the provider.
20250508 102239.561 INFO             PET5 index= 332                            MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM : If true, need to register the buffer to use GPU direct RDMA.
20250508 102239.561 INFO             PET5 index= 333                        MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD : The threshold to start using GPU direct RDMA.
20250508 102239.561 INFO             PET5 index= 334                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET5 index= 335                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET5 index= 336                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20250508 102239.561 INFO             PET5 index= 337                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET5 index= 338                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20250508 102239.561 INFO             PET5 index= 339                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20250508 102239.561 INFO             PET5 index= 340                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20250508 102239.561 INFO             PET5 index= 341                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20250508 102239.561 INFO             PET5 index= 342              MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS : Specifies the number of optimized memory regions supported by the provider. An optimized memory region is used for lower-overhead, unordered RMA operations. It uses a low-overhead RX path and additionally, a low-overhead packet format may be used to target an optimized memory region.
20250508 102239.561 INFO             PET5 index= 343                     MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL : Specifies the interval for manually flushing RMA operations when automatic progress is not enabled. It the underlying OFI provider supports auto data progress, this value is ignored. If the value is -1, this optimization will be turned off.
20250508 102239.561 INFO             PET5 index= 344                             MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX : Specifies the maximum number of iovecs to allocate for RMA operations to/from noncontiguous buffers.
20250508 102239.561 INFO             PET5 index= 345                        MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which OFI native path switches from eager to rendezvous mode. It does not affect the AM path eager limit. Having this gives a way to reliably test native non-path. If the number is positive, OFI will init the MPIDI_OFI_global.max_msg_size to the value of cvar. If the number is negative, OFI will init the MPIDI_OFI_globa.max_msg_size using whatever provider gives (which might be unlimited for socket provider).
20250508 102239.561 INFO             PET5 index= 346                                  MPIR_CVAR_CH4_OFI_MAX_NICS : If set to positive number, this cvar determines the maximum number of physical nics to use (if more than one is available). If the number is -1, underlying netmod or shmmod automatically uses an optimal number depending on what is detected on the system up to the limit determined by MPIDI_MAX_NICS (in ofi_types.h).
20250508 102239.561 INFO             PET5 index= 347                 MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING : If true, this cvar enables striping of large messages across multiple NICs.
20250508 102239.561 INFO             PET5 index= 348              MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD : Striping will happen for message sizes beyond this threshold.
20250508 102239.561 INFO             PET5 index= 349                  MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING : Multi-NIC hashing means to use more than one NIC to send and receive messages above a certain size.  If set to positive number, this feature will be turned on. If set to 0, this feature will be turned off. If the number is -1, MPICH automatically determines whether to use multi-nic hashing depending on what is detected on the system (e.g., number of NICs available, number of processes sharing the NICs).
20250508 102239.561 INFO             PET5 index= 350                     MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE : Controls the multirecv am buffer size. It is recommended to match this to the hugepage size so that the buffer can be allocated at the page boundary.
20250508 102239.561 INFO             PET5 index= 351                                  MPIR_CVAR_OFI_USE_MIN_NICS : If true and all nodes do not have the same number of NICs, MPICH will fall back to using the fewest number of NICs instead of returning an error.
20250508 102239.561 INFO             PET5 index= 352                          MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED : If true, enable OFI triggered ops for MPI collectives.
20250508 102239.561 INFO             PET5 index= 353                      MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the sender side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET5 index= 354                   MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE : Specifies GPU engine type for GPU pt2pt on the receiver side.
compute - use a compute engine
copy_high_bandwidth - use a high-bandwidth copy engine
copy_low_latency - use a low-latency copy engine
yaksa - use Yaksa
20250508 102239.561 INFO             PET5 index= 355                       MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE : If true, enable pipeline for GPU data transfer. GPU pipeline does not support non-contiguous datatypes or mixed buffer types (i.e. GPU send buffer, host recv buffer). If GPU pipeline is enabled, the unsupported scenarios will cause undefined behavior if encountered.
20250508 102239.561 INFO             PET5 index= 356                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD : This is the threshold to start using GPU pipeline.
20250508 102239.561 INFO             PET5 index= 357                    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU pipeline data transfer.
20250508 102239.561 INFO             PET5 index= 358        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU pipeline data transfer in each block/chunk of the pool.
20250508 102239.561 INFO             PET5 index= 359              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU pipeline data transfer
20250508 102239.561 INFO             PET5 index= 360              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the sender side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET5 index= 361              MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE : Specifies the GPU engine type for GPU pipeline on the receiver side, default is MPL_GPU_ENGINE_TYPE_COMPUTE
20250508 102239.561 INFO             PET5 index= 362                      MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE : Avoid use fi_inject_write. For some provider, e.g. tcp;ofi_rxm, inject write may break the synchronization.
20250508 102239.561 INFO             PET5 index= 363                                       MPIR_CVAR_UCX_DT_RECV : Variable to select method for receiving noncontiguous data
true                - Use UCX datatype with pack/unpack callbacks
false               - MPICH will decide to pack/unpack at completion or use IOVs
based on the datatype
20250508 102239.561 INFO             PET5 index= 364                          MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE : By default, we will cache ipc handles using the specialized cache mechanism. If the
gpu-specific backend does not implement a specialized cache, then we will fallback to
the generic cache mechanism. Users can optionally force the generic cache mechanism or
disable ipc caching entirely.
generic - use the cache mechanism in the generic layer
specialized - use the cache mechanism in a gpu-specific mpl layer (if applicable)
disabled - disable caching completely
20250508 102239.561 INFO             PET5 index= 365                         MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD (in bytes), then enable GPU-based single copy protocol for intranode communication. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET5 index= 366                    MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE : If a send message size is less than or equal to MPIR_CVAR_CH4_IPC_GPU_FAST_COPY_MAX_SIZE (in bytes), then enable GPU-basedfast memcpy. The environment variable is valid only when then GPU IPC shmmod is enabled.
20250508 102239.561 INFO             PET5 index= 367                       MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE : Variable to select implementation for ZE shareable IPC handle
pidfd - use pidfd_getfd syscall to implement shareable IPC handle
drmfd - force to use device fd-based shareable IPC handle
20250508 102239.561 INFO             PET5 index= 368                           MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE : By default, select engine type automatically
auto - select automatically
compute - use compute engine
copy_high_bandwidth - use high-bandwidth copy engine
copy_low_latency - use low-latency copy engine
20250508 102239.561 INFO             PET5 index= 369                   MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL : By default, use read protocol.
auto - select automatically
read - use read protocol
write - use write protocol if remote device is visible
20250508 102239.561 INFO             PET5 index= 370                           MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR : If an address is used more than once in the last ten send operations, map it for IPC use even if it is below the IPC threshold.
20250508 102239.561 INFO             PET5 index= 371                                  MPIR_CVAR_CH4_XPMEM_ENABLE : To manually disable XPMEM set to 0. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET5 index= 372                       MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD : If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD (in bytes), then enable XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled.
20250508 102239.561 INFO             PET5 index= 373                       MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
ipc_read - Uses read-based collective with ipc
20250508 102239.561 INFO             PET5 index= 374                      MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node bcast
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET5 index= 375                      MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET5 index= 376                     MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node reduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET5 index= 377                   MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node allreduce
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET5 index= 378                     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node barrier
mpir           - Fallback to MPIR collectives
release_gather - Force shm optimized algo using release, gather primitives
auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)
20250508 102239.561 INFO             PET5 index= 379                    MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM : Variable to select algorithm for intra-node alltoall
mpir           - Fallback to MPIR collectives (default)
ipc_read    - Uses read-based collective with ipc
20250508 102239.561 INFO             PET5 index= 380                              MPIR_CVAR_POSIX_POLL_FREQUENCY : This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20250508 102239.561 INFO             PET5 index= 381                 MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read bcast only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET5 index= 382              MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD : Use gpu ipc read alltoall only when the message size is larger than this threshold.
20250508 102239.561 INFO             PET5 index= 383                         MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD : Use posix optimized collectives (release_gather) only when the total number of Bcast, Reduce, Barrier, and Allreduce calls on the node level communicator is more than this threshold.
20250508 102239.561 INFO             PET5 index= 384                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : If non-empty, this cvar specifies which shm posix eager module to use
20250508 102239.561 INFO             PET5 index= 385         MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.561 INFO             PET5 index= 386                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS : The number of cells used for the depth of the iqueue.
20250508 102239.561 INFO             PET5 index= 387                    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE : Size of each cell.
20250508 102239.561 INFO             PET5 index= 388                           MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE : Maximum shared memory created per node for optimized intra-node collectives (in KB)
20250508 102239.561 INFO             PET5 index= 389                 MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the bcast buffer (in bytes)
20250508 102239.561 INFO             PET5 index= 390                         MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS : Number of cells the bcast buffer is divided into
20250508 102239.561 INFO             PET5 index= 391                MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE : Total size of the reduce buffer per rank (in bytes)
20250508 102239.561 INFO             PET5 index= 392                        MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS : Number of cells the reduce buffer is divided into, for each rank
20250508 102239.561 INFO             PET5 index= 393                         MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node bcast
20250508 102239.561 INFO             PET5 index= 394                         MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE : Tree type for intra-node bcast tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET5 index= 395                        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL : K value for the kary/knomial tree for intra-node reduce
20250508 102239.561 INFO             PET5 index= 396                        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE : Tree type for intra-node reduce tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.
20250508 102239.561 INFO             PET5 index= 397             MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES : Enable collective specific intra-node trees which leverage the memory hierarchy of a machine. Depends on hwloc to extract the binding information of each rank. Pick a leader rank per package (socket), then create a per_package tree for ranks on a same package, package leaders tree for package leaders. For Bcast - Assemble the per_package and package_leaders tree in such a way that leaders interact among themselves first before interacting with package local ranks. Both the package_leaders and per_package trees are left skewed (children are added from left to right, first child to be added is the first one to be processed in traversal) For Reduce - Assemble the per_package and package_leaders tree in such a way that a leader rank interacts with its package local ranks first, then with the other package leaders. Both the per_package and package_leaders tree is right skewed (children are added in reverse order, first child to be added is the last one to be processed in traversal) The tree radix and tree type of
20250508 102239.561 INFO             PET5 index= 398                               MPIR_CVAR_BARRIER_COMPOSITION : Select composition (inter_node + intra_node) for Barrier 0 Auto selection 1 NM + SHM 2 NM only
20250508 102239.562 INFO             PET5 index= 399                                 MPIR_CVAR_BCAST_COMPOSITION : Select composition (inter_node + intra_node) for Bcast 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.562 INFO             PET5 index= 400                             MPIR_CVAR_ALLREDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Allreduce 0 Auto selection 1 NM + SHM with reduce + bcast 2 NM only composition 3 SHM only composition 4 Multi leaders based inter node + intra node composition
20250508 102239.562 INFO             PET5 index= 401                             MPIR_CVAR_ALLGATHER_COMPOSITION : Select composition (inter_node + intra_node) for Allgather 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.562 INFO             PET5 index= 402                              MPIR_CVAR_ALLTOALL_COMPOSITION : Select composition (inter_node + intra_node) for Alltoall 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition
20250508 102239.562 INFO             PET5 index= 403                                MPIR_CVAR_REDUCE_COMPOSITION : Select composition (inter_node + intra_node) for Reduce 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only
20250508 102239.562 INFO             PET5 index= 404                             MPIR_CVAR_ALLTOALL_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Alltoall (in bytes)
20250508 102239.562 INFO             PET5 index= 405                            MPIR_CVAR_ALLGATHER_SHM_PER_RANK : Shared memory region per rank for multi-leaders based composition for MPI_Allgather (in bytes)
20250508 102239.562 INFO             PET5 index= 406                                   MPIR_CVAR_NUM_MULTI_LEADS : Number of leader ranks per node to be used for multi-leaders based collective algorithms
20250508 102239.562 INFO             PET5 index= 407                          MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER : Shared memory region per node-leader for multi-leaders based composition for MPI_Allreduce (in bytes) If it is undefined by the user, it is set to the message size of the first call to the algorithm. Max shared memory size is limited to 4MB.
20250508 102239.562 INFO             PET5 index= 408                        MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER : Amount of data reduced in allreduce delta composition's reduce local step (in bytes). Smaller msg size per leader avoids cache misses and improves performance. Experiments indicate 512 to be the best value.
20250508 102239.562 INFO             PET5 index= 409                      MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS : number of offsets in the allreduce delta composition's local copy The value of 2 performed the best in our 2 NIC test cases.
20250508 102239.562 INFO             PET5 index= 410                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20250508 102239.562 INFO             PET5 index= 411                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20250508 102239.562 INFO             PET5 index= 412                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20250508 102239.562 INFO             PET5 index= 413                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20250508 102239.562 INFO             PET5 index= 414                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) lockless
20250508 102239.562 INFO             PET5 index= 415                                      MPIR_CVAR_CH4_NUM_VCIS : Sets the number of VCIs to be implicitly used (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET5 index= 416                                  MPIR_CVAR_CH4_RESERVE_VCIS : Sets the number of VCIs that user can explicitly allocate (should be a subset of MPIDI_CH4_MAX_VCIS).
20250508 102239.562 INFO             PET5 index= 417               MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE : Defines the location of tuning file.
20250508 102239.562 INFO             PET5 index= 418                               MPIR_CVAR_CH4_IOV_DENSITY_MIN : Defines the threshold of high-density datatype. The density is calculated by (datatype_size / datatype_num_contig_blocks).
20250508 102239.562 INFO             PET5 index= 419                              MPIR_CVAR_CH4_PACK_BUFFER_SIZE : Specifies the number of buffers for packing/unpacking active messages in each block of the pool. The size here should be greater or equal to the max of the eager buffer limit of SHM and NETMOD.
20250508 102239.562 INFO             PET5 index= 420                    MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK : Specifies the number of buffers for packing/unpacking active messages in each block of the pool.
20250508 102239.562 INFO             PET5 index= 421                          MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS : Specifies the max number of buffers for packing/unpacking buffers in the pool. Use 0 for unlimited.
20250508 102239.562 INFO             PET5 index= 422                       MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ : Specifies the buffer size (in bytes) for GPU collectives data transfer.
20250508 102239.562 INFO             PET5 index= 423                MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK : Specifies the number of buffers for GPU collectives data transfer in each block/chunk of the pool.
20250508 102239.562 INFO             PET5 index= 424                      MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS : Specifies the total number of buffers for GPU collectives data transfer.
20250508 102239.562 INFO             PET5 index= 425                               MPIR_CVAR_CH4_GLOBAL_PROGRESS : If on, poll global progress every once a while. With per-vci configuration, turning global progress off may improve the threading performance.
20250508 102239.562 INFO             PET5 index= 426                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20250508 102239.562 INFO             PET5 index= 427                           MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ : Enable stream enqueue operations via stream work queue. Requires progress thread on the corresponding MPIX stream. Reference: MPIX_Stream_progress and MPIX_Start_progress_thread.
20250508 102239.562 INFO             PET5 index= 428                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20250508 102239.562 INFO             PET5 index= 429                MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS : If true, allows RMA synchronization calls to dynamically reduce the frequency of internal progress polling for incoming RMA active messages received on the target process. The RMA synchronization call initially polls progress with a low frequency (defined by MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL) to reduce synchronization overhead. Once any RMA active message has been received, it will always poll progress once at every synchronization call to ensure prompt target-side progress. Effective only for passive target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}.
20250508 102239.562 INFO             PET5 index= 430                      MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL : Specifies a static interval of progress polling for incoming RMA active messages received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Invalid when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET5 index= 431             MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL : Specifies the interval of progress polling with low frequency for incoming RMA active message received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Used when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.
20250508 102239.562 INFO             PET5 index= 432            MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE : The genq shmem code allocates pools of cells on each process and, when needed, a cell is removed from the pool and passed to another process. This can happen by either removing a cell from the pool of the sending process or from the pool of the receiving process. This CVAR determines which pool to use. If true, the cell will come from the sender-side. If false, the cell will com from the receiver-side. There are specific advantages of using receiver-side cells when combined with the "avx" fast configure option, which allows MPICH to use AVX streaming copy intrintrinsics, when available, to avoid polluting the cache of the sender with the data being copied to the receiver. Using receiver-side cells does have the trade-off of requiring an MPMC lock for the free queue rather than an MPSC lock, which is used for sender-side cells. Initial performance analysis shows that using the MPMC lock in this case had no significant performance loss. By default, the queue will continue to use sender-side queues until the pe
20250508 102239.562 INFO             PET5 index= 433                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20250508 102239.562 INFO             PET5 index= 434                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20250508 102239.562 INFO             PET5 index= 435                             MPIR_CVAR_SHM_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20250508 102239.562 INFO             PET5 index= 436                                 MPIR_CVAR_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20250508 102239.562 INFO             PET5 index= 437                                MPIR_CVAR_ENABLE_HEAVY_YIELD : If enabled, use nanosleep to ensure other threads have a chance to grab the lock. Note: this may not work with some thread runtimes, e.g. non-preemptive user-level threads.
20250508 102239.562 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20250508 102239.562 INFO             PET5 main: --- VMK::log() start -------------------------------------
20250508 102239.562 INFO             PET5 main: vm located at: 0x145f05ea0
20250508 102239.562 INFO             PET5 main: mpionly=1 threadsflag=0
20250508 102239.562 INFO             PET5 main: ssiCount=1 localSsi=0
20250508 102239.562 INFO             PET5 main: devCount=0 ssiLocalDevCount=0
20250508 102239.562 INFO             PET5 main: petCount=6 ssiLocalPetCount=6
20250508 102239.562 INFO             PET5 main: localPet=5 mypthid=0x1edb24f40 ssiLocalPet=5 currentSsiPe=-1
20250508 102239.562 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.562 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.562 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20250508 102239.562 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.562 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20250508 102239.562 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.562 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20250508 102239.562 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.562 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20250508 102239.562 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.562 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20250508 102239.562 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.562 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20250508 102239.562 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20250508 102239.563 INFO             PET5 Executing 'userm1_setvm'
20250508 102239.563 INFO             PET5 Executing 'userm1_register'
20250508 102239.563 INFO             PET5 Executing 'userm2_setvm'
20250508 102239.563 DEBUG            PET5 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 DEBUG            PET5 vmkt_create()#228 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20250508 102239.563 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20250508 102239.563 INFO             PET5 model1: vm located at: 0x137005e10
20250508 102239.563 INFO             PET5 model1: mpionly=1 threadsflag=0
20250508 102239.563 INFO             PET5 model1: ssiCount=1 localSsi=0
20250508 102239.563 INFO             PET5 model1: devCount=0 ssiLocalDevCount=0
20250508 102239.563 INFO             PET5 model1: petCount=6 ssiLocalPetCount=6
20250508 102239.563 INFO             PET5 model1: localPet=5 mypthid=0x1edb24f40 ssiLocalPet=5 currentSsiPe=-1
20250508 102239.563 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20250508 102239.563 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20250508 102239.563 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20250508 102239.563 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20250508 102239.563 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20250508 102239.563 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20250508 102239.563 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20250508 102239.563 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20250508 102239.563 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20250508 102239.563 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20250508 102239.563 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20250508 102239.563 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20250508 102239.563 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20250508 102239.563 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20250508 102239.568 INFO             PET5 Entering 'user1_run'
20250508 102239.568 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102239.837 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102240.069 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102240.294 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102240.519 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20250508 102240.745 INFO             PET5 Exiting 'user1_run'
20250508 102249.040 INFO             PET5  NUMBER_OF_PROCESSORS           6
20250508 102249.040 INFO             PET5  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20250508 102249.040 INFO             PET5  Finalizing ESMF
20250508 102249.040 INFO             PET5 ESMCI_IO_Handler.C:335 ESMCI::IO_Handler::finalize() 
20250508 102249.040 INFO             PET5 ESMCI_PIO_Handler.C:357 ESMCI::PIO_Handler::finalize() 
20250508 102249.040 INFO             PET5 ESMCI_IO_Handler.C:337 ESMCI::IO_Handler::finalize() after finalize, localrc = 0
20250508 102249.040 INFO             PET5 ESMCI_IO_Handler.C:360 ESMCI::IO_Handler::finalize() before return, localrc = 0
